{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMg2xGR0EOBxbaJ2PMj1MUJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nid989/LLMs-Overview/blob/main/langchain/functions_tools_and_agents_with_langchain.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "-A1gbwqgbTYg"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install openai python-dotenv langchain tiktoken\n",
        "!pip install pydantic==1.10.8\n",
        "!pip install langchain[docarray]\n",
        "!pip install wikipedia"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import openai\n",
        "\n",
        "from dotenv import load_dotenv, find_dotenv\n",
        "_ = load_dotenv(find_dotenv(filename=\"./.env.txt\"))\n",
        "openai.api_key = os.environ['OPENAI_API_KEY']"
      ],
      "metadata": {
        "id": "fvbIf4azetPw"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### `OpenAI: Function Calls`"
      ],
      "metadata": {
        "id": "vKtQnzMDqvvP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# Example dummy function hard coded to return the same weather\n",
        "# IN production, this could be your backend API or an external API\n",
        "def get_current_weather(location, unit=\"fahrenheit\"):\n",
        "    \"\"\"Get the current weather in a given location\"\"\"\n",
        "    weather_info = {\n",
        "        \"location\": location,\n",
        "        \"temperature\": \"72\",\n",
        "        \"unit\": unit,\n",
        "        \"forecast\": [\"sunny\", \"windy\"]\n",
        "    }\n",
        "    return json.dumps(weather_info)"
      ],
      "metadata": {
        "id": "5_ELmmZye_IU"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define a function (`As proposed by OpenAI`)\n",
        "# `description` parameter (both) are really important, since this will be passed directly\n",
        "# to the LLM and the model is gonna use this description to determine whether to use the function :)\n",
        "functions = [\n",
        "    {\n",
        "        \"name\": \"get_current_weather\",\n",
        "        \"description\": \"Get the current weather in a given location\",\n",
        "        \"parameters\": {\n",
        "            \"type\": \"object\",\n",
        "            \"properties\": {\n",
        "                \"location\": {\n",
        "                    \"type\": \"string\",\n",
        "                    \"description\": \"The city and state, e.g. San Fransico, CA\"\n",
        "                },\n",
        "                \"unit\": {\n",
        "                    \"type\": \"string\",\n",
        "                    \"enum\": [\"celsius\", \"fahrenheit\"]\n",
        "                }\n",
        "            },\n",
        "            \"required\": [\"location\"]\n",
        "        }\n",
        "    }\n",
        "]"
      ],
      "metadata": {
        "id": "vBfBXSZafHn1"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": \"what's the weather like in Boston?\"\n",
        "    }\n",
        "]"
      ],
      "metadata": {
        "id": "cl-2b237hUyR"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import openai"
      ],
      "metadata": {
        "id": "05OpIZCLhdrn"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = openai.chat.completions.create(\n",
        "    model=\"gpt-3.5-turbo-0613\",\n",
        "    messages=messages,\n",
        "    functions=functions\n",
        ")"
      ],
      "metadata": {
        "id": "vE89GoJuhvi2"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ewHgWcjGh49h",
        "outputId": "8602a10e-615e-4930-8b40-824ad1fb43de"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ChatCompletion(id='chatcmpl-8Nq0rkKe4Tj673swdBsZNWKLqSzNS', choices=[Choice(finish_reason='function_call', index=0, message=ChatCompletionMessage(content=None, role='assistant', function_call=FunctionCall(arguments='{\\n  \"location\": \"Boston, MA\"\\n}', name='get_current_weather'), tool_calls=None))], created=1700692173, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=18, prompt_tokens=84, total_tokens=102))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response_message = response.choices[0].message\n",
        "response_message"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "snRwYHJ9i40K",
        "outputId": "fa6f0d66-ea02-4d93-bb5a-a25c03aa9319"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatCompletionMessage(content=None, role='assistant', function_call=FunctionCall(arguments='{\\n  \"location\": \"Boston, MA\"\\n}', name='get_current_weather'), tool_calls=None)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response_message.content # None"
      ],
      "metadata": {
        "id": "uaQkoI-ljlKa"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response_message.function_call"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ald76vh0j14d",
        "outputId": "4963026a-6e22-479a-b684-7ee0b5622745"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "FunctionCall(arguments='{\\n  \"location\": \"Boston, MA\"\\n}', name='get_current_weather')"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "args = json.loads(response_message.function_call.arguments)\n",
        "args"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yiA_Idxkj3vo",
        "outputId": "dc151b18-a27c-4fcf-dfdb-90b83c54cc7a"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'location': 'Boston, MA'}"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "get_current_weather(args)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "P1pHD8yokAbO",
        "outputId": "0de1bf63-58c1-4bd9-bf92-ce529434b4d9"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'{\"location\": {\"location\": \"Boston, MA\"}, \"temperature\": \"72\", \"unit\": \"fahrenheit\", \"forecast\": [\"sunny\", \"windy\"]}'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# w/ OpenAI Function Calls, it doesnot utilize the defined function to generate the response\n",
        "# rather it will tell us what function to call and what the arguments to the specific function should be!"
      ],
      "metadata": {
        "id": "JtE8luHqkTKT"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# what if the message passed to the OpenAI model API is not related to the already defined function at all.\n",
        "messages = [\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": \"hi!\"\n",
        "    }\n",
        "]"
      ],
      "metadata": {
        "id": "ICMbpNByk_cD"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = openai.chat.completions.create(\n",
        "    model=\"gpt-3.5-turbo-0613\",\n",
        "    messages=messages,\n",
        "    functions=functions\n",
        ")"
      ],
      "metadata": {
        "id": "ClTlvPlYlSaU"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(response) # output `message` includes not None content and no function call argument as seen in the previous response"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hs7E8El7la9Q",
        "outputId": "7313658d-38eb-48e2-9bd7-168ded5ced66"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ChatCompletion(id='chatcmpl-8Nq0uuNh4Qayr8deb2uWORIGLWh6h', choices=[Choice(finish_reason='stop', index=0, message=ChatCompletionMessage(content='Hello! How can I assist you today?', role='assistant', function_call=None, tool_calls=None))], created=1700692176, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=10, prompt_tokens=78, total_tokens=88))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# `function_call` parameter either to force function call or not\n",
        "# mode: `auto`\n",
        "messages = [\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": \"hi!\"\n",
        "    }\n",
        "]\n",
        "\n",
        "response = openai.chat.completions.create(\n",
        "    model=\"gpt-3.5-turbo-0613\",\n",
        "    messages=messages,\n",
        "    functions=functions,\n",
        "    function_call=\"auto\" # default (i.e. the LLM choose b/w functions and self-response)\n",
        ")\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LDbPJCOilcUg",
        "outputId": "f298b25c-1240-4b37-817b-bcd3c6a665e5"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ChatCompletion(id='chatcmpl-8Nq0xVEjw10jAP6wCjOTw9N79q0t3', choices=[Choice(finish_reason='stop', index=0, message=ChatCompletionMessage(content='Hello! How can I assist you today?', role='assistant', function_call=None, tool_calls=None))], created=1700692179, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=10, prompt_tokens=78, total_tokens=88))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# mode: `none` # force LLM not make a function call\n",
        "messages = [\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": \"What's the weather in Boston?\"\n",
        "    }\n",
        "]\n",
        "\n",
        "response = openai.chat.completions.create(\n",
        "    model=\"gpt-3.5-turbo-0613\",\n",
        "    messages=messages,\n",
        "    functions=functions,\n",
        "    function_call=\"none\"\n",
        ")\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-aFiM0eimGwN",
        "outputId": "9e62faa5-324b-4ed3-acbc-5ac2a0706671"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ChatCompletion(id='chatcmpl-8Nq11qGZeTBWTrxy8BHsAnXv16gcK', choices=[Choice(finish_reason='stop', index=0, message=ChatCompletionMessage(content='The weather in Boston is currently unavailable.', role='assistant', function_call=None, tool_calls=None))], created=1700692183, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=8, prompt_tokens=84, total_tokens=92))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# mode: assign a specific function (i.e. force the model to compulsorily use the function-call w/ function name provided)\n",
        "messages = [\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": \"hi!\"\n",
        "    }\n",
        "]\n",
        "\n",
        "response = openai.chat.completions.create(\n",
        "    model=\"gpt-3.5-turbo-0613\",\n",
        "    messages=messages,\n",
        "    functions=functions,\n",
        "    function_call={\"name\": \"get_current_weather\"}\n",
        ")\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aB0lD7FFmheD",
        "outputId": "814e593c-908f-402f-eb1b-0d672e88bb90"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ChatCompletion(id='chatcmpl-8Nq1HolNNR6bN3CvQq9Yl0ebzJnU7', choices=[Choice(finish_reason='stop', index=0, message=ChatCompletionMessage(content=None, role='assistant', function_call=FunctionCall(arguments='{\\n  \"location\": \"San Francisco, CA\"\\n}', name='get_current_weather'), tool_calls=None))], created=1700692199, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=12, prompt_tokens=85, total_tokens=97))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": \"What's the weather in Boston?\",\n",
        "    }\n",
        "]\n",
        "\n",
        "response = openai.chat.completions.create(\n",
        "    model=\"gpt-3.5-turbo-0613\",\n",
        "    messages=messages,\n",
        "    functions=functions,\n",
        "    function_call={\"name\": \"get_current_weather\"}\n",
        ")\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mq7dkHe6m-Ci",
        "outputId": "6f5b540f-2a31-4b77-ec48-1d4ed0ba1e6e"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ChatCompletion(id='chatcmpl-8Nq1LlSP25CEb7GBZeWupYA4Iqx2M', choices=[Choice(finish_reason='stop', index=0, message=ChatCompletionMessage(content=None, role='assistant', function_call=FunctionCall(arguments='{\\n\"location\": \"Boston, MA\"\\n}', name='get_current_weather'), tool_calls=None))], created=1700692203, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=10, prompt_tokens=90, total_tokens=100))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# NOTE: under the `usage` output parameter, we can see that w/ and w/o `functions` as input-argument\n",
        "# the prompt_tokens utilization differs drastically, from 85 (w/ function-calling) to 15 (w/o function calling)\n",
        "# since extra tokens are utilized to describe the function-calling procedure to the OpenAI LLM API.\n",
        "\n",
        "# This can put constraint, since we have a limit over the OpenAI `prompt_tokens`, thus now we do have to keep\n",
        "# track of the total input tokens already used by function-calling proc. alongwith the messages supplied"
      ],
      "metadata": {
        "id": "25XEz9XRnJJY"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": \"What's the weather in Boston?\"\n",
        "    }\n",
        "]\n",
        "\n",
        "response = openai.chat.completions.create(\n",
        "    model=\"gpt-3.5-turbo-0613\",\n",
        "    messages=messages,\n",
        "    functions=functions,\n",
        "    function_call=\"auto\"\n",
        ")\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jlktbK-loje1",
        "outputId": "c0f691d1-df40-4706-e35c-f1ea42e61b26"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ChatCompletion(id='chatcmpl-8Nq1NvrHtL0n9eEF48WDLjCxvXbKd', choices=[Choice(finish_reason='function_call', index=0, message=ChatCompletionMessage(content=None, role='assistant', function_call=FunctionCall(arguments='{\\n  \"location\": \"Boston, MA\"\\n}', name='get_current_weather'), tool_calls=None))], created=1700692205, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=18, prompt_tokens=83, total_tokens=101))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "messages.append(response.choices[0].message) # appending to the list of messages to simulate calling to function w/ args"
      ],
      "metadata": {
        "id": "BUA4wT7Ko-Oa"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "args = json.loads(response.choices[0].message.function_call.arguments)\n",
        "observation = get_current_weather(args)"
      ],
      "metadata": {
        "id": "evmyd7EcpDbt"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "messages.append(\n",
        "    {\n",
        "        \"role\": \"function\",\n",
        "        \"name\": \"get_current_weather\",\n",
        "        \"content\": observation\n",
        "    }\n",
        ")\n",
        "print(messages)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zKmSkCiXpbgJ",
        "outputId": "6075829d-4c3c-495b-c1b2-147dea6e2f92"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'role': 'user', 'content': \"What's the weather in Boston?\"}, ChatCompletionMessage(content=None, role='assistant', function_call=FunctionCall(arguments='{\\n  \"location\": \"Boston, MA\"\\n}', name='get_current_weather'), tool_calls=None), {'role': 'function', 'name': 'get_current_weather', 'content': '{\"location\": {\"location\": \"Boston, MA\"}, \"temperature\": \"72\", \"unit\": \"fahrenheit\", \"forecast\": [\"sunny\", \"windy\"]}'}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = openai.chat.completions.create(\n",
        "    model=\"gpt-3.5-turbo-0613\",\n",
        "    messages=messages\n",
        ")\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IJQ7szfXp9gf",
        "outputId": "9eecd936-8ce4-4372-f9b1-8162a825b5e6"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ChatCompletion(id='chatcmpl-8Nq1mIZOwKcs02GpmmSVM80wMOIOn', choices=[Choice(finish_reason='stop', index=0, message=ChatCompletionMessage(content='The weather in Boston is currently sunny and windy with a temperature of 72°F.', role='assistant', function_call=None, tool_calls=None))], created=1700692230, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=17, prompt_tokens=76, total_tokens=93))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### `LangChain Expression Language (LECL)`"
      ],
      "metadata": {
        "id": "DDXne8KZq0X5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.schema.output_parser import StrOutputParser  # will take a ChatMessage and convert to string"
      ],
      "metadata": {
        "id": "V1-cQo87qVYR"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### `Simple Chain`"
      ],
      "metadata": {
        "id": "zSCYebZs9L4G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = ChatPromptTemplate.from_template(\n",
        "    \"tell me a short joke about {topic}\"\n",
        ")\n",
        "model = ChatOpenAI()\n",
        "output_parser = StrOutputParser()"
      ],
      "metadata": {
        "id": "OlqSn0rrskzz"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain = prompt | model | output_parser"
      ],
      "metadata": {
        "id": "7GEFG7726zef"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain.invoke({\"topic\": \"bears\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "BM2mQykK61av",
        "outputId": "252b62b2-5baa-4e7c-f9e6-aaecde4c74c6"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Why don't bears like fast food?\\n\\nBecause they can't catch it!\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### `More Complex Chain and Runnnable Map (to supply user-provided inputs to prompt)`"
      ],
      "metadata": {
        "id": "3H3JEPfB9OY1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "from langchain.vectorstores import DocArrayInMemorySearch"
      ],
      "metadata": {
        "id": "FSScROxq631r"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorstore = DocArrayInMemorySearch.from_texts(\n",
        "    [\"harrison worked at kensho\", \"bears like to eat honey\"],\n",
        "    embedding=OpenAIEmbeddings()\n",
        ")\n",
        "retriever = vectorstore.as_retriever()"
      ],
      "metadata": {
        "id": "u9c_26vW9vVE"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "retriever.get_relevant_documents(\"Where did harrison work?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ijSu_G2E95wL",
        "outputId": "9baedcf1-8d72-4a51-c6d4-ba5ebbe1f441"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(page_content='harrison worked at kensho'),\n",
              " Document(page_content='bears like to eat honey')]"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "retriever.get_relevant_documents(\"What do bears like to eat?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "owe-gU1r-8vz",
        "outputId": "3ad06911-d902-4d0f-e80f-e93c3438df6f"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(page_content='bears like to eat honey'),\n",
              " Document(page_content='harrison worked at kensho')]"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "template = \"\"\"Answer the question based only on the following context:\n",
        "{context}\n",
        "\n",
        "Question: {question}\"\"\"\n",
        "prompt = ChatPromptTemplate.from_template(template)"
      ],
      "metadata": {
        "id": "yYtzGC6Z_AEk"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# How are we going to build the chain?\n",
        "# i. We want the first and only input to the chain to be the user question.\n",
        "# ii. Then we want to fetch the relevant document for the context which will be passed to the prompt\n",
        "# iii. Finally, we want to pass the formulated prompt to the model and the output-parser later on."
      ],
      "metadata": {
        "id": "XGQ9tz4b_VmP"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# i. we want to create `Something` which takes a single question and turns it into\n",
        "# a dictionary of 2 elements, `context` and `question`.\n",
        "from langchain.schema.runnable import RunnableMap"
      ],
      "metadata": {
        "id": "GT70l7ug_3mN"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain = RunnableMap({\n",
        "    \"context\": lambda x: retriever.get_relevant_documents(x[\"question\"]),\n",
        "    \"question\": lambda x: x[\"question\"]\n",
        "}) | prompt | model | output_parser"
      ],
      "metadata": {
        "id": "eNwH8Vu6_NkX"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain.invoke({\"question\": \"Where did harrison work?\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "6Y0EKHhiAVHS",
        "outputId": "f88fb040-fc92-417f-ce91-7546af6bf0a5"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Harrison worked at Kensho.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = RunnableMap({\n",
        "    \"context\": lambda x: retriever.get_relevant_documents(x[\"question\"]),\n",
        "    \"question\": lambda x: x[\"question\"]\n",
        "})"
      ],
      "metadata": {
        "id": "YQ6jOQx_AdOB"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs.invoke({\"question\": \"Where did harrison work?\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wq6soeLxAu2s",
        "outputId": "7ee488b0-fdf9-4b9f-9009-c849654e5eb3"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'context': [Document(page_content='harrison worked at kensho'),\n",
              "  Document(page_content='bears like to eat honey')],\n",
              " 'question': 'Where did harrison work?'}"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### `Bind and OpenAI Functions`"
      ],
      "metadata": {
        "id": "Jwcn0ZHuA6u2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "functions = [\n",
        "    {\n",
        "        \"name\": \"weather_search\",\n",
        "        \"description\": \"Search for weather given at airport code\",\n",
        "        \"parameters\": {\n",
        "            \"type\": \"object\",\n",
        "            \"properties\": {\n",
        "                \"airport_code\": {\n",
        "                    \"type\": \"string\",\n",
        "                    \"description\": \"The airport code to get the weather for\"\n",
        "                },\n",
        "            },\n",
        "            \"required\": [\"airport_code\"]\n",
        "        }\n",
        "    }\n",
        "]"
      ],
      "metadata": {
        "id": "1QoWxzQdAxw0"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"human\", \"{input}\")\n",
        "    ]\n",
        ")\n",
        "model = ChatOpenAI(temperature=0).bind(functions=functions)"
      ],
      "metadata": {
        "id": "sQTvnQsdBl4x"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "runnable = prompt | model"
      ],
      "metadata": {
        "id": "EdnRM8DBCCJB"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "runnable.invoke({\"input\": \"what is the weather in sf?\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NXCNSCLcCDyG",
        "outputId": "464a4121-d605-4296-bf02-c77b7b21a284"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{\\n  \"airport_code\": \"SFO\"\\n}', 'name': 'weather_search'}})"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "functions = [\n",
        "    {\n",
        "        \"name\": \"weather_search\",\n",
        "        \"description\": \"Search for weather given at airport code\",\n",
        "        \"parameters\": {\n",
        "            \"type\": \"object\",\n",
        "            \"properties\": {\n",
        "                \"airport_code\": {\n",
        "                    \"type\": \"string\",\n",
        "                    \"description\": \"The airport code to get the weather for\"\n",
        "                },\n",
        "            },\n",
        "            \"required\": [\"airport_code\"]\n",
        "        }\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"sports_search\",\n",
        "        \"description\": \"Search for news of recent sport events\",\n",
        "        \"parameters\": {\n",
        "            \"type\": \"object\",\n",
        "            \"properties\": {\n",
        "                \"team_name\": {\n",
        "                    \"type\": \"string\",\n",
        "                    \"description\": \"The sports team to search for\"\n",
        "                },\n",
        "            },\n",
        "            \"required\": [\"team_name\"]\n",
        "        }\n",
        "    }\n",
        "]"
      ],
      "metadata": {
        "id": "o20B6WgjCKSD"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = model.bind(functions=functions)"
      ],
      "metadata": {
        "id": "BOgn4_zuCyjB"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "runnable = prompt | model"
      ],
      "metadata": {
        "id": "HmQXyp6KC142"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "runnable.invoke({\"input\": \"how did the patriots do yesterday?\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "76T7wZtpC6CH",
        "outputId": "481c37e9-6d8d-4492-abfb-8a7bf5776bf4"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{\\n  \"team_name\": \"patriots\"\\n}', 'name': 'sports_search'}})"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### `Fallbacks`\n",
        "`with LangChain, we can attach fallbacks not only to the model but any component of the chain or the chain as whole.`"
      ],
      "metadata": {
        "id": "ezgtaUErDD18"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# let's say we want to output a JSON formatted string from the LLM\n",
        "# NOTE: using an older version of OpenAI LLM API just to showcase the need for `fallbacks`.\n",
        "from langchain.llms import OpenAI\n",
        "import json"
      ],
      "metadata": {
        "id": "xBKFDf1_DAdW"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "simple_model = OpenAI(\n",
        "    temperature=0,\n",
        "    max_tokens=1000,\n",
        "    model=\"text-davinci-001\"\n",
        ")\n",
        "simple_chain = simple_model | json.loads"
      ],
      "metadata": {
        "id": "PUDoUk-rDjcQ"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "challenge = \"write three poems in a json blob, where each poem is a json blob of a title, author, and first line\""
      ],
      "metadata": {
        "id": "ZhqbepjNDvbq"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "simple_model.invoke(challenge)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "id": "P9ZDOvZ1D6Sh",
        "outputId": "87c6973f-b36a-4720-cd53-5dcf8f16a645"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n\\n[\"The Waste Land\",\"T.S. Eliot\",\"April is the cruelest month, breeding lilacs out of the dead land\"]\\n\\n[\"The Raven\",\"Edgar Allan Poe\",\"Once upon a midnight dreary, while I pondered, weak and weary\"]\\n\\n[\"Ode to a Nightingale\",\"John Keats\",\"Thou still unravish\\'d bride of quietness, Thou foster-child of silence and slow time\"]'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# calling the below line, we get the JSONDecodeError, since the output string\n",
        "# eventhough it has some structure, cannot be decoded by json.loads() function.\n",
        "# simple_chain.invoke(challenge)"
      ],
      "metadata": {
        "id": "BTWtvJ6bD9l8"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = ChatOpenAI(temperature=0) # using a newer version of OpenAI models\n",
        "chain = model | StrOutputParser() | json.loads"
      ],
      "metadata": {
        "id": "LECQ5UVzEMAE"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain.invoke(challenge)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JxrrGy6REppM",
        "outputId": "30faac52-f61f-42a9-a29c-8049b70b5abf"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'poem1': {'title': 'Whispers of the Wind',\n",
              "  'author': 'Emily Rivers',\n",
              "  'first_line': 'Softly it comes, the whisper of the wind'},\n",
              " 'poem2': {'title': 'Silent Serenade',\n",
              "  'author': 'Jacob Moore',\n",
              "  'first_line': 'In the stillness of night, a silent serenade'},\n",
              " 'poem3': {'title': 'Dancing Shadows',\n",
              "  'author': 'Sophia Anderson',\n",
              "  'first_line': 'Shadows dance upon the moonlit floor'}}"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# so we can start with a simple chain and attach fallbacks with `.with_fallbacks`\n",
        "# so first the core runnable `simple_chain` will try to run and if error is raised\n",
        "# then it goes through list of runnables provided as fallbacks, in this case its just\n",
        "# 1 extra runnable `chain`.\n",
        "final_chain = simple_chain.with_fallbacks([chain])"
      ],
      "metadata": {
        "id": "XPnH0FawEr7D"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_chain.invoke(challenge)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5vhLOKw9Fjur",
        "outputId": "4b36d114-6c90-4fad-f168-04b232cdaf98"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'poem1': {'title': 'Whispers of the Wind',\n",
              "  'author': 'Emily Rivers',\n",
              "  'first_line': 'Softly it comes, the whisper of the wind'},\n",
              " 'poem2': {'title': 'Silent Serenade',\n",
              "  'author': 'Jacob Moore',\n",
              "  'first_line': 'In the stillness of night, a silent serenade'},\n",
              " 'poem3': {'title': 'Dancing Shadows',\n",
              "  'author': 'Sophia Anderson',\n",
              "  'first_line': 'Shadows dance upon the walls, a secret ballet'}}"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import langchain\n",
        "langchain.debug = True\n",
        "final_chain.invoke(challenge)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tu4qx4rZFmba",
        "outputId": "496a6adb-e259-4704-ff31-fb6159eae178"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:RunnableWithFallbacks] Entering Chain run with input:\n",
            "\u001b[0m{\n",
            "  \"input\": \"write three poems in a json blob, where each poem is a json blob of a title, author, and first line\"\n",
            "}\n",
            "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:RunnableWithFallbacks > 2:chain:RunnableSequence] Entering Chain run with input:\n",
            "\u001b[0m{\n",
            "  \"input\": \"write three poems in a json blob, where each poem is a json blob of a title, author, and first line\"\n",
            "}\n",
            "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:chain:RunnableWithFallbacks > 2:chain:RunnableSequence > 3:llm:OpenAI] Entering LLM run with input:\n",
            "\u001b[0m{\n",
            "  \"prompts\": [\n",
            "    \"write three poems in a json blob, where each poem is a json blob of a title, author, and first line\"\n",
            "  ]\n",
            "}\n",
            "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[1:chain:RunnableWithFallbacks > 2:chain:RunnableSequence > 3:llm:OpenAI] [1.67s] Exiting LLM run with output:\n",
            "\u001b[0m{\n",
            "  \"generations\": [\n",
            "    [\n",
            "      {\n",
            "        \"text\": \"\\n\\n[\\\"The Waste Land\\\",\\\"T.S. Eliot\\\",\\\"April is the cruelest month, breeding lilacs out of the dead land\\\"]\\n\\n[\\\"The Raven\\\",\\\"Edgar Allan Poe\\\",\\\"Once upon a midnight dreary, while I pondered, weak and weary\\\"]\\n\\n[\\\"Ode to a Nightingale\\\",\\\"John Keats\\\",\\\"Thou still unravish'd bride of quietness, Thou foster-child of silence and slow time\\\"]\",\n",
            "        \"generation_info\": {\n",
            "          \"finish_reason\": \"stop\",\n",
            "          \"logprobs\": null\n",
            "        },\n",
            "        \"type\": \"Generation\"\n",
            "      }\n",
            "    ]\n",
            "  ],\n",
            "  \"llm_output\": {\n",
            "    \"token_usage\": {\n",
            "      \"prompt_tokens\": 24,\n",
            "      \"completion_tokens\": 94,\n",
            "      \"total_tokens\": 118\n",
            "    },\n",
            "    \"model_name\": \"text-davinci-001\"\n",
            "  },\n",
            "  \"run\": null\n",
            "}\n",
            "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:RunnableWithFallbacks > 2:chain:RunnableSequence > 4:chain:loads] Entering Chain run with input:\n",
            "\u001b[0m{\n",
            "  \"input\": \"\\n\\n[\\\"The Waste Land\\\",\\\"T.S. Eliot\\\",\\\"April is the cruelest month, breeding lilacs out of the dead land\\\"]\\n\\n[\\\"The Raven\\\",\\\"Edgar Allan Poe\\\",\\\"Once upon a midnight dreary, while I pondered, weak and weary\\\"]\\n\\n[\\\"Ode to a Nightingale\\\",\\\"John Keats\\\",\\\"Thou still unravish'd bride of quietness, Thou foster-child of silence and slow time\\\"]\"\n",
            "}\n",
            "\u001b[31;1m\u001b[1;3m[chain/error]\u001b[0m \u001b[1m[1:chain:RunnableWithFallbacks > 2:chain:RunnableSequence > 4:chain:loads] [1ms] Chain run errored with error:\n",
            "\u001b[0m\"JSONDecodeError('Extra data: line 5 column 1 (char 103)')\"\n",
            "\u001b[31;1m\u001b[1;3m[chain/error]\u001b[0m \u001b[1m[1:chain:RunnableWithFallbacks > 2:chain:RunnableSequence] [1.68s] Chain run errored with error:\n",
            "\u001b[0m\"JSONDecodeError('Extra data: line 5 column 1 (char 103)')\"\n",
            "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:RunnableWithFallbacks > 5:chain:RunnableSequence] Entering Chain run with input:\n",
            "\u001b[0m{\n",
            "  \"input\": \"write three poems in a json blob, where each poem is a json blob of a title, author, and first line\"\n",
            "}\n",
            "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:chain:RunnableWithFallbacks > 5:chain:RunnableSequence > 6:llm:ChatOpenAI] Entering LLM run with input:\n",
            "\u001b[0m{\n",
            "  \"prompts\": [\n",
            "    \"Human: write three poems in a json blob, where each poem is a json blob of a title, author, and first line\"\n",
            "  ]\n",
            "}\n",
            "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[1:chain:RunnableWithFallbacks > 5:chain:RunnableSequence > 6:llm:ChatOpenAI] [19.76s] Exiting LLM run with output:\n",
            "\u001b[0m{\n",
            "  \"generations\": [\n",
            "    [\n",
            "      {\n",
            "        \"text\": \"{\\n  \\\"poem1\\\": {\\n    \\\"title\\\": \\\"Whispers of the Wind\\\",\\n    \\\"author\\\": \\\"Emily Rivers\\\",\\n    \\\"first_line\\\": \\\"Softly it comes, the whisper of the wind\\\"\\n  },\\n  \\\"poem2\\\": {\\n    \\\"title\\\": \\\"Silent Serenade\\\",\\n    \\\"author\\\": \\\"Jacob Moore\\\",\\n    \\\"first_line\\\": \\\"In the stillness of night, a silent serenade\\\"\\n  },\\n  \\\"poem3\\\": {\\n    \\\"title\\\": \\\"Dancing Shadows\\\",\\n    \\\"author\\\": \\\"Sophia Anderson\\\",\\n    \\\"first_line\\\": \\\"Shadows dance upon the walls, a secret ballet\\\"\\n  }\\n}\",\n",
            "        \"generation_info\": {\n",
            "          \"finish_reason\": \"stop\"\n",
            "        },\n",
            "        \"type\": \"ChatGeneration\",\n",
            "        \"message\": {\n",
            "          \"lc\": 1,\n",
            "          \"type\": \"constructor\",\n",
            "          \"id\": [\n",
            "            \"langchain\",\n",
            "            \"schema\",\n",
            "            \"messages\",\n",
            "            \"AIMessage\"\n",
            "          ],\n",
            "          \"kwargs\": {\n",
            "            \"content\": \"{\\n  \\\"poem1\\\": {\\n    \\\"title\\\": \\\"Whispers of the Wind\\\",\\n    \\\"author\\\": \\\"Emily Rivers\\\",\\n    \\\"first_line\\\": \\\"Softly it comes, the whisper of the wind\\\"\\n  },\\n  \\\"poem2\\\": {\\n    \\\"title\\\": \\\"Silent Serenade\\\",\\n    \\\"author\\\": \\\"Jacob Moore\\\",\\n    \\\"first_line\\\": \\\"In the stillness of night, a silent serenade\\\"\\n  },\\n  \\\"poem3\\\": {\\n    \\\"title\\\": \\\"Dancing Shadows\\\",\\n    \\\"author\\\": \\\"Sophia Anderson\\\",\\n    \\\"first_line\\\": \\\"Shadows dance upon the walls, a secret ballet\\\"\\n  }\\n}\",\n",
            "            \"additional_kwargs\": {}\n",
            "          }\n",
            "        }\n",
            "      }\n",
            "    ]\n",
            "  ],\n",
            "  \"llm_output\": {\n",
            "    \"token_usage\": {\n",
            "      \"completion_tokens\": 136,\n",
            "      \"prompt_tokens\": 31,\n",
            "      \"total_tokens\": 167\n",
            "    },\n",
            "    \"model_name\": \"gpt-3.5-turbo\",\n",
            "    \"system_fingerprint\": null\n",
            "  },\n",
            "  \"run\": null\n",
            "}\n",
            "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:RunnableWithFallbacks > 5:chain:RunnableSequence > 7:parser:StrOutputParser] Entering Parser run with input:\n",
            "\u001b[0m[inputs]\n",
            "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:RunnableWithFallbacks > 5:chain:RunnableSequence > 7:parser:StrOutputParser] [0ms] Exiting Parser run with output:\n",
            "\u001b[0m{\n",
            "  \"output\": \"{\\n  \\\"poem1\\\": {\\n    \\\"title\\\": \\\"Whispers of the Wind\\\",\\n    \\\"author\\\": \\\"Emily Rivers\\\",\\n    \\\"first_line\\\": \\\"Softly it comes, the whisper of the wind\\\"\\n  },\\n  \\\"poem2\\\": {\\n    \\\"title\\\": \\\"Silent Serenade\\\",\\n    \\\"author\\\": \\\"Jacob Moore\\\",\\n    \\\"first_line\\\": \\\"In the stillness of night, a silent serenade\\\"\\n  },\\n  \\\"poem3\\\": {\\n    \\\"title\\\": \\\"Dancing Shadows\\\",\\n    \\\"author\\\": \\\"Sophia Anderson\\\",\\n    \\\"first_line\\\": \\\"Shadows dance upon the walls, a secret ballet\\\"\\n  }\\n}\"\n",
            "}\n",
            "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:RunnableWithFallbacks > 5:chain:RunnableSequence > 8:chain:loads] Entering Chain run with input:\n",
            "\u001b[0m{\n",
            "  \"input\": \"{\\n  \\\"poem1\\\": {\\n    \\\"title\\\": \\\"Whispers of the Wind\\\",\\n    \\\"author\\\": \\\"Emily Rivers\\\",\\n    \\\"first_line\\\": \\\"Softly it comes, the whisper of the wind\\\"\\n  },\\n  \\\"poem2\\\": {\\n    \\\"title\\\": \\\"Silent Serenade\\\",\\n    \\\"author\\\": \\\"Jacob Moore\\\",\\n    \\\"first_line\\\": \\\"In the stillness of night, a silent serenade\\\"\\n  },\\n  \\\"poem3\\\": {\\n    \\\"title\\\": \\\"Dancing Shadows\\\",\\n    \\\"author\\\": \\\"Sophia Anderson\\\",\\n    \\\"first_line\\\": \\\"Shadows dance upon the walls, a secret ballet\\\"\\n  }\\n}\"\n",
            "}\n",
            "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:RunnableWithFallbacks > 5:chain:RunnableSequence > 8:chain:loads] [1ms] Exiting Chain run with output:\n",
            "\u001b[0m{\n",
            "  \"poem1\": {\n",
            "    \"title\": \"Whispers of the Wind\",\n",
            "    \"author\": \"Emily Rivers\",\n",
            "    \"first_line\": \"Softly it comes, the whisper of the wind\"\n",
            "  },\n",
            "  \"poem2\": {\n",
            "    \"title\": \"Silent Serenade\",\n",
            "    \"author\": \"Jacob Moore\",\n",
            "    \"first_line\": \"In the stillness of night, a silent serenade\"\n",
            "  },\n",
            "  \"poem3\": {\n",
            "    \"title\": \"Dancing Shadows\",\n",
            "    \"author\": \"Sophia Anderson\",\n",
            "    \"first_line\": \"Shadows dance upon the walls, a secret ballet\"\n",
            "  }\n",
            "}\n",
            "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:RunnableWithFallbacks > 5:chain:RunnableSequence] [19.77s] Exiting Chain run with output:\n",
            "\u001b[0m{\n",
            "  \"poem1\": {\n",
            "    \"title\": \"Whispers of the Wind\",\n",
            "    \"author\": \"Emily Rivers\",\n",
            "    \"first_line\": \"Softly it comes, the whisper of the wind\"\n",
            "  },\n",
            "  \"poem2\": {\n",
            "    \"title\": \"Silent Serenade\",\n",
            "    \"author\": \"Jacob Moore\",\n",
            "    \"first_line\": \"In the stillness of night, a silent serenade\"\n",
            "  },\n",
            "  \"poem3\": {\n",
            "    \"title\": \"Dancing Shadows\",\n",
            "    \"author\": \"Sophia Anderson\",\n",
            "    \"first_line\": \"Shadows dance upon the walls, a secret ballet\"\n",
            "  }\n",
            "}\n",
            "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:RunnableWithFallbacks] [21.46s] Exiting Chain run with output:\n",
            "\u001b[0m{\n",
            "  \"poem1\": {\n",
            "    \"title\": \"Whispers of the Wind\",\n",
            "    \"author\": \"Emily Rivers\",\n",
            "    \"first_line\": \"Softly it comes, the whisper of the wind\"\n",
            "  },\n",
            "  \"poem2\": {\n",
            "    \"title\": \"Silent Serenade\",\n",
            "    \"author\": \"Jacob Moore\",\n",
            "    \"first_line\": \"In the stillness of night, a silent serenade\"\n",
            "  },\n",
            "  \"poem3\": {\n",
            "    \"title\": \"Dancing Shadows\",\n",
            "    \"author\": \"Sophia Anderson\",\n",
            "    \"first_line\": \"Shadows dance upon the walls, a secret ballet\"\n",
            "  }\n",
            "}\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'poem1': {'title': 'Whispers of the Wind',\n",
              "  'author': 'Emily Rivers',\n",
              "  'first_line': 'Softly it comes, the whisper of the wind'},\n",
              " 'poem2': {'title': 'Silent Serenade',\n",
              "  'author': 'Jacob Moore',\n",
              "  'first_line': 'In the stillness of night, a silent serenade'},\n",
              " 'poem3': {'title': 'Dancing Shadows',\n",
              "  'author': 'Sophia Anderson',\n",
              "  'first_line': 'Shadows dance upon the walls, a secret ballet'}}"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "langchain.debug = False"
      ],
      "metadata": {
        "id": "szXbn70xGKCB"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### `Interface`"
      ],
      "metadata": {
        "id": "Ecy-SD9QF0vP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = ChatPromptTemplate.from_template(\n",
        "    \"Tell me a short joke about {topic}\"\n",
        ")\n",
        "model = ChatOpenAI()\n",
        "output_parser = StrOutputParser()\n",
        "\n",
        "chain = prompt | model | output_parser"
      ],
      "metadata": {
        "id": "BdWgZoAXFui8"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We will explore few different elements of the interface"
      ],
      "metadata": {
        "id": "aJrx69_7GOCx"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain.invoke({\"topic\": \"bears\"}) # synchronous method that calls it on 1 single input."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "yIQvpwo-GFMW",
        "outputId": "4c72015c-9023-4923-930b-d41404ec3f87"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Why don't bears wear shoes?\\n\\nBecause they already have bear feet!\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chain.batch([{\"topic\": \"bears\"}, {\"topic\": \"frogs\"}]) # synchronous method that calls it on list of inputs."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lwq0emARGHtU",
        "outputId": "58300b93-0be6-419f-a0ea-8ba799a4d812"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"Why don't bears wear shoes?\\n\\nBecause they have bear feet!\",\n",
              " \"Why don't frogs like to play basketball?\\n\\nBecause they always get too jumpy!\"]"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# stream back the output based on tokens generated (ideal for showcasing a streaming property when recording bot-replies in chat-interface)\n",
        "for t in chain.stream({\"topic\": \"bears\"}):\n",
        "    print(t)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jc_5Ym47GlFt",
        "outputId": "7afb35e4-8d0a-48cc-b36f-ee6f35363b86"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Why\n",
            " don\n",
            "'t\n",
            " bears\n",
            " wear\n",
            " socks\n",
            "?\n",
            " \n",
            "\n",
            "\n",
            "Because\n",
            " they\n",
            " have\n",
            " bear\n",
            " feet\n",
            "!\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# calls it in a asynchronous manner\n",
        "response = await chain.ainvoke({\"topic\": \"bears\"})\n",
        "response"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "QN6hbpj-GsWE",
        "outputId": "e9bc6721-5fca-43dc-bdb2-51ed17cd0481"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Why don't bears wear shoes?\\n\\nBecause they have bear feet!\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### `OpenAI Function Calling in LangChain`"
      ],
      "metadata": {
        "id": "WnbiHXcdHxxz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Objective: How to use OpenAI functions and combine them with LCEL (LangChain Expression Language)\n",
        "# Additionally, explore pydantic which makes it easier to work with OpenAI functions."
      ],
      "metadata": {
        "id": "Pr4KsMpjHMLu"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List\n",
        "from pydantic import BaseModel, Field"
      ],
      "metadata": {
        "id": "d1xEb5LeK-Mf"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### `Pydantic Syntax`\n",
        "`Pydantic data classes are a blend of Python's data classes with the validation power of pydantic (since in normal python we specify data-types for each attribute but they're not validated and are just used for user reference)`\n",
        "\n",
        "`They offer a concise way to define data structures while ensuring that the data adheres to specified types and constraints.`"
      ],
      "metadata": {
        "id": "O0XYDF2YNWPJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# in standard python you would create a class like this:\n",
        "class User:\n",
        "    def __init__(self, name: str, age: int, email: str):\n",
        "        self.name = name\n",
        "        self.age = age\n",
        "        self.email = email"
      ],
      "metadata": {
        "id": "T1Nfu6SGNVQu"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "foo = User(name=\"Joe\", age=32, email=\"joe@gmail.com\")\n",
        "foo.name"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "ekpZcFR-N_JR",
        "outputId": "8e1a4ef2-f775-494a-9948-b1b4884a3cab"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Joe'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "foo = User(name=\"Joe\", age=\"bar\", email=\"joe@gmail.com\")\n",
        "foo.age # expected data-type for attr. `age` was \"int\" but the class cannot validate and accepted \"str\" too."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "YL8-46CvOK9s",
        "outputId": "4a4a791a-bd9a-495f-8eb4-4e6b5e1b8a89"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'bar'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class pUser(BaseModel):\n",
        "    name: str\n",
        "    age: int\n",
        "    email: str"
      ],
      "metadata": {
        "id": "EO3_uE_rORXF"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "foo_p = pUser(name=\"Joe\", age=32, email=\"joe@gmail.com\")\n",
        "foo_p.name"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "k4x5hoOtPBZh",
        "outputId": "49fdd477-7d19-4bcf-f60e-4f51f7d873a4"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Joe'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Note: below code will give ValidationError, since attr. `age` is supposed to be \"int\"\n",
        "# foo_p = pUser(name=\"Joe\", age=\"bar\", email=\"joe@gmail.com\")"
      ],
      "metadata": {
        "id": "p-tM-lqCPGBu"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# with Pydantic we can nest these data-structures\n",
        "class Class(BaseModel):\n",
        "    students: List[pUser]"
      ],
      "metadata": {
        "id": "CuDnjIokPWFe"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "obj = Class(\n",
        "    students=[pUser(name=\"Joe\", age=32, email=\"joe@gmail.com\")]\n",
        ")"
      ],
      "metadata": {
        "id": "sVEifobYPurH"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "obj"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FKZHhGJfP6u7",
        "outputId": "83c9de2a-d28e-4458-c8f9-fcf14ccd8450"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Class(students=[pUser(name='Joe', age=32, email='joe@gmail.com')])"
            ]
          },
          "metadata": {},
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### `Pydantic to OpenAI function definition`"
      ],
      "metadata": {
        "id": "iVqr03ZFQAMi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class WeatherSearch(BaseModel):\n",
        "    \"\"\"Call this with an airport code to get the weather at that airport\"\"\"\n",
        "    airport_code: str = Field(description=\"airport code to get weather for\")"
      ],
      "metadata": {
        "id": "GAxrI9TWP7jA"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.utils.openai_functions import convert_pydantic_to_openai_function"
      ],
      "metadata": {
        "id": "ifbwAEuMQRPf"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "weather_function = convert_pydantic_to_openai_function(WeatherSearch)"
      ],
      "metadata": {
        "id": "mSyboysyQUo5"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "weather_function # the main description is pulled from the DocString of the class definition"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r0JgzY7CQaVo",
        "outputId": "8007a7c0-ebd4-46c7-d7ee-b95af99c2364"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'name': 'WeatherSearch',\n",
              " 'description': 'Call this with an airport code to get the weather at that airport',\n",
              " 'parameters': {'title': 'WeatherSearch',\n",
              "  'description': 'Call this with an airport code to get the weather at that airport',\n",
              "  'type': 'object',\n",
              "  'properties': {'airport_code': {'title': 'Airport Code',\n",
              "    'description': 'airport code to get weather for',\n",
              "    'type': 'string'}},\n",
              "  'required': ['airport_code']}}"
            ]
          },
          "metadata": {},
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class WeatherSearch(BaseModel):\n",
        "    airport_code: str = Field(description=\"airport code to get weather for\")"
      ],
      "metadata": {
        "id": "dQuVKRVhRJ6q"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# DocString is a required **compulsory when defining a class definition inherited from Pydantic BaseModel\n",
        "# Note: the below code will produce error a `KeyError` on calling `convert_pydantic_to_openai_function`\n",
        "# since the DocString is not provided\n",
        "# convert_pydantic_to_openai_function(WeatherSearch)"
      ],
      "metadata": {
        "id": "0N1Wg4-dQa0f"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# However, when defining the attributes it is not compulsory to provide the description\n",
        "class WeatherSearch2(BaseModel):\n",
        "    \"\"\"Call this with an airport code to get the weather at that airport\"\"\"\n",
        "    airport_code: str"
      ],
      "metadata": {
        "id": "78vNJxprRQ1r"
      },
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "convert_pydantic_to_openai_function(WeatherSearch2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vdqgJNMTRrFQ",
        "outputId": "bd0c0b35-9191-4320-c5b5-25f17af3824b"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'name': 'WeatherSearch2',\n",
              " 'description': 'Call this with an airport code to get the weather at that airport',\n",
              " 'parameters': {'title': 'WeatherSearch2',\n",
              "  'description': 'Call this with an airport code to get the weather at that airport',\n",
              "  'type': 'object',\n",
              "  'properties': {'airport_code': {'title': 'Airport Code', 'type': 'string'}},\n",
              "  'required': ['airport_code']}}"
            ]
          },
          "metadata": {},
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# combining OpenAI function with LCEL\n",
        "from langchain.chat_models import ChatOpenAI"
      ],
      "metadata": {
        "id": "AUhlVa07RuFz"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = ChatOpenAI()"
      ],
      "metadata": {
        "id": "kbwN9aDtR1Sz"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.invoke(\"what is the weather in SF today?\", functions=[weather_function]) # w/ keyword args `functions`"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PgRZFLaFR4-s",
        "outputId": "828dda54-185d-445d-d62c-34ffdcde0a74"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{\\n  \"airport_code\": \"SFO\"\\n}', 'name': 'WeatherSearch'}})"
            ]
          },
          "metadata": {},
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# we can bind the function invocation to the model,\n",
        "# reason: we can pass that {model, function} pair together and\n",
        "# do not have to worry about the explicit mentioning of kwargs\n",
        "model_with_function = model.bind(functions=[weather_function])"
      ],
      "metadata": {
        "id": "jH2SW81CSJeb"
      },
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_with_function.invoke(\"what is the weather in SF?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b6wuibyMSi-k",
        "outputId": "3fd9a8a8-86e4-4ad5-99b4-3ec0ef16a423"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{\\n  \"airport_code\": \"SFO\"\\n}', 'name': 'WeatherSearch'}})"
            ]
          },
          "metadata": {},
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### `Forcing it to use a function`"
      ],
      "metadata": {
        "id": "XNhoKqmmStpY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_with_forced_function = model.bind(functions=[weather_function], function_call={\"name\": \"WeatherSearch\"})"
      ],
      "metadata": {
        "id": "17Rb5IUlSmcD"
      },
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_with_forced_function.invoke(\"what is the weather in SF?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jYvIN6XbS2dV",
        "outputId": "178f7a18-13e1-4cd7-d739-050a8b3a3bf4"
      },
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{\\n  \"airport_code\": \"SFO\"\\n}', 'name': 'WeatherSearch'}})"
            ]
          },
          "metadata": {},
          "execution_count": 94
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_with_forced_function.invoke(\"hi!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B1J7nMPYS5mL",
        "outputId": "db70f19a-d68e-4924-cda0-4c5c69fec6ff"
      },
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{\\n  \"airport_code\": \"LAX\"\\n}', 'name': 'WeatherSearch'}})"
            ]
          },
          "metadata": {},
          "execution_count": 95
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### `Using in a Chain`"
      ],
      "metadata": {
        "id": "TchZNvPdTADm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import ChatPromptTemplate"
      ],
      "metadata": {
        "id": "ZkA8fK0xS9tD"
      },
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"You are a helpful assistant\"),\n",
        "    (\"user\", \"{input}\")\n",
        "])"
      ],
      "metadata": {
        "id": "VAWebDO2TDy6"
      },
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain = prompt | model_with_function"
      ],
      "metadata": {
        "id": "YqFrYWu4TK1B"
      },
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain.invoke({\"input\": \"what is the weather in SF?\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ya_-t8-STNYq",
        "outputId": "f7cd9742-5b81-43cc-f366-7efed651b7e2"
      },
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{\\n  \"airport_code\": \"SFO\"\\n}', 'name': 'WeatherSearch'}})"
            ]
          },
          "metadata": {},
          "execution_count": 99
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### `Using multiple functions`"
      ],
      "metadata": {
        "id": "HLnbSUROTUB7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# we can pass a set of functions and let the LLM decide which to use based on the question context."
      ],
      "metadata": {
        "id": "RRuEVUfvTQDH"
      },
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class WeatherSearch(BaseModel):\n",
        "    \"\"\"Call this with an airport code to get the weather at that airport\"\"\"\n",
        "    airport_code: str = Field(description=\"airport code to get weather for\")\n",
        "\n",
        "class ArtistSearch(BaseModel):\n",
        "    \"\"\"Call this to get the names of songs by a particular artist\"\"\"\n",
        "    artist_name: str = Field(description=\"name of the artisit to look up\")\n",
        "    n: int = Field(description=\"number of results\")"
      ],
      "metadata": {
        "id": "FSdpAIijTim0"
      },
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "functions = [\n",
        "    convert_pydantic_to_openai_function(WeatherSearch),\n",
        "    convert_pydantic_to_openai_function(ArtistSearch)\n",
        "]"
      ],
      "metadata": {
        "id": "lj4xRFcTTz6r"
      },
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_with_functions = model.bind(functions=functions)"
      ],
      "metadata": {
        "id": "9aKv8uo2T3--"
      },
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_with_functions.invoke(\"what is the weather in SF?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1fhsxgkqUDMx",
        "outputId": "fb4aacf5-98a1-49bd-c90e-e03f9f03c821"
      },
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{\\n\"airport_code\": \"SFO\"\\n}', 'name': 'WeatherSearch'}})"
            ]
          },
          "metadata": {},
          "execution_count": 104
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_with_functions.invoke(\"what are three songs by taylor swift?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1aFVMo9fUHmd",
        "outputId": "94a3584a-ba68-47ef-f295-2c1bdfa2873c"
      },
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{\\n  \"artist_name\": \"taylor swift\",\\n  \"n\": 3\\n}', 'name': 'ArtistSearch'}})"
            ]
          },
          "metadata": {},
          "execution_count": 105
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_with_functions.invoke(\"hi!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H0rkKFnzUMFk",
        "outputId": "7bd63440-ce28-44dc-8d9d-ce7d8e4aabcd"
      },
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='Hello! How can I assist you today?')"
            ]
          },
          "metadata": {},
          "execution_count": 106
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### `Tagging and Extraction`\n",
        "`Allows to extract structured data from unstructured text`"
      ],
      "metadata": {
        "id": "PEDeYcqqUYU7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List\n",
        "from pydantic import BaseModel, Field\n",
        "from langchain.utils.openai_functions import convert_pydantic_to_openai_function"
      ],
      "metadata": {
        "id": "DkuK5qCKUOwp"
      },
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### `Tagging`"
      ],
      "metadata": {
        "id": "sdKQ_JXUc3a9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Tagging(BaseModel):\n",
        "    \"\"\"Tag the piece of text with particular info.\"\"\"\n",
        "    sentiment: str = Field(description=\"sentiment of text, should be `pos`, `neg` or `neutral`\") # through the tags (i.e. `pos`, `neg`, `neutral`) we're telling the LLM how the data should be structured,\n",
        "    language: str = Field(description=\"language of text (should be ISO 639-1 code)\")"
      ],
      "metadata": {
        "id": "4uRoUvYObXj0"
      },
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "convert_pydantic_to_openai_function(Tagging)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y6Zeu66JbqHW",
        "outputId": "a5e9935b-0711-4f0f-cbb8-c0098ed45d4d"
      },
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'name': 'Tagging',\n",
              " 'description': 'Tag the piece of text with particular info.',\n",
              " 'parameters': {'title': 'Tagging',\n",
              "  'description': 'Tag the piece of text with particular info.',\n",
              "  'type': 'object',\n",
              "  'properties': {'sentiment': {'title': 'Sentiment',\n",
              "    'description': 'sentiment of text, should be `pos`, `neg` or `neutral`',\n",
              "    'type': 'string'},\n",
              "   'language': {'title': 'Language',\n",
              "    'description': 'language of text (should be ISO 639-1 code)',\n",
              "    'type': 'string'}},\n",
              "  'required': ['sentiment', 'language']}}"
            ]
          },
          "metadata": {},
          "execution_count": 109
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.chat_models import ChatOpenAI"
      ],
      "metadata": {
        "id": "as63BtVebsII"
      },
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = ChatOpenAI(temperature=0.)"
      ],
      "metadata": {
        "id": "nOhhx72vb9jQ"
      },
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tagging_functions = [convert_pydantic_to_openai_function(Tagging)]"
      ],
      "metadata": {
        "id": "dkLEPtfHb_o3"
      },
      "execution_count": 112,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"Think carefully, and then tag the text as instructed\"),\n",
        "    (\"user\", \"{input}\")\n",
        "])"
      ],
      "metadata": {
        "id": "G34VtvzucB9S"
      },
      "execution_count": 113,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_with_functions = model.bind(\n",
        "    functions=tagging_functions,\n",
        "    function_call={\"name\": \"Tagging\"}\n",
        ")"
      ],
      "metadata": {
        "id": "gLwK254-cJ_z"
      },
      "execution_count": 114,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tagging_chain = prompt | model_with_functions"
      ],
      "metadata": {
        "id": "cCCq5jWQcRxv"
      },
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tagging_chain.invoke({\"input\": \"I love langchain\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MBBeNvKMcU0c",
        "outputId": "cd19532e-a614-422d-c8d9-6bc0c7bae6db"
      },
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{\\n  \"sentiment\": \"pos\",\\n  \"language\": \"en\"\\n}', 'name': 'Tagging'}})"
            ]
          },
          "metadata": {},
          "execution_count": 116
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tagging_chain.invoke({\"input\": \"no mi piace questo cibo\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wLn9s-F_cYlk",
        "outputId": "f8258f20-67c8-44d0-effc-7a85c7135b24"
      },
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{\\n  \"sentiment\": \"neg\",\\n  \"language\": \"it\"\\n}', 'name': 'Tagging'}})"
            ]
          },
          "metadata": {},
          "execution_count": 117
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.output_parsers.openai_functions import JsonOutputFunctionsParser"
      ],
      "metadata": {
        "id": "mbUFW8Qwcqj2"
      },
      "execution_count": 118,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tagging_chain = prompt | model_with_functions | JsonOutputFunctionsParser()"
      ],
      "metadata": {
        "id": "qBffyqITce2j"
      },
      "execution_count": 119,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tagging_chain.invoke({\"input\": \"non mi piace questo cibo\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jEibUJHZcvsd",
        "outputId": "bd8e5ba6-5293-4011-b254-8e73416314c0"
      },
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'sentiment': 'neg', 'language': 'it'}"
            ]
          },
          "metadata": {},
          "execution_count": 120
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### `Extraction`\n",
        "`Extraction is similar to tagging, but used for extracting multiple piece of information`"
      ],
      "metadata": {
        "id": "qJEmbYayc5Uj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Optional\n",
        "class Person(BaseModel):\n",
        "    \"\"\"Information about a Person.\"\"\"\n",
        "    name: str = Field(description=\"person's name\")\n",
        "    age: Optional[int] = Field(description=\"person's age\")"
      ],
      "metadata": {
        "id": "DU8nYwYXc0FY"
      },
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Information(BaseModel):\n",
        "    \"\"\"Information to extract\"\"\"\n",
        "    people: List[Person] = Field(description=\"List of info about people\")"
      ],
      "metadata": {
        "id": "Td8q36AFdavJ"
      },
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "convert_pydantic_to_openai_function(Information)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WWuxN5_jduR9",
        "outputId": "7f087e66-8c3c-4dad-a32f-6bffc449865f"
      },
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'name': 'Information',\n",
              " 'description': 'Information to extract',\n",
              " 'parameters': {'title': 'Information',\n",
              "  'description': 'Information to extract',\n",
              "  'type': 'object',\n",
              "  'properties': {'people': {'title': 'People',\n",
              "    'description': 'List of info about people',\n",
              "    'type': 'array',\n",
              "    'items': {'title': 'Person',\n",
              "     'description': 'Information about a Person.',\n",
              "     'type': 'object',\n",
              "     'properties': {'name': {'title': 'Name',\n",
              "       'description': \"person's name\",\n",
              "       'type': 'string'},\n",
              "      'age': {'title': 'Age',\n",
              "       'description': \"person's age\",\n",
              "       'type': 'integer'}},\n",
              "     'required': ['name']}}},\n",
              "  'required': ['people']}}"
            ]
          },
          "metadata": {},
          "execution_count": 123
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "extraction_functions = [convert_pydantic_to_openai_function(Information)]\n",
        "extraction_model = model.bind(functions=extraction_functions, function_call={\"name\": \"Information\"})"
      ],
      "metadata": {
        "id": "-4MrEN_PdwiP"
      },
      "execution_count": 124,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "extraction_model.invoke(\"Joe is 30, his mom is Martha\") # model seems to think when it doenot know a person's age, then \"age=0\", but we can do better!"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YckXvY1Pd8tK",
        "outputId": "d3372954-e54c-4bdd-b6d7-b932c555bcc0"
      },
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{\\n  \"people\": [\\n    {\\n      \"name\": \"Joe\",\\n      \"age\": 30\\n    },\\n    {\\n      \"name\": \"Martha\",\\n      \"age\": 0\\n    }\\n  ]\\n}', 'name': 'Information'}})"
            ]
          },
          "metadata": {},
          "execution_count": 125
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Note: below prompt makes sure via `system-prior` that when a particular\n",
        "# attribute value is unknown, then ignore that attribute for that instance.\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"Extract the relevant information, if not explicitly provided do not guess. Extract partial info\"),\n",
        "    (\"user\", \"{input}\")\n",
        "])"
      ],
      "metadata": {
        "id": "JocGGU5feCw_"
      },
      "execution_count": 126,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "extraction_chain = prompt | extraction_model"
      ],
      "metadata": {
        "id": "BcKJwLzievJ3"
      },
      "execution_count": 127,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "extraction_chain.invoke({\"input\": \"Joe is 30, his mom is Martha\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XQEWbzEleyYZ",
        "outputId": "2d9318dc-0e15-4bb1-e61d-4d5a7a199658"
      },
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{\\n  \"people\": [\\n    {\\n      \"name\": \"Joe\",\\n      \"age\": 30\\n    },\\n    {\\n      \"name\": \"Martha\"\\n    }\\n  ]\\n}', 'name': 'Information'}})"
            ]
          },
          "metadata": {},
          "execution_count": 128
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "extraction_chain = prompt | extraction_model | JsonOutputFunctionsParser()"
      ],
      "metadata": {
        "id": "K_nxWcrVe3Zz"
      },
      "execution_count": 129,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Note: considering from the standpoint of `Extraction procedure`, we do not require\n",
        "# the `people` item to be specified, what we want is just a list of dictionaries\n",
        "# constituting the function calls\n",
        "extraction_chain.invoke({\"input\": \"Joe is 30, his mom is Martha\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PsagjXBHfBvx",
        "outputId": "bcd3f0bc-22f5-4118-8039-49821ef97cc6"
      },
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'people': [{'name': 'Joe', 'age': 30}, {'name': 'Martha'}]}"
            ]
          },
          "metadata": {},
          "execution_count": 130
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# looks for particular key in the output and extract only that\n",
        "from langchain.output_parsers.openai_functions import JsonKeyOutputFunctionsParser"
      ],
      "metadata": {
        "id": "g5QX3lPSfIb-"
      },
      "execution_count": 131,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "extraction_chain = prompt | extraction_model | JsonKeyOutputFunctionsParser(key_name=\"people\")"
      ],
      "metadata": {
        "id": "Vapn-_gmgANM"
      },
      "execution_count": 132,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "extraction_chain.invoke({\"input\": \"Joe is 30, his mom is Martha\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l0uvjx0KgDSP",
        "outputId": "f68c1782-212f-4102-feb8-90b8dcf2c113"
      },
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'name': 'Joe', 'age': 30}, {'name': 'Martha'}]"
            ]
          },
          "metadata": {},
          "execution_count": 133
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### `Doing it for real`\n",
        "`We can apply tagging to a larger body of text.`\n",
        "`For example, let's load this blog post and extract tag information from a sub-set of the text`"
      ],
      "metadata": {
        "id": "CLzwTR5JgUAb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import WebBaseLoader\n",
        "loader = WebBaseLoader(\"https://lilianweng.github.io/posts/2023-06-23-agent/\")\n",
        "documents = loader.load()"
      ],
      "metadata": {
        "id": "9kIYHNdmgKxN"
      },
      "execution_count": 134,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc = documents[0]"
      ],
      "metadata": {
        "id": "8h7Dnye9jx2J"
      },
      "execution_count": 135,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "page_content = doc.page_content[:10000]\n",
        "print(page_content[:1000])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jtHIiOhpj4CZ",
        "outputId": "f67847a0-27fd-48b1-efeb-32ae55b75d81"
      },
      "execution_count": 136,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "LLM Powered Autonomous Agents | Lil'Log\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Lil'Log\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Posts\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Archive\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Search\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Tags\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "FAQ\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "emojisearch.app\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "      LLM Powered Autonomous Agents\n",
            "    \n",
            "Date: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian Weng\n",
            "\n",
            "\n",
            " \n",
            "\n",
            "\n",
            "Table of Contents\n",
            "\n",
            "\n",
            "\n",
            "Agent System Overview\n",
            "\n",
            "Component One: Planning\n",
            "\n",
            "Task Decomposition\n",
            "\n",
            "Self-Reflection\n",
            "\n",
            "\n",
            "Component Two: Memory\n",
            "\n",
            "Types of Memory\n",
            "\n",
            "Maximum Inner Product Search (MIPS)\n",
            "\n",
            "\n",
            "Component Three: Tool Use\n",
            "\n",
            "Case Studies\n",
            "\n",
            "Scientific Discovery Agent\n",
            "\n",
            "Generative Agents Simulation\n",
            "\n",
            "Proof-of-Concept Examples\n",
            "\n",
            "\n",
            "Challenges\n",
            "\n",
            "Citation\n",
            "\n",
            "References\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Overview(BaseModel):\n",
        "    \"\"\"Overview of a section of text.\"\"\"\n",
        "    summary: str = Field(description=\"Provide a concise summary of the content.\")\n",
        "    language: str = Field(description=\"Provide the language that the content is written in.\")\n",
        "    keywords: str = Field(description=\"Provide keywords related to the content.\")"
      ],
      "metadata": {
        "id": "jYpBJAvBj7fJ"
      },
      "execution_count": 137,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "overview_tagging_functions = [\n",
        "    convert_pydantic_to_openai_function(Overview)\n",
        "]\n",
        "tagging_model = model.bind(\n",
        "    functions=overview_tagging_functions,\n",
        "    function_call={\"name\": \"Overview\"}\n",
        ")\n",
        "tagging_chain = prompt | tagging_model | JsonOutputFunctionsParser()"
      ],
      "metadata": {
        "id": "8r-pjVqFka7K"
      },
      "execution_count": 138,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tagging_chain.invoke({\"input\": page_content})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sRxsDvkmkqpt",
        "outputId": "0820e1c6-6491-4c21-bf08-63fff619d625"
      },
      "execution_count": 139,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'summary': 'This article discusses the concept of building autonomous agents powered by LLM (large language model) as their core controller. It explores the key components of such agent systems, including planning, memory, and tool use. It also covers various techniques for task decomposition and self-reflection in autonomous agents. The article provides examples of case studies and challenges in implementing LLM-powered agents.',\n",
              " 'language': 'English',\n",
              " 'keywords': 'LLM, autonomous agents, planning, memory, tool use, task decomposition, self-reflection, case studies, challenges'}"
            ]
          },
          "metadata": {},
          "execution_count": 139
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Paper(BaseModel):\n",
        "    \"\"\"Information about papers mentioned.\"\"\"\n",
        "    title: str\n",
        "    author: Optional[str]\n",
        "\n",
        "class Info(BaseModel):\n",
        "    \"\"\"Information to extract.\"\"\"\n",
        "    papers: List[Paper]"
      ],
      "metadata": {
        "id": "sLugzp2UktZB"
      },
      "execution_count": 140,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "paper_extraction_function = [\n",
        "    convert_pydantic_to_openai_function(Info)\n",
        "]\n",
        "extraction_model = model.bind(\n",
        "    functions=paper_extraction_function,\n",
        "    function_call={\"name\": \"Info\"}\n",
        ")\n",
        "extraction_chain = prompt | extraction_model | JsonKeyOutputFunctionsParser(key_name=\"papers\")"
      ],
      "metadata": {
        "id": "SwiqKca2lIIw"
      },
      "execution_count": 141,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "extraction_chain.invoke({\"input\": page_content})\n",
        "# this is actually title we're passing in, we don't want that, it should rather extract the papers mentioned within."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lCYP-5w6lm8C",
        "outputId": "686c9ce5-2aa1-4e1a-968d-48f469ec083f"
      },
      "execution_count": 142,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'title': 'LLM Powered Autonomous Agents', 'author': 'Lilian Weng'}]"
            ]
          },
          "metadata": {},
          "execution_count": 142
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "template = \"\"\"A article will be passed to you. Extract from it all papers that are mentioned by this article.\n",
        "\n",
        "Do not extract the name of the article itself. If no papers are mentioned that's fine - you don't need to extract any! Just return an empty list.\n",
        "\n",
        "Do not make up or guess ANY extra information. Only extract what exactly is in the text.\"\"\"\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", template),\n",
        "    (\"human\", \"{input}\")\n",
        "])"
      ],
      "metadata": {
        "id": "wEs4ykUllVFt"
      },
      "execution_count": 143,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "extraction_chain = prompt | extraction_model | JsonKeyOutputFunctionsParser(key_name=\"papers\")"
      ],
      "metadata": {
        "id": "ZQoygKbYlkJA"
      },
      "execution_count": 144,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "extraction_chain.invoke({\"input\": page_content})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lAnnviLyl7O6",
        "outputId": "27ac0ad4-d979-4361-8afc-852f4341c5c0"
      },
      "execution_count": 145,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'title': 'Chain of thought (CoT; Wei et al. 2022)', 'author': 'Wei et al.'},\n",
              " {'title': 'Tree of Thoughts (Yao et al. 2023)', 'author': 'Yao et al.'},\n",
              " {'title': 'LLM+P (Liu et al. 2023)', 'author': 'Liu et al.'},\n",
              " {'title': 'ReAct (Yao et al. 2023)', 'author': 'Yao et al.'},\n",
              " {'title': 'Reflexion (Shinn & Labash 2023)', 'author': 'Shinn & Labash'},\n",
              " {'title': 'Chain of Hindsight (CoH; Liu et al. 2023)',\n",
              "  'author': 'Liu et al.'},\n",
              " {'title': 'Algorithm Distillation (AD; Laskin et al. 2023)',\n",
              "  'author': 'Laskin et al.'}]"
            ]
          },
          "metadata": {},
          "execution_count": 145
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "extraction_chain.invoke({\"input\": \"hi\"}) # based on the instruction from prompt `return empty list`"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "spl3i8Vfl8pq",
        "outputId": "d90737d2-e4b1-4be1-cf88-f54c7ff78061"
      },
      "execution_count": 146,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {},
          "execution_count": 146
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# what is we want to extract all the papers mentioned in the whole article.\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_overlap=0)"
      ],
      "metadata": {
        "id": "GwbOlg5umStN"
      },
      "execution_count": 147,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "splits = text_splitter.split_text(doc.page_content)"
      ],
      "metadata": {
        "id": "qpZTLIoWmpEr"
      },
      "execution_count": 148,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(splits)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hsRP6nmjmsaz",
        "outputId": "c985966f-6cb1-485e-e49f-feeb474ef639"
      },
      "execution_count": 149,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "14"
            ]
          },
          "metadata": {},
          "execution_count": 149
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Given the above proc., we are gonna try to automatize the above proc. + extraction + output-parsing\n",
        "# i. We're gonna take the doc.page_content and split it up into chunks\n",
        "# ii. pass individual split to the extraction chain defined above\n",
        "# iii. finally, join all the results"
      ],
      "metadata": {
        "id": "SIH9GSY8mtqt"
      },
      "execution_count": 150,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def flatten(matrix):\n",
        "    flat_list = []\n",
        "    for row in matrix:\n",
        "        flat_list += row\n",
        "    return flat_list"
      ],
      "metadata": {
        "id": "LJbx94DInVJD"
      },
      "execution_count": 151,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "flatten([[1, 2], [3, 4]])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4mv2x42VnY51",
        "outputId": "94b94397-fa50-4e96-e4f1-bc18a967eb7c"
      },
      "execution_count": 152,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1, 2, 3, 4]"
            ]
          },
          "metadata": {},
          "execution_count": 152
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# i. (a) we need to create some-method of split the page-content and passing it into the chain.\n",
        "# for that we require each split (i.e. a string chunk) to be fed as dictionary with the \"input\" key-value\n",
        "from langchain.schema.runnable import RunnableLambda"
      ],
      "metadata": {
        "id": "5oZEyQIFnbMY"
      },
      "execution_count": 153,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# RunnableLambda is simple wrapper in LangChain that takes in a lambda function and converts to Runnable object\n",
        "prep = RunnableLambda(\n",
        "    lambda x: [{\"input\": doc_chunk} for doc_chunk in text_splitter.split_text(x)]\n",
        ")"
      ],
      "metadata": {
        "id": "VLNebKfencYg"
      },
      "execution_count": 154,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prep.invoke(\"hi\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Q8zZOqMoAlE",
        "outputId": "89cd77a2-c20e-457e-8c67-4a9ecabc7323"
      },
      "execution_count": 155,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'input': 'hi'}]"
            ]
          },
          "metadata": {},
          "execution_count": 155
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# NOTE: extraction_chain operates on single, and here we have list of elements,\n",
        "# .map() -> take previous input (i.e. list) and map the chain (i.e. extraction_chain) over them.\n",
        "# Additionally, since `flatten` is not the first component so not compulsory to wrap it into a\n",
        "# RunnableLambda (but we can its optional, works either way)\n",
        "chain = prep | extraction_chain.map() | flatten"
      ],
      "metadata": {
        "id": "0SFZaIfCoBbZ"
      },
      "execution_count": 156,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# chain.invoke(doc.page_content)"
      ],
      "metadata": {
        "id": "O9shV-jYo3JA"
      },
      "execution_count": 159,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### `Tools and Routing`"
      ],
      "metadata": {
        "id": "EXegzhKYpQ4l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- `Create your own tools`\n",
        "- `Build a tool based on an OpenAPI spec`\n",
        "    - `Predating LLMs, the OpenAPI specification is routinely used by servie providers to describe the APIs.`\n",
        "- `Select from multiple possible tools - called \"rounting\"`"
      ],
      "metadata": {
        "id": "6RpcgjG8ADgz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# tool is a decorator from langchain, which can be put on top of a function\n",
        "# which will automatically convert the function into a langchain tool.\n",
        "from langchain.agents import tool"
      ],
      "metadata": {
        "id": "PnpYhcP9o4bT"
      },
      "execution_count": 160,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@tool\n",
        "def search(query: str) -> str:\n",
        "    \"\"\"\"Search for weather online\"\"\"\n",
        "    return \"42f\""
      ],
      "metadata": {
        "id": "5SJsTOdiqewH"
      },
      "execution_count": 161,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "search.name"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "nuaIfxGGqoNR",
        "outputId": "bbe3665f-c02d-4334-f855-2e3507468352"
      },
      "execution_count": 162,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'search'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 162
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "search.description"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "JwgGUL6Bqo2C",
        "outputId": "9f82462f-fec1-44d0-b551-0f7cb76bed82"
      },
      "execution_count": 163,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'search(query: str) -> str - \"Search for weather online'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 163
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "search.args"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7iu6cKXCqpzE",
        "outputId": "67f4e73d-2077-4638-f5a2-a099953db265"
      },
      "execution_count": 164,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'query': {'title': 'Query', 'type': 'string'}}"
            ]
          },
          "metadata": {},
          "execution_count": 164
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# we can improve upon this by defining an explicit structure for input schema\n",
        "# because, the description of the input is what LLM uses to determine what the input should be.\n",
        "from pydantic import BaseModel, Field\n",
        "class SearchInput(BaseModel):\n",
        "    query: str = Field(description=\"Thing to search for\")"
      ],
      "metadata": {
        "id": "TQI8rPB5qqba"
      },
      "execution_count": 165,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@tool(args_schema=SearchInput)\n",
        "def search(query: str) -> str:\n",
        "    \"\"\"Search for the weather online.\"\"\"\n",
        "    return \"42f\""
      ],
      "metadata": {
        "id": "Gamg4kBUrITT"
      },
      "execution_count": 166,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "search.args"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gc6PMzqDrRM7",
        "outputId": "898eaacb-0507-4d9a-c89d-05bb15bd88f7"
      },
      "execution_count": 167,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'query': {'title': 'Query',\n",
              "  'description': 'Thing to search for',\n",
              "  'type': 'string'}}"
            ]
          },
          "metadata": {},
          "execution_count": 167
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# still callable!\n",
        "search.run(\"sf\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "pH2vbSBlrSiu",
        "outputId": "4bcf43ce-7c20-4f06-a9a8-e7a6f9dbebc6"
      },
      "execution_count": 168,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'42f'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 168
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# i. create a tool which gets the temperature, given the latitude and longitude."
      ],
      "metadata": {
        "id": "wE5eRRrY0PQ8"
      },
      "execution_count": 169,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from pydantic import BaseModel, Field\n",
        "import datetime\n",
        "\n",
        "# define the input schema\n",
        "# NOTE: `...` is used to specific that the field is required\n",
        "class OpenMeteoInput(BaseModel):\n",
        "    latitude: float = Field(..., description=\"Latitude of the location to fetch weather data for\")\n",
        "    longitude: float = Field(..., description=\"Longitude of the location to fetch weather data for\")\n",
        "\n",
        "@tool(args_schema=OpenMeteoInput)\n",
        "def get_current_temperature(latitude: float, longitude: float) -> dict:\n",
        "    \"\"\"Fetch current temperature for given coordinates.\"\"\"\n",
        "\n",
        "    BASE_URL = \"https://api.open-meteo.com/v1/forecast\"\n",
        "\n",
        "    # parameters for the request\n",
        "    params = {\n",
        "        \"latitude\": latitude,\n",
        "        \"longitude\": longitude,\n",
        "        \"hourly\": \"temperature_2m\",\n",
        "        \"forecast_days\": 1\n",
        "    }\n",
        "\n",
        "    # make the request\n",
        "    response = requests.get(BASE_URL, params=params)\n",
        "\n",
        "    if response.status_code == 200:\n",
        "        results = response.json()\n",
        "    else:\n",
        "        raise Exception(f\"API Request failed with status code: {response.status_code}\")\n",
        "\n",
        "    current_utc_time = datetime.datetime.utcnow()\n",
        "    time_list = [datetime.datetime.fromisoformat(time_str.replace('Z', '+00:00')) for time_str in results['hourly']['time']]\n",
        "    temperature_list = results[\"hourly\"][\"temperature_2m\"]\n",
        "\n",
        "    closest_time_index = min(range(len(time_list)), key=lambda i: abs(time_list[i] - current_utc_time))\n",
        "    current_temperature = temperature_list[closest_time_index]\n",
        "\n",
        "    return f\"The current temperature is {current_temperature}°C\""
      ],
      "metadata": {
        "id": "kft0oGYm02RX"
      },
      "execution_count": 170,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_current_temperature.name"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "MxZb_hOV2kQb",
        "outputId": "e696fc43-7576-4594-fc47-48b6b91ab979"
      },
      "execution_count": 171,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'get_current_temperature'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 171
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "get_current_temperature.description"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "Augno9rF2ord",
        "outputId": "0722214d-e457-4a49-e4ee-e16cf152c350"
      },
      "execution_count": 172,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'get_current_temperature(latitude: float, longitude: float) -> dict - Fetch current temperature for given coordinates.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 172
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "get_current_temperature.args"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "21OE-_7K2p8K",
        "outputId": "3f8df363-404f-49e2-87db-d5a3a2cc7807"
      },
      "execution_count": 173,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'latitude': {'title': 'Latitude',\n",
              "  'description': 'Latitude of the location to fetch weather data for',\n",
              "  'type': 'number'},\n",
              " 'longitude': {'title': 'Longitude',\n",
              "  'description': 'Longitude of the location to fetch weather data for',\n",
              "  'type': 'number'}}"
            ]
          },
          "metadata": {},
          "execution_count": 173
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# we can convert the tool into the exact OpenAI required JSON blob for function definition\n",
        "from langchain.tools.render import format_tool_to_openai_function"
      ],
      "metadata": {
        "id": "Jh4wvdWf2sNt"
      },
      "execution_count": 174,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "format_tool_to_openai_function(get_current_temperature)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NrFCb1Du3Eb_",
        "outputId": "68c56076-59ac-4009-a40b-46f988e43049"
      },
      "execution_count": 175,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'name': 'get_current_temperature',\n",
              " 'description': 'get_current_temperature(latitude: float, longitude: float) -> dict - Fetch current temperature for given coordinates.',\n",
              " 'parameters': {'title': 'OpenMeteoInput',\n",
              "  'type': 'object',\n",
              "  'properties': {'latitude': {'title': 'Latitude',\n",
              "    'description': 'Latitude of the location to fetch weather data for',\n",
              "    'type': 'number'},\n",
              "   'longitude': {'title': 'Longitude',\n",
              "    'description': 'Longitude of the location to fetch weather data for',\n",
              "    'type': 'number'}},\n",
              "  'required': ['latitude', 'longitude']}}"
            ]
          },
          "metadata": {},
          "execution_count": 175
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# this tool is callable\n",
        "get_current_temperature({\n",
        "    \"latitude\": 13,\n",
        "    \"longitude\": 14\n",
        "})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "TfYEdLtD3S5V",
        "outputId": "9fe6ffb5-013d-4ace-b514-e1d41fdeb7cc"
      },
      "execution_count": 176,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The current temperature is 21.5°C'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 176
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ii. create a wikipedia tool and it can search things on wikipedia"
      ],
      "metadata": {
        "id": "aB_kgzed3Y5y"
      },
      "execution_count": 177,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import wikipedia\n",
        "@tool\n",
        "def search_wikipedia(query: str) -> str:\n",
        "    \"\"\"Run Wikipedia search and get page summaries\"\"\"\n",
        "    page_titles = wikipedia.search(query)\n",
        "    summaries = []\n",
        "    for page_title in page_titles[:3]:\n",
        "        try:\n",
        "            wiki_page = wikipedia.page(title=page_title, auto_suggest=False)\n",
        "            summaries.append(f\"Page: {page_title}\\nSummary: {wiki_page.summary}\")\n",
        "        except (\n",
        "            self.wiki_client.exceptions.PageError,\n",
        "            self.wiki_client.exceptions.DisambiguationError,\n",
        "        ):\n",
        "            pass\n",
        "    if not summaries:\n",
        "        return \"No good Wikipedia Search Result was found\"\n",
        "    return \"\\n\\n\".join(summaries)"
      ],
      "metadata": {
        "id": "cWA4mf4v3nkY"
      },
      "execution_count": 178,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "search_wikipedia.name"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "Lr-Lcuav3ofw",
        "outputId": "8cf9006c-a944-4321-ee5b-eb758ee21107"
      },
      "execution_count": 179,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'search_wikipedia'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 179
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "search_wikipedia.description"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "_p1pJTzG4fsU",
        "outputId": "2b05aaa5-b2cd-426b-fc09-b1bef41126e4"
      },
      "execution_count": 180,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'search_wikipedia(query: str) -> str - Run Wikipedia search and get page summaries'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 180
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "search_wikipedia.args"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xiw1hRsM4hBO",
        "outputId": "e263b99d-12fb-4895-f17e-e763aa5a3225"
      },
      "execution_count": 181,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'query': {'title': 'Query', 'type': 'string'}}"
            ]
          },
          "metadata": {},
          "execution_count": 181
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "format_tool_to_openai_function(search_wikipedia)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c1XvKbHU4h9w",
        "outputId": "c4a03843-00c8-4e83-929d-d71139e75190"
      },
      "execution_count": 182,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'name': 'search_wikipedia',\n",
              " 'description': 'search_wikipedia(query: str) -> str - Run Wikipedia search and get page summaries',\n",
              " 'parameters': {'title': 'search_wikipediaSchemaSchema',\n",
              "  'type': 'object',\n",
              "  'properties': {'query': {'title': 'Query', 'type': 'string'}},\n",
              "  'required': ['query']}}"
            ]
          },
          "metadata": {},
          "execution_count": 182
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "search_wikipedia({\"query\": \"langchain\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145
        },
        "id": "2SoPxZIJ4kfN",
        "outputId": "6734b995-2bd6-484d-c02f-9a2f18e33e15"
      },
      "execution_count": 183,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Page: LangChain\\nSummary: LangChain is a framework designed to simplify the creation of applications using large language models (LLMs). As a language model integration framework, LangChain\\'s use-cases largely overlap with those of language models in general, including document analysis and summarization, chatbots, and code analysis.\\n\\nPage: OpenAI\\nSummary: OpenAI is an American artificial intelligence (AI) research organization consisting of the non-profit OpenAI, Inc. registered in Delaware and its for-profit subsidiary OpenAI Global, LLC. OpenAI researches artificial intelligence with the declared intention of developing \"safe and beneficial\" artificial general intelligence, which it defines as \"highly autonomous systems that outperform humans at most economically valuable work\". OpenAI has also developed several large language models, such as ChatGPT and GPT-4, as well as advanced image generation models like DALL-E 3, and in the past published open-source models.The organization was founded in December 2015 by Ilya Sutskever, Greg Brockman, Trevor Blackwell, Vicki Cheung, Andrej Karpathy, Durk Kingma, Jessica Livingston, John Schulman, Pamela Vagata, and Wojciech Zaremba, with Sam Altman and Elon Musk serving as the initial board members. Microsoft provided OpenAI Global LLC with a $1 billion investment in 2019 and a $10 billion investment in 2023.On November 17, 2023, the board removed Altman as CEO, while Brockman was removed as chairman and then resigned as president. Four days later, both returned after negotiations with the board, and most of the board members resigned. The new initial board included former Salesforce co-CEO Bret Taylor as chairman.\\n\\nPage: Prompt engineering\\nSummary: Prompt engineering is the process of structuring text that can be interpreted and understood by a generative AI model. A prompt is natural language text describing the task that an AI should perform.A prompt for a text-to-text model can be a query such as \"what is Fermat\\'s little theorem?\", a command such as \"write a poem about leaves falling\", a short statement of feedback (for example, \"too verbose\", \"too formal\", \"rephrase again\", \"omit this word\") or a longer statement including context, instructions, and input data. Prompt engineering may involve phrasing a query, specifying a style, providing relevant context or assigning a role to the AI such as \"Act as a native French speaker\". A prompt may include a few examples for a model to learn from, such as \"maison -> house, chat -> cat, chien -> dog\", an approach called few-shot learning.When communicating with a text-to-image or a text-to-audio model, a typical prompt is a description of a desired output such as \"a high-quality photo of an astronaut riding a horse\" or \"Lo-fi slow BPM electro chill with organic samples\". Prompting a text-to-image model may involve adding, removing, emphasizing and re-ordering words to achieve a desired subject, style, layout, lighting, and aesthetic.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 183
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# previously, we have created function (defined them) and then created OpenAI function definitions\n",
        "# for those functions.\n",
        "# Often time, function which we want to interact with are exposed through APIs and often times, APIs\n",
        "# a specific specification for the inputs and outputs called as OpenAPI Specification.\n",
        "\n",
        "# Below, we take a example OpenAPI Sepcification and convert it into a OpenAI function definitions"
      ],
      "metadata": {
        "id": "KxxgY03A47qW"
      },
      "execution_count": 184,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains.openai_functions.openapi import openapi_spec_to_openai_fn # openapispec -> openai_fn\n",
        "from langchain.utilities.openapi import OpenAPISpec # used to load OpenAPISpec in the first place"
      ],
      "metadata": {
        "id": "Lu6tE10L5uo9"
      },
      "execution_count": 185,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"\"\"\n",
        "{\n",
        "  \"openapi\": \"3.0.0\",\n",
        "  \"info\": {\n",
        "    \"version\": \"1.0.0\",\n",
        "    \"title\": \"Swagger Petstore\",\n",
        "    \"license\": {\n",
        "      \"name\": \"MIT\"\n",
        "    }\n",
        "  },\n",
        "  \"servers\": [\n",
        "    {\n",
        "      \"url\": \"http://petstore.swagger.io/v1\"\n",
        "    }\n",
        "  ],\n",
        "  \"paths\": {\n",
        "    \"/pets\": {\n",
        "      \"get\": {\n",
        "        \"summary\": \"List all pets\",\n",
        "        \"operationId\": \"listPets\",\n",
        "        \"tags\": [\n",
        "          \"pets\"\n",
        "        ],\n",
        "        \"parameters\": [\n",
        "          {\n",
        "            \"name\": \"limit\",\n",
        "            \"in\": \"query\",\n",
        "            \"description\": \"How many items to return at one time (max 100)\",\n",
        "            \"required\": false,\n",
        "            \"schema\": {\n",
        "              \"type\": \"integer\",\n",
        "              \"maximum\": 100,\n",
        "              \"format\": \"int32\"\n",
        "            }\n",
        "          }\n",
        "        ],\n",
        "        \"responses\": {\n",
        "          \"200\": {\n",
        "            \"description\": \"A paged array of pets\",\n",
        "            \"headers\": {\n",
        "              \"x-next\": {\n",
        "                \"description\": \"A link to the next page of responses\",\n",
        "                \"schema\": {\n",
        "                  \"type\": \"string\"\n",
        "                }\n",
        "              }\n",
        "            },\n",
        "            \"content\": {\n",
        "              \"application/json\": {\n",
        "                \"schema\": {\n",
        "                  \"$ref\": \"#/components/schemas/Pets\"\n",
        "                }\n",
        "              }\n",
        "            }\n",
        "          },\n",
        "          \"default\": {\n",
        "            \"description\": \"unexpected error\",\n",
        "            \"content\": {\n",
        "              \"application/json\": {\n",
        "                \"schema\": {\n",
        "                  \"$ref\": \"#/components/schemas/Error\"\n",
        "                }\n",
        "              }\n",
        "            }\n",
        "          }\n",
        "        }\n",
        "      },\n",
        "      \"post\": {\n",
        "        \"summary\": \"Create a pet\",\n",
        "        \"operationId\": \"createPets\",\n",
        "        \"tags\": [\n",
        "          \"pets\"\n",
        "        ],\n",
        "        \"responses\": {\n",
        "          \"201\": {\n",
        "            \"description\": \"Null response\"\n",
        "          },\n",
        "          \"default\": {\n",
        "            \"description\": \"unexpected error\",\n",
        "            \"content\": {\n",
        "              \"application/json\": {\n",
        "                \"schema\": {\n",
        "                  \"$ref\": \"#/components/schemas/Error\"\n",
        "                }\n",
        "              }\n",
        "            }\n",
        "          }\n",
        "        }\n",
        "      }\n",
        "    },\n",
        "    \"/pets/{petId}\": {\n",
        "      \"get\": {\n",
        "        \"summary\": \"Info for a specific pet\",\n",
        "        \"operationId\": \"showPetById\",\n",
        "        \"tags\": [\n",
        "          \"pets\"\n",
        "        ],\n",
        "        \"parameters\": [\n",
        "          {\n",
        "            \"name\": \"petId\",\n",
        "            \"in\": \"path\",\n",
        "            \"required\": true,\n",
        "            \"description\": \"The id of the pet to retrieve\",\n",
        "            \"schema\": {\n",
        "              \"type\": \"string\"\n",
        "            }\n",
        "          }\n",
        "        ],\n",
        "        \"responses\": {\n",
        "          \"200\": {\n",
        "            \"description\": \"Expected response to a valid request\",\n",
        "            \"content\": {\n",
        "              \"application/json\": {\n",
        "                \"schema\": {\n",
        "                  \"$ref\": \"#/components/schemas/Pet\"\n",
        "                }\n",
        "              }\n",
        "            }\n",
        "          },\n",
        "          \"default\": {\n",
        "            \"description\": \"unexpected error\",\n",
        "            \"content\": {\n",
        "              \"application/json\": {\n",
        "                \"schema\": {\n",
        "                  \"$ref\": \"#/components/schemas/Error\"\n",
        "                }\n",
        "              }\n",
        "            }\n",
        "          }\n",
        "        }\n",
        "      }\n",
        "    }\n",
        "  },\n",
        "  \"components\": {\n",
        "    \"schemas\": {\n",
        "      \"Pet\": {\n",
        "        \"type\": \"object\",\n",
        "        \"required\": [\n",
        "          \"id\",\n",
        "          \"name\"\n",
        "        ],\n",
        "        \"properties\": {\n",
        "          \"id\": {\n",
        "            \"type\": \"integer\",\n",
        "            \"format\": \"int64\"\n",
        "          },\n",
        "          \"name\": {\n",
        "            \"type\": \"string\"\n",
        "          },\n",
        "          \"tag\": {\n",
        "            \"type\": \"string\"\n",
        "          }\n",
        "        }\n",
        "      },\n",
        "      \"Pets\": {\n",
        "        \"type\": \"array\",\n",
        "        \"maxItems\": 100,\n",
        "        \"items\": {\n",
        "          \"$ref\": \"#/components/schemas/Pet\"\n",
        "        }\n",
        "      },\n",
        "      \"Error\": {\n",
        "        \"type\": \"object\",\n",
        "        \"required\": [\n",
        "          \"code\",\n",
        "          \"message\"\n",
        "        ],\n",
        "        \"properties\": {\n",
        "          \"code\": {\n",
        "            \"type\": \"integer\",\n",
        "            \"format\": \"int32\"\n",
        "          },\n",
        "          \"message\": {\n",
        "            \"type\": \"string\"\n",
        "          }\n",
        "        }\n",
        "      }\n",
        "    }\n",
        "  }\n",
        "}\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "9BwRt9zQ5277"
      },
      "execution_count": 186,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ## Below code is not working, Google Colab had some issue running it, might be OpenAPI class problem \"`super` object has no attribute `parse_obj`\"\"\n",
        "# # load OpenAPISpec from the above text\n",
        "# spec = OpenAPISpec.from_text(text)\n",
        "\n",
        "# pet_openai_functions, pet_callables = openapi_spec_to_openai_fn(spec)\n",
        "\n",
        "# from langchain.chat_models import ChatOpenAI\n",
        "# model = ChatOpenAI(temperature=0.).bind(functions=pet_openai_functions)\n",
        "\n",
        "# # example runs\n",
        "# model.invoke(\"what are three pets names\"), model.invoke(\"tell me about pet with id 42\")"
      ],
      "metadata": {
        "id": "boZLZOrS6Gz7"
      },
      "execution_count": 187,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### `Routing`"
      ],
      "metadata": {
        "id": "ETUFsArX7bWK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Here we're going to use the 2 tools defined above `get_current_temperature` and `search_wikipedia`\n",
        "# we're gonna make OpenAI decide which tool to select based on their corresponding function definition\n",
        "# and then we're also going to invoke the tool to get response. => This is called Routing\n",
        "\n",
        "# Routing: where we used LLM to determine which path to take and also the input to the paths"
      ],
      "metadata": {
        "id": "Dt_nBM9Q6Lp-"
      },
      "execution_count": 188,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "functions = [\n",
        "    format_tool_to_openai_function(f) for f in [\n",
        "        search_wikipedia, get_current_temperature\n",
        "    ]\n",
        "]\n",
        "functions"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "drijHz0q8vFV",
        "outputId": "4cc93427-ea9a-4652-936b-4159aee599c9"
      },
      "execution_count": 189,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'name': 'search_wikipedia',\n",
              "  'description': 'search_wikipedia(query: str) -> str - Run Wikipedia search and get page summaries',\n",
              "  'parameters': {'title': 'search_wikipediaSchemaSchema',\n",
              "   'type': 'object',\n",
              "   'properties': {'query': {'title': 'Query', 'type': 'string'}},\n",
              "   'required': ['query']}},\n",
              " {'name': 'get_current_temperature',\n",
              "  'description': 'get_current_temperature(latitude: float, longitude: float) -> dict - Fetch current temperature for given coordinates.',\n",
              "  'parameters': {'title': 'OpenMeteoInput',\n",
              "   'type': 'object',\n",
              "   'properties': {'latitude': {'title': 'Latitude',\n",
              "     'description': 'Latitude of the location to fetch weather data for',\n",
              "     'type': 'number'},\n",
              "    'longitude': {'title': 'Longitude',\n",
              "     'description': 'Longitude of the location to fetch weather data for',\n",
              "     'type': 'number'}},\n",
              "   'required': ['latitude', 'longitude']}}]"
            ]
          },
          "metadata": {},
          "execution_count": 189
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chat_models import ChatOpenAI"
      ],
      "metadata": {
        "id": "y5rDgfJK9LhX"
      },
      "execution_count": 190,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = ChatOpenAI(temperature=0.).bind(functions=functions)"
      ],
      "metadata": {
        "id": "RAmrQzPP9C44"
      },
      "execution_count": 191,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.invoke(\"what is the weather in sf right now?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FxV2iODF9HDg",
        "outputId": "7deb60ac-a2dc-40c5-f692-f3fed90055a4"
      },
      "execution_count": 192,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{\\n  \"latitude\": 37.7749,\\n  \"longitude\": -122.4194\\n}', 'name': 'get_current_temperature'}})"
            ]
          },
          "metadata": {},
          "execution_count": 192
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.invoke(\"what is langchain\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wKByMM9_9RR9",
        "outputId": "4c2e69fa-3149-4273-a0c5-1dd60a0e7ecc"
      },
      "execution_count": 193,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{\\n  \"query\": \"langchain\"\\n}', 'name': 'search_wikipedia'}})"
            ]
          },
          "metadata": {},
          "execution_count": 193
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import ChatPromptTemplate\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"You are helpful but sassy assistant\"),\n",
        "    (\"user\", \"{input}\")\n",
        "])\n",
        "chain = prompt | model"
      ],
      "metadata": {
        "id": "eLps-Iwk9Wd6"
      },
      "execution_count": 194,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain.invoke({\"input\": \"what is the weather in sf right now?\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xJsOkc1r9n14",
        "outputId": "b11239ba-1b82-49ed-eaf7-2e0a8dfe083a"
      },
      "execution_count": 195,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{\\n  \"latitude\": 37.7749,\\n  \"longitude\": -122.4194\\n}', 'name': 'get_current_temperature'}})"
            ]
          },
          "metadata": {},
          "execution_count": 195
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# this will is going to take in output and parse that to determine whether its a function call\n",
        "# or a response and if it a function call, what function is to be called and what's input\n",
        "from langchain.agents.output_parsers import OpenAIFunctionsAgentOutputParser"
      ],
      "metadata": {
        "id": "GpFW897J9uVE"
      },
      "execution_count": 196,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain = prompt | model | OpenAIFunctionsAgentOutputParser()"
      ],
      "metadata": {
        "id": "ZdXYkmEa9z2y"
      },
      "execution_count": 197,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = chain.invoke({\"input\": \"what is the weather in sf right now?\"})"
      ],
      "metadata": {
        "id": "j-1ueWzw9-oL"
      },
      "execution_count": 198,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zY7UCcG3-WM6",
        "outputId": "9a8a192d-8251-4d0d-d854-059a06b38857"
      },
      "execution_count": 199,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "langchain.schema.agent.AgentActionMessageLog"
            ]
          },
          "metadata": {},
          "execution_count": 199
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result.tool"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "pt5IMRWI-XcH",
        "outputId": "3c531bcb-5986-4abb-a410-e0e947c35142"
      },
      "execution_count": 200,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'get_current_temperature'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 200
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result.tool_input"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qi40L1B6-bX7",
        "outputId": "ab262ab5-9cfb-4c28-fa4b-3b04543f6269"
      },
      "execution_count": 201,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'latitude': 37.7749, 'longitude': -122.4194}"
            ]
          },
          "metadata": {},
          "execution_count": 201
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "get_current_temperature(result.tool_input)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "4OkFD-CF-dM_",
        "outputId": "6e0decbe-a7ae-4a2c-de36-e57399a88197"
      },
      "execution_count": 202,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The current temperature is 17.5°C'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 202
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result = chain.invoke({\"input\": \"hi!\"})"
      ],
      "metadata": {
        "id": "j_NmM6e_-foD"
      },
      "execution_count": 203,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gl8V0pT--jNh",
        "outputId": "d2a99c89-bae0-43d0-f038-6b6ec6b47b41"
      },
      "execution_count": 204,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "langchain.schema.agent.AgentFinish"
            ]
          },
          "metadata": {},
          "execution_count": 204
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result.return_values # `.return_values` is common to all AgentFinish response types"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r4MRkHUK-kRE",
        "outputId": "8187a642-d9ae-4d7f-bdfd-5577fbcce9a5"
      },
      "execution_count": 205,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'output': 'Hello! How can I assist you today?'}"
            ]
          },
          "metadata": {},
          "execution_count": 205
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# define Route function, it will act on the model output and decide either scenarios\n",
        "from langchain.schema.agent import AgentFinish\n",
        "def route(result):\n",
        "    if isinstance(result, AgentFinish):\n",
        "        return result.return_values[\"output\"]\n",
        "    else:\n",
        "        tools = {\n",
        "            \"search_wikipedia\": search_wikipedia,\n",
        "            \"get_current_temperature\": get_current_temperature\n",
        "        }\n",
        "        return tools[result.tool].run(result.tool_input) # apply the response to the callable"
      ],
      "metadata": {
        "id": "aKTcEzzq-mr4"
      },
      "execution_count": 206,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain = prompt | model | OpenAIFunctionsAgentOutputParser() | route"
      ],
      "metadata": {
        "id": "9Ycu49kk_d0i"
      },
      "execution_count": 207,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = chain.invoke({\"input\": \"what is the weather in san fransisco now?\"})"
      ],
      "metadata": {
        "id": "GniNEA_Y_j2O"
      },
      "execution_count": 208,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "DK0sFOgf_shF",
        "outputId": "19901ef0-dc0a-4a88-8cd4-a2c618cfdee4"
      },
      "execution_count": 209,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The current temperature is 17.5°C'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 209
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result = chain.invoke({\"input\": \"what is langchain?\"})"
      ],
      "metadata": {
        "id": "CJ-4r0Hc_uYb"
      },
      "execution_count": 210,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145
        },
        "id": "W1J4JjNR_yv9",
        "outputId": "206e6362-9e17-4ce2-a633-a559b5847563"
      },
      "execution_count": 211,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Page: LangChain\\nSummary: LangChain is a framework designed to simplify the creation of applications using large language models (LLMs). As a language model integration framework, LangChain\\'s use-cases largely overlap with those of language models in general, including document analysis and summarization, chatbots, and code analysis.\\n\\nPage: OpenAI\\nSummary: OpenAI is an American artificial intelligence (AI) research organization consisting of the non-profit OpenAI, Inc. registered in Delaware and its for-profit subsidiary OpenAI Global, LLC. OpenAI researches artificial intelligence with the declared intention of developing \"safe and beneficial\" artificial general intelligence, which it defines as \"highly autonomous systems that outperform humans at most economically valuable work\". OpenAI has also developed several large language models, such as ChatGPT and GPT-4, as well as advanced image generation models like DALL-E 3, and in the past published open-source models.The organization was founded in December 2015 by Ilya Sutskever, Greg Brockman, Trevor Blackwell, Vicki Cheung, Andrej Karpathy, Durk Kingma, Jessica Livingston, John Schulman, Pamela Vagata, and Wojciech Zaremba, with Sam Altman and Elon Musk serving as the initial board members. Microsoft provided OpenAI Global LLC with a $1 billion investment in 2019 and a $10 billion investment in 2023.On November 17, 2023, the board removed Altman as CEO, while Brockman was removed as chairman and then resigned as president. Four days later, both returned after negotiations with the board, and most of the board members resigned. The new initial board included former Salesforce co-CEO Bret Taylor as chairman.\\n\\nPage: Prompt engineering\\nSummary: Prompt engineering is the process of structuring text that can be interpreted and understood by a generative AI model. A prompt is natural language text describing the task that an AI should perform.A prompt for a text-to-text model can be a query such as \"what is Fermat\\'s little theorem?\", a command such as \"write a poem about leaves falling\", a short statement of feedback (for example, \"too verbose\", \"too formal\", \"rephrase again\", \"omit this word\") or a longer statement including context, instructions, and input data. Prompt engineering may involve phrasing a query, specifying a style, providing relevant context or assigning a role to the AI such as \"Act as a native French speaker\". A prompt may include a few examples for a model to learn from, such as \"maison -> house, chat -> cat, chien -> dog\", an approach called few-shot learning.When communicating with a text-to-image or a text-to-audio model, a typical prompt is a description of a desired output such as \"a high-quality photo of an astronaut riding a horse\" or \"Lo-fi slow BPM electro chill with organic samples\". Prompting a text-to-image model may involve adding, removing, emphasizing and re-ordering words to achieve a desired subject, style, layout, lighting, and aesthetic.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 211
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chain.invoke({\"input\": \"hi!\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "30G3cIkk_zHK",
        "outputId": "e2586500-62b0-40a5-8ef8-cd1b68a0fb22"
      },
      "execution_count": 212,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Hello! How can I assist you today?'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 212
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### `Conversational Agent`"
      ],
      "metadata": {
        "id": "jl88ajqB_5xE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "`This will comprise tool usage and chat memory to mimic something like ChatGPT`\n",
        "\n",
        "- `Build some tools`\n",
        "- `Write your own agent loop using LCEL`\n",
        "- `Utilize agent_executor which:`\n",
        "    - `Implements the agent loop`\n",
        "    - `Adds error handling, early stopping, tracing etc.`"
      ],
      "metadata": {
        "id": "HUFUMbksAOg_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from pydantic import BaseModel, Field\n",
        "import datetime\n",
        "\n",
        "# Define the input schema\n",
        "class OpenMeteoInput(BaseModel):\n",
        "    latitude: float = Field(..., description=\"Latitude of the location to fetch weather data for\")\n",
        "    longitude: float = Field(..., description=\"Longitude of the location to fetch weather data for\")\n",
        "\n",
        "@tool(args_schema=OpenMeteoInput)\n",
        "def get_current_temperature(latitude: float, longitude: float) -> dict:\n",
        "    \"\"\"Fetch current temperature for given coordinates.\"\"\"\n",
        "\n",
        "    BASE_URL = \"https://api.open-meteo.com/v1/forecast\"\n",
        "\n",
        "    # parameters for the request\n",
        "    params = {\n",
        "        'latitude': latitude,\n",
        "        'longitude': longitude,\n",
        "        'hourly': 'temperature_2m',\n",
        "        'forecast_days': 1,\n",
        "    }\n",
        "\n",
        "    # make the request\n",
        "    response = requests.get(BASE_URL, params=params)\n",
        "\n",
        "    if response.status_code == 200:\n",
        "        results = response.json()\n",
        "    else:\n",
        "        raise Exception(f\"API Request failed with status code: {response.status_code}\")\n",
        "\n",
        "    current_utc_time = datetime.datetime.utcnow()\n",
        "    time_list = [datetime.datetime.fromisoformat(time_str.replace('Z', '+00:00')) for time_str in results['hourly']['time']]\n",
        "    temperature_list = results['hourly']['temperature_2m']\n",
        "\n",
        "    closest_time_index = min(range(len(time_list)), key=lambda i: abs(time_list[i] - current_utc_time))\n",
        "    current_temperature = temperature_list[closest_time_index]\n",
        "\n",
        "    return f'The current temperature is {current_temperature}°C'"
      ],
      "metadata": {
        "id": "0EM_Whfu_1SN"
      },
      "execution_count": 213,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import wikipedia\n",
        "\n",
        "@tool\n",
        "def search_wikipedia(query: str) -> str:\n",
        "    \"\"\"Run Wikipedia search and get page summaries.\"\"\"\n",
        "    page_titles = wikipedia.search(query)\n",
        "    summaries = []\n",
        "    for page_title in page_titles[: 3]:\n",
        "        try:\n",
        "            wiki_page =  wikipedia.page(title=page_title, auto_suggest=False)\n",
        "            summaries.append(f\"Page: {page_title}\\nSummary: {wiki_page.summary}\")\n",
        "        except (\n",
        "            self.wiki_client.exceptions.PageError,\n",
        "            self.wiki_client.exceptions.DisambiguationError,\n",
        "        ):\n",
        "            pass\n",
        "    if not summaries:\n",
        "        return \"No good Wikipedia Search Result was found\"\n",
        "    return \"\\n\\n\".join(summaries)"
      ],
      "metadata": {
        "id": "Rs3jQC3WCXDq"
      },
      "execution_count": 214,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tools = [get_current_temperature, search_wikipedia]"
      ],
      "metadata": {
        "id": "Lni2nEQ1CYNe"
      },
      "execution_count": 215,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.tools.render import format_tool_to_openai_function\n",
        "from langchain.agents.output_parsers import OpenAIFunctionsAgentOutputParser"
      ],
      "metadata": {
        "id": "fYW3tDF-CZjT"
      },
      "execution_count": 216,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "functions = [format_tool_to_openai_function(f) for f in tools]\n",
        "model = ChatOpenAI(temperature=0).bind(functions=functions)\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"You are helpful but sassy assistant\"),\n",
        "    (\"user\", \"{input}\"),\n",
        "])\n",
        "chain = prompt | model | OpenAIFunctionsAgentOutputParser()"
      ],
      "metadata": {
        "id": "2cwvObKXCdau"
      },
      "execution_count": 217,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = chain.invoke({\"input\": \"what is the weather is sf?\"})"
      ],
      "metadata": {
        "id": "iVfdArWyCebs"
      },
      "execution_count": 218,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result.tool"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "h5YFwGqrCfuD",
        "outputId": "042a7f3a-5dcd-44c9-a585-2f3d48895a69"
      },
      "execution_count": 219,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'get_current_temperature'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 219
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result.tool_input"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RLSwXZgXCgrC",
        "outputId": "0141e29e-2212-479c-d619-14574b29b4ce"
      },
      "execution_count": 220,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'latitude': 37.7749, 'longitude': -122.4194}"
            ]
          },
          "metadata": {},
          "execution_count": 220
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import MessagesPlaceholder\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"You are helpful but sassy assistant\"),\n",
        "    (\"user\", \"{input}\"),\n",
        "    MessagesPlaceholder(variable_name=\"agent_scratchpad\") # place where we can pass a list of observation retrieved after utilization of certain tool\n",
        "])"
      ],
      "metadata": {
        "id": "Dnna2eqkChzX"
      },
      "execution_count": 221,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain = prompt | model | OpenAIFunctionsAgentOutputParser()"
      ],
      "metadata": {
        "id": "tbjGV5sdDKo6"
      },
      "execution_count": 222,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result1 = chain.invoke({\n",
        "    \"input\": \"what is the weather is sf?\",\n",
        "    \"agent_scratchpad\": []\n",
        "})"
      ],
      "metadata": {
        "id": "WvPLwESCDM-7"
      },
      "execution_count": 223,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result1.tool"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "CcCKmFymDebp",
        "outputId": "4dee6202-2dcb-43fe-db08-e5ed6a4d641f"
      },
      "execution_count": 224,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'get_current_temperature'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 224
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "observation = get_current_temperature(result1.tool_input)"
      ],
      "metadata": {
        "id": "9a6-VVv_DfQL"
      },
      "execution_count": 225,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "observation"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "Ev8ISB_oDhxq",
        "outputId": "ff811c59-5c71-420a-90a1-79bf4d26c22a"
      },
      "execution_count": 226,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The current temperature is 17.5°C'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 226
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type(result1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kF8O_giJDiQD",
        "outputId": "f8d18bd1-4992-463d-8dbd-8d01c4f2ea25"
      },
      "execution_count": 227,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "langchain.schema.agent.AgentActionMessageLog"
            ]
          },
          "metadata": {},
          "execution_count": 227
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.agents.format_scratchpad import format_to_openai_functions"
      ],
      "metadata": {
        "id": "4qb750C4Djy1"
      },
      "execution_count": 228,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result1.message_log # contains list of messages that makes up how we arrive at current AgentAction"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dFJ_qaZjD2Gi",
        "outputId": "d0aac72c-589a-4bff-bb4e-5d105ce34b0b"
      },
      "execution_count": 229,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{\\n  \"latitude\": 37.7749,\\n  \"longitude\": -122.4194\\n}', 'name': 'get_current_temperature'}})]"
            ]
          },
          "metadata": {},
          "execution_count": 229
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "format_to_openai_functions([(result1, observation), ])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fchd-pzxD3KP",
        "outputId": "c6aa8da2-9533-4da3-f2cf-a577ba43b767"
      },
      "execution_count": 230,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{\\n  \"latitude\": 37.7749,\\n  \"longitude\": -122.4194\\n}', 'name': 'get_current_temperature'}}),\n",
              " FunctionMessage(content='The current temperature is 17.5°C', name='get_current_temperature')]"
            ]
          },
          "metadata": {},
          "execution_count": 230
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result2 = chain.invoke({\n",
        "    \"input\": \"what is the weather is sf?\",\n",
        "    \"agent_scratchpad\": format_to_openai_functions([(result1, observation)])\n",
        "})"
      ],
      "metadata": {
        "id": "YbHhPrvuEXKe"
      },
      "execution_count": 231,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result2 # `AgentFinish`"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7sbl7vrdEnlj",
        "outputId": "5ee3b53d-3957-43f5-f59c-b15614d7bdb4"
      },
      "execution_count": 232,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AgentFinish(return_values={'output': 'The current temperature in San Francisco is 17.5°C.'}, log='The current temperature in San Francisco is 17.5°C.')"
            ]
          },
          "metadata": {},
          "execution_count": 232
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.schema.agent import AgentFinish\n",
        "def run_agent(user_input):\n",
        "    intermediate_steps = []\n",
        "    while True:\n",
        "        result = chain.invoke({\n",
        "            \"input\": user_input,\n",
        "            \"agent_scratchpad\": format_to_openai_functions(intermediate_steps)\n",
        "        })\n",
        "        if isinstance(result, AgentFinish):\n",
        "            return result\n",
        "        tool = {\n",
        "            \"search_wikipedia\": search_wikipedia,\n",
        "            \"get_current_temperature\": get_current_temperature,\n",
        "        }[result.tool]\n",
        "        observation = tool.run(result.tool_input)\n",
        "        intermediate_steps.append((result, observation))"
      ],
      "metadata": {
        "id": "hEuK3EwSEoy6"
      },
      "execution_count": 233,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# `RunnablePassthrough` takes an inital input and passes it through\n",
        "from langchain.schema.runnable import RunnablePassthrough\n",
        "agent_chain = RunnablePassthrough.assign(\n",
        "    agent_scratchpad = lambda x: format_to_openai_functions(x[\"intermediate_steps\"])\n",
        ") | chain"
      ],
      "metadata": {
        "id": "HdFUhcbiFHBZ"
      },
      "execution_count": 234,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_agent(user_input):\n",
        "    intermediate_steps = []\n",
        "    while True:\n",
        "        result = agent_chain.invoke({\n",
        "            \"input\": user_input,\n",
        "            \"intermediate_steps\": intermediate_steps\n",
        "        })\n",
        "        if isinstance(result, AgentFinish):\n",
        "            return result\n",
        "        tool = {\n",
        "            \"search_wikipedia\": search_wikipedia,\n",
        "            \"get_current_temperature\": get_current_temperature,\n",
        "        }[result.tool]\n",
        "        observation = tool.run(result.tool_input)\n",
        "        intermediate_steps.append((result, observation))"
      ],
      "metadata": {
        "id": "JMKxftwCFbJk"
      },
      "execution_count": 235,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "run_agent(\"what is the weather is sf?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lzki-30sFc-m",
        "outputId": "f747cedf-975e-48bf-f7cd-cc198336a386"
      },
      "execution_count": 236,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AgentFinish(return_values={'output': 'The current temperature in San Francisco is 17.5°C.'}, log='The current temperature in San Francisco is 17.5°C.')"
            ]
          },
          "metadata": {},
          "execution_count": 236
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "run_agent(\"what is langchain?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YT_AhbUnFeiP",
        "outputId": "0240bacc-4807-4675-cebb-b8d8e8e0d2a0"
      },
      "execution_count": 237,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AgentFinish(return_values={'output': 'I couldn\\'t find specific information about \"LangChain\" in my search results. It\\'s possible that LangChain is a term or concept that is not widely known or documented. If you have any additional information or context about LangChain, I may be able to provide more assistance.'}, log='I couldn\\'t find specific information about \"LangChain\" in my search results. It\\'s possible that LangChain is a term or concept that is not widely known or documented. If you have any additional information or context about LangChain, I may be able to provide more assistance.')"
            ]
          },
          "metadata": {},
          "execution_count": 237
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "run_agent(\"hi!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aOPEe3PJFiwT",
        "outputId": "96f77708-362b-4c97-b487-7fb512c95ba2"
      },
      "execution_count": 238,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AgentFinish(return_values={'output': 'Hello! How can I assist you today?'}, log='Hello! How can I assist you today?')"
            ]
          },
          "metadata": {},
          "execution_count": 238
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# there is a class in LangChain which work similar to the above defined `run_agent` function which loops over responses\n",
        "# of the LLMs and make observations and decide upon further. Additionally, it has better logging mechanism, error handling for LLMs and tools"
      ],
      "metadata": {
        "id": "j7ndzk1EFsbU"
      },
      "execution_count": 239,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.agents import AgentExecutor\n",
        "agent_executor = AgentExecutor(agent=agent_chain, tools=tools, verbose=True)"
      ],
      "metadata": {
        "id": "ToEBU0RYGS-z"
      },
      "execution_count": 240,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent_executor.invoke({\"input\": \"what is LangChain?\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gSpJxr6xGbff",
        "outputId": "3e09b568-ceb8-49fe-8cee-6669d99a5c29"
      },
      "execution_count": 241,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m\n",
            "Invoking: `search_wikipedia` with `{'query': 'LangChain'}`\n",
            "\n",
            "\n",
            "\u001b[0m\u001b[33;1m\u001b[1;3mPage: LangChain\n",
            "Summary: LangChain is a framework designed to simplify the creation of applications using large language models (LLMs). As a language model integration framework, LangChain's use-cases largely overlap with those of language models in general, including document analysis and summarization, chatbots, and code analysis.\n",
            "\n",
            "Page: Prompt engineering\n",
            "Summary: Prompt engineering is the process of structuring text that can be interpreted and understood by a generative AI model. A prompt is natural language text describing the task that an AI should perform.A prompt for a text-to-text model can be a query such as \"what is Fermat's little theorem?\", a command such as \"write a poem about leaves falling\", a short statement of feedback (for example, \"too verbose\", \"too formal\", \"rephrase again\", \"omit this word\") or a longer statement including context, instructions, and input data. Prompt engineering may involve phrasing a query, specifying a style, providing relevant context or assigning a role to the AI such as \"Act as a native French speaker\". A prompt may include a few examples for a model to learn from, such as \"maison -> house, chat -> cat, chien -> dog\", an approach called few-shot learning.When communicating with a text-to-image or a text-to-audio model, a typical prompt is a description of a desired output such as \"a high-quality photo of an astronaut riding a horse\" or \"Lo-fi slow BPM electro chill with organic samples\". Prompting a text-to-image model may involve adding, removing, emphasizing and re-ordering words to achieve a desired subject, style, layout, lighting, and aesthetic.\n",
            "\n",
            "Page: OpenAI\n",
            "Summary: OpenAI is an American artificial intelligence (AI) research organization consisting of the non-profit OpenAI, Inc. registered in Delaware and its for-profit subsidiary OpenAI Global, LLC. OpenAI researches artificial intelligence with the declared intention of developing \"safe and beneficial\" artificial general intelligence, which it defines as \"highly autonomous systems that outperform humans at most economically valuable work\". OpenAI has also developed several large language models, such as ChatGPT and GPT-4, as well as advanced image generation models like DALL-E 3, and in the past published open-source models.The organization was founded in December 2015 by Ilya Sutskever, Greg Brockman, Trevor Blackwell, Vicki Cheung, Andrej Karpathy, Durk Kingma, Jessica Livingston, John Schulman, Pamela Vagata, and Wojciech Zaremba, with Sam Altman and Elon Musk serving as the initial board members. Microsoft provided OpenAI Global LLC with a $1 billion investment in 2019 and a $10 billion investment in 2023.On November 17, 2023, the board removed Altman as CEO, while Brockman was removed as chairman and then resigned as president. Four days later, both returned after negotiations with the board, and most of the board members resigned. The new initial board included former Salesforce co-CEO Bret Taylor as chairman.\u001b[0m\u001b[32;1m\u001b[1;3mLangChain is a framework designed to simplify the creation of applications using large language models (LLMs). It is a language model integration framework that can be used for various tasks such as document analysis and summarization, chatbots, and code analysis. LangChain's use-cases largely overlap with those of language models in general.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input': 'what is LangChain?',\n",
              " 'output': \"LangChain is a framework designed to simplify the creation of applications using large language models (LLMs). It is a language model integration framework that can be used for various tasks such as document analysis and summarization, chatbots, and code analysis. LangChain's use-cases largely overlap with those of language models in general.\"}"
            ]
          },
          "metadata": {},
          "execution_count": 241
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "agent_executor.invoke({\"input\": \"my name is bob\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tjPVJHbmGeYp",
        "outputId": "b2fa76b7-72aa-4f57-b1e1-3fa943a2463d"
      },
      "execution_count": 242,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mHello Bob! How can I assist you today?\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input': 'my name is bob', 'output': 'Hello Bob! How can I assist you today?'}"
            ]
          },
          "metadata": {},
          "execution_count": 242
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "agent_executor.invoke({\"input\": \"what is my name\"}) # this is since there is not memory factor for previous message"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kkW4IBVBG5ZU",
        "outputId": "66b7a587-5849-4982-8082-83cf386a8fcd"
      },
      "execution_count": 243,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mI'm sorry, but I don't have access to personal information.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input': 'what is my name',\n",
              " 'output': \"I'm sorry, but I don't have access to personal information.\"}"
            ]
          },
          "metadata": {},
          "execution_count": 243
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"You are helpful but sassy assistant\"),\n",
        "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
        "    (\"user\", \"{input}\"),\n",
        "    MessagesPlaceholder(variable_name=\"agent_scratchpad\")\n",
        "])"
      ],
      "metadata": {
        "id": "_4DcbalCHDMW"
      },
      "execution_count": 244,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent_chain = RunnablePassthrough.assign(\n",
        "    agent_scratchpad = lambda x: format_to_openai_functions(x[\"intermediate_steps\"])\n",
        ") | prompt | model | OpenAIFunctionsAgentOutputParser()"
      ],
      "metadata": {
        "id": "L5ZJzqB6HRiH"
      },
      "execution_count": 245,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.memory import ConversationBufferMemory\n",
        "# `return_messages=True` argument makes sure that the returned messages are appended in form of a list\n",
        "# since if `False` it would return as string which is not ideal for MessagesPlaceholder\n",
        "memory = ConversationBufferMemory(return_messages=True, memory_key=\"chat_history\")"
      ],
      "metadata": {
        "id": "KByxEoamIaNO"
      },
      "execution_count": 246,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent_executor = AgentExecutor(agent=agent_chain, tools=tools, verbose=True, memory=memory)"
      ],
      "metadata": {
        "id": "ZZ2q9BKWIhM8"
      },
      "execution_count": 247,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent_executor.invoke({\"input\": \"my name is bob\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CPoCNhejI7BL",
        "outputId": "eba1d0e5-695a-4cb4-de6b-a0c8cb2ceba8"
      },
      "execution_count": 248,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mHello Bob! How can I assist you today?\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input': 'my name is bob',\n",
              " 'chat_history': [HumanMessage(content='my name is bob'),\n",
              "  AIMessage(content='Hello Bob! How can I assist you today?')],\n",
              " 'output': 'Hello Bob! How can I assist you today?'}"
            ]
          },
          "metadata": {},
          "execution_count": 248
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "agent_executor.invoke({\"input\": \"whats my name\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G5JNMvgnI8Ut",
        "outputId": "7d358c75-c500-41b2-de48-1fbc8f455abc"
      },
      "execution_count": 249,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mYour name is Bob.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input': 'whats my name',\n",
              " 'chat_history': [HumanMessage(content='my name is bob'),\n",
              "  AIMessage(content='Hello Bob! How can I assist you today?'),\n",
              "  HumanMessage(content='whats my name'),\n",
              "  AIMessage(content='Your name is Bob.')],\n",
              " 'output': 'Your name is Bob.'}"
            ]
          },
          "metadata": {},
          "execution_count": 249
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "agent_executor.invoke({\"input\": \"whats the weather in sf?\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nojmSJA3I9c5",
        "outputId": "ed262a70-ecf8-466b-e097-5357565caf10"
      },
      "execution_count": 250,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m\n",
            "Invoking: `get_current_temperature` with `{'latitude': 37.7749, 'longitude': -122.4194}`\n",
            "\n",
            "\n",
            "\u001b[0m\u001b[36;1m\u001b[1;3mThe current temperature is 17.5°C\u001b[0m\u001b[32;1m\u001b[1;3mThe current temperature in San Francisco is 17.5°C.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input': 'whats the weather in sf?',\n",
              " 'chat_history': [HumanMessage(content='my name is bob'),\n",
              "  AIMessage(content='Hello Bob! How can I assist you today?'),\n",
              "  HumanMessage(content='whats my name'),\n",
              "  AIMessage(content='Your name is Bob.'),\n",
              "  HumanMessage(content='whats the weather in sf?'),\n",
              "  AIMessage(content='The current temperature in San Francisco is 17.5°C.')],\n",
              " 'output': 'The current temperature in San Francisco is 17.5°C.'}"
            ]
          },
          "metadata": {},
          "execution_count": 250
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "`Additional module to Create a chatbot, which utilizes panel library to create a chat interface and utilizes the AgentExecutor class to mimic a chatagent and then converse with user.`"
      ],
      "metadata": {
        "id": "mBh1nW1WJXTT"
      }
    }
  ]
}