{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM3xx89NKE+PofKYl1ITbQb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nid989/LLMs-Overview/blob/main/langchain/functions_tools_and_agents_with_langchain.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "-A1gbwqgbTYg"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install openai python-dotenv langchain tiktoken\n",
        "!pip install pydantic==1.10.8\n",
        "!pip install langchain[docarray]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import openai\n",
        "\n",
        "from dotenv import load_dotenv, find_dotenv\n",
        "_ = load_dotenv(find_dotenv(filename=\"./.env.txt\"))\n",
        "openai.api_key = os.environ['OPENAI_API_KEY']"
      ],
      "metadata": {
        "id": "fvbIf4azetPw"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### `OpenAI: Function Calls`"
      ],
      "metadata": {
        "id": "vKtQnzMDqvvP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# Example dummy function hard coded to return the same weather\n",
        "# IN production, this could be your backend API or an external API\n",
        "def get_current_weather(location, unit=\"fahrenheit\"):\n",
        "    \"\"\"Get the current weather in a given location\"\"\"\n",
        "    weather_info = {\n",
        "        \"location\": location,\n",
        "        \"temperature\": \"72\",\n",
        "        \"unit\": unit,\n",
        "        \"forecast\": [\"sunny\", \"windy\"]\n",
        "    }\n",
        "    return json.dumps(weather_info)"
      ],
      "metadata": {
        "id": "5_ELmmZye_IU"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define a function (`As proposed by OpenAI`)\n",
        "# `description` parameter (both) are really important, since this will be passed directly\n",
        "# to the LLM and the model is gonna use this description to determine whether to use the function :)\n",
        "functions = [\n",
        "    {\n",
        "        \"name\": \"get_current_weather\",\n",
        "        \"description\": \"Get the current weather in a given location\",\n",
        "        \"parameters\": {\n",
        "            \"type\": \"object\",\n",
        "            \"properties\": {\n",
        "                \"location\": {\n",
        "                    \"type\": \"string\",\n",
        "                    \"description\": \"The city and state, e.g. San Fransico, CA\"\n",
        "                },\n",
        "                \"unit\": {\n",
        "                    \"type\": \"string\",\n",
        "                    \"enum\": [\"celsius\", \"fahrenheit\"]\n",
        "                }\n",
        "            },\n",
        "            \"required\": [\"location\"]\n",
        "        }\n",
        "    }\n",
        "]"
      ],
      "metadata": {
        "id": "vBfBXSZafHn1"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": \"what's the weather like in Boston?\"\n",
        "    }\n",
        "]"
      ],
      "metadata": {
        "id": "cl-2b237hUyR"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import openai"
      ],
      "metadata": {
        "id": "05OpIZCLhdrn"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = openai.chat.completions.create(\n",
        "    model=\"gpt-3.5-turbo-0613\",\n",
        "    messages=messages,\n",
        "    functions=functions\n",
        ")"
      ],
      "metadata": {
        "id": "vE89GoJuhvi2"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ewHgWcjGh49h",
        "outputId": "9f0c65d1-4534-4c60-9a31-e7ca84972557"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ChatCompletion(id='chatcmpl-8NRXBU3NwwEKBLKOxUnDEcxjevmdE', choices=[Choice(finish_reason='function_call', index=0, message=ChatCompletionMessage(content=None, role='assistant', function_call=FunctionCall(arguments='{\\n  \"location\": \"Boston, MA\"\\n}', name='get_current_weather'), tool_calls=None))], created=1700598077, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=18, prompt_tokens=84, total_tokens=102))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response_message = response.choices[0].message\n",
        "response_message"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "snRwYHJ9i40K",
        "outputId": "12a4b13f-2c19-412e-d42e-7ffbedb15edd"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatCompletionMessage(content=None, role='assistant', function_call=FunctionCall(arguments='{\\n  \"location\": \"Boston, MA\"\\n}', name='get_current_weather'), tool_calls=None)"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response_message.content # None"
      ],
      "metadata": {
        "id": "uaQkoI-ljlKa"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response_message.function_call"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ald76vh0j14d",
        "outputId": "b939850d-c7dc-4d77-f87b-0c33ee7a6267"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "FunctionCall(arguments='{\\n  \"location\": \"Boston, MA\"\\n}', name='get_current_weather')"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "args = json.loads(response_message.function_call.arguments)\n",
        "args"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yiA_Idxkj3vo",
        "outputId": "759f1d55-8b7b-4793-9937-be23e79a4ce7"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'location': 'Boston, MA'}"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "get_current_weather(args)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "P1pHD8yokAbO",
        "outputId": "5e65d579-456b-4747-a265-10127091a3b0"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'{\"location\": {\"location\": \"Boston, MA\"}, \"temperature\": \"72\", \"unit\": \"fahrenheit\", \"forecast\": [\"sunny\", \"windy\"]}'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# w/ OpenAI Function Calls, it doesnot utilize the defined function to generate the response\n",
        "# rather it will tell us what function to call and what the arguments to the specific function should be!"
      ],
      "metadata": {
        "id": "JtE8luHqkTKT"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# what if the message passed to the OpenAI model API is not related to the already defined function at all.\n",
        "messages = [\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": \"hi!\"\n",
        "    }\n",
        "]"
      ],
      "metadata": {
        "id": "ICMbpNByk_cD"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = openai.chat.completions.create(\n",
        "    model=\"gpt-3.5-turbo-0613\",\n",
        "    messages=messages,\n",
        "    functions=functions\n",
        ")"
      ],
      "metadata": {
        "id": "ClTlvPlYlSaU"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(response) # output `message` includes not None content and no function call argument as seen in the previous response"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hs7E8El7la9Q",
        "outputId": "bfe461ad-d6da-4763-ff82-baa25daa8d0f"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ChatCompletion(id='chatcmpl-8NRiAXzwV24TFvTYFUKrFOqlSFVTi', choices=[Choice(finish_reason='stop', index=0, message=ChatCompletionMessage(content='Hello! How can I assist you today?', role='assistant', function_call=None, tool_calls=None))], created=1700598758, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=10, prompt_tokens=78, total_tokens=88))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# `function_call` parameter either to force function call or not\n",
        "# mode: `auto`\n",
        "messages = [\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": \"hi!\"\n",
        "    }\n",
        "]\n",
        "\n",
        "response = openai.chat.completions.create(\n",
        "    model=\"gpt-3.5-turbo-0613\",\n",
        "    messages=messages,\n",
        "    functions=functions,\n",
        "    function_call=\"auto\" # default (i.e. the LLM choose b/w functions and self-response)\n",
        ")\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LDbPJCOilcUg",
        "outputId": "b23a4c75-650d-422e-ada9-02566f1be21f"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ChatCompletion(id='chatcmpl-8NRlN8NUZmfQmVJh4q7pKU3rZ9Zd0', choices=[Choice(finish_reason='stop', index=0, message=ChatCompletionMessage(content='Hello! How can I assist you today?', role='assistant', function_call=None, tool_calls=None))], created=1700598957, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=10, prompt_tokens=78, total_tokens=88))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# mode: `none` # force LLM not make a function call\n",
        "messages = [\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": \"What's the weather in Boston?\"\n",
        "    }\n",
        "]\n",
        "\n",
        "response = openai.chat.completions.create(\n",
        "    model=\"gpt-3.5-turbo-0613\",\n",
        "    messages=messages,\n",
        "    functions=functions,\n",
        "    function_call=\"none\"\n",
        ")\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-aFiM0eimGwN",
        "outputId": "2c9ab514-fba7-4def-c981-846a96867a82"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ChatCompletion(id='chatcmpl-8NRmnQPmuPOHS0qbM6DE1TMTjhwTK', choices=[Choice(finish_reason='stop', index=0, message=ChatCompletionMessage(content='The current weather in Boston is not available.', role='assistant', function_call=None, tool_calls=None))], created=1700599045, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=9, prompt_tokens=84, total_tokens=93))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# mode: assign a specific function (i.e. force the model to compulsorily use the function-call w/ function name provided)\n",
        "messages = [\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": \"hi!\"\n",
        "    }\n",
        "]\n",
        "\n",
        "response = openai.chat.completions.create(\n",
        "    model=\"gpt-3.5-turbo-0613\",\n",
        "    messages=messages,\n",
        "    functions=functions,\n",
        "    function_call={\"name\": \"get_current_weather\"}\n",
        ")\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aB0lD7FFmheD",
        "outputId": "94404dbd-f282-4925-ee18-bbbdbfdff2b5"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ChatCompletion(id='chatcmpl-8NRohsA1OLf1tWmGtBFjf8nRVIO8O', choices=[Choice(finish_reason='stop', index=0, message=ChatCompletionMessage(content=None, role='assistant', function_call=FunctionCall(arguments='{\\n  \"location\": \"San Francisco, CA\"\\n}', name='get_current_weather'), tool_calls=None))], created=1700599163, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=12, prompt_tokens=85, total_tokens=97))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": \"What's the weather in Boston?\",\n",
        "    }\n",
        "]\n",
        "\n",
        "response = openai.chat.completions.create(\n",
        "    model=\"gpt-3.5-turbo-0613\",\n",
        "    messages=messages,\n",
        "    functions=functions,\n",
        "    function_call={\"name\": \"get_current_weather\"}\n",
        ")\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mq7dkHe6m-Ci",
        "outputId": "1918181a-df11-48b5-86aa-bfd07c1cb4d9"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ChatCompletion(id='chatcmpl-8NRyYjEgE66WrfuV730Bpgvs8ofAD', choices=[Choice(finish_reason='stop', index=0, message=ChatCompletionMessage(content=None, role='assistant', function_call=FunctionCall(arguments='{\\n  \"location\": \"Boston, MA\"\\n}', name='get_current_weather'), tool_calls=None))], created=1700599774, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=11, prompt_tokens=90, total_tokens=101))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# NOTE: under the `usage` output parameter, we can see that w/ and w/o `functions` as input-argument\n",
        "# the prompt_tokens utilization differs drastically, from 85 (w/ function-calling) to 15 (w/o function calling)\n",
        "# since extra tokens are utilized to describe the function-calling procedure to the OpenAI LLM API.\n",
        "\n",
        "# This can put constraint, since we have a limit over the OpenAI `prompt_tokens`, thus now we do have to keep\n",
        "# track of the total input tokens already used by function-calling proc. alongwith the messages supplied"
      ],
      "metadata": {
        "id": "25XEz9XRnJJY"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": \"What's the weather in Boston?\"\n",
        "    }\n",
        "]\n",
        "\n",
        "response = openai.chat.completions.create(\n",
        "    model=\"gpt-3.5-turbo-0613\",\n",
        "    messages=messages,\n",
        "    functions=functions,\n",
        "    function_call=\"auto\"\n",
        ")\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jlktbK-loje1",
        "outputId": "8f9d34d1-3d5a-4661-e9c0-1202faf7e18c"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ChatCompletion(id='chatcmpl-8NS06f41dvims8pNYxkI8ler59GdU', choices=[Choice(finish_reason='function_call', index=0, message=ChatCompletionMessage(content=None, role='assistant', function_call=FunctionCall(arguments='{\\n  \"location\": \"Boston, MA\"\\n}', name='get_current_weather'), tool_calls=None))], created=1700599870, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=18, prompt_tokens=83, total_tokens=101))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "messages.append(response.choices[0].message) # appending to the list of messages to simulate calling to function w/ args"
      ],
      "metadata": {
        "id": "BUA4wT7Ko-Oa"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "args = json.loads(response.choices[0].message.function_call.arguments)\n",
        "observation = get_current_weather(args)"
      ],
      "metadata": {
        "id": "evmyd7EcpDbt"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "messages.append(\n",
        "    {\n",
        "        \"role\": \"function\",\n",
        "        \"name\": \"get_current_weather\",\n",
        "        \"content\": observation\n",
        "    }\n",
        ")\n",
        "print(messages)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zKmSkCiXpbgJ",
        "outputId": "b1ce3ab7-294a-4b72-ed24-9eb29f381f7a"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'role': 'user', 'content': \"What's the weather in Boston?\"}, ChatCompletionMessage(content=None, role='assistant', function_call=FunctionCall(arguments='{\\n  \"location\": \"Boston, MA\"\\n}', name='get_current_weather'), tool_calls=None), ChatCompletionMessage(content=None, role='assistant', function_call=FunctionCall(arguments='{\\n  \"location\": \"Boston, MA\"\\n}', name='get_current_weather'), tool_calls=None), {'role': 'function', 'name': 'get_current_weather', 'content': '{\"location\": {\"location\": \"Boston, MA\"}, \"temperature\": \"72\", \"unit\": \"fahrenheit\", \"forecast\": [\"sunny\", \"windy\"]}'}, {'role': 'function', 'name': 'get_current_weather', 'content': '{\"location\": {\"location\": \"Boston, MA\"}, \"temperature\": \"72\", \"unit\": \"fahrenheit\", \"forecast\": [\"sunny\", \"windy\"]}'}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = openai.chat.completions.create(\n",
        "    model=\"gpt-3.5-turbo-0613\",\n",
        "    messages=messages\n",
        ")\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IJQ7szfXp9gf",
        "outputId": "859099fe-6d34-43c4-db61-30c95395a890"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ChatCompletion(id='chatcmpl-8NS35jnze9q3iVfQ70qUnl2wKXf7i', choices=[Choice(finish_reason='stop', index=0, message=ChatCompletionMessage(content='The current weather in Boston is 72Â°F with sunny and windy conditions.', role='assistant', function_call=None, tool_calls=None))], created=1700600055, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=15, prompt_tokens=97, total_tokens=112))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### `LangChain Expression Language (LECL)`"
      ],
      "metadata": {
        "id": "DDXne8KZq0X5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.schema.output_parser import StrOutputParser  # will take a ChatMessage and convert to string"
      ],
      "metadata": {
        "id": "V1-cQo87qVYR"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### `Simple Chain`"
      ],
      "metadata": {
        "id": "zSCYebZs9L4G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = ChatPromptTemplate.from_template(\n",
        "    \"tell me a short joke about {topic}\"\n",
        ")\n",
        "model = ChatOpenAI()\n",
        "output_parser = StrOutputParser()"
      ],
      "metadata": {
        "id": "OlqSn0rrskzz"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain = prompt | model | output_parser"
      ],
      "metadata": {
        "id": "7GEFG7726zef"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain.invoke({\"topic\": \"bears\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "BM2mQykK61av",
        "outputId": "9067fd75-c9f5-461d-ec5c-4a41b3f24e39"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Why don't bears like fast food?\\nBecause they can't catch it!\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### `More Complex Chain and Runnnable Map (to supply user-provided inputs to prompt)`"
      ],
      "metadata": {
        "id": "3H3JEPfB9OY1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "from langchain.vectorstores import DocArrayInMemorySearch"
      ],
      "metadata": {
        "id": "FSScROxq631r"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorstore = DocArrayInMemorySearch.from_texts(\n",
        "    [\"harrison worked at kensho\", \"bears like to eat honey\"],\n",
        "    embedding=OpenAIEmbeddings()\n",
        ")\n",
        "retriever = vectorstore.as_retriever()"
      ],
      "metadata": {
        "id": "u9c_26vW9vVE"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "retriever.get_relevant_documents(\"Where did harrison work?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ijSu_G2E95wL",
        "outputId": "3a8cb1a3-9c1c-43d9-fd97-0fe8b14a8a2b"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(page_content='harrison worked at kensho'),\n",
              " Document(page_content='bears like to eat honey')]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "retriever.get_relevant_documents(\"What do bears like to eat?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "owe-gU1r-8vz",
        "outputId": "5c79f85c-2f3b-4fe8-cf23-0688619be51c"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(page_content='bears like to eat honey'),\n",
              " Document(page_content='harrison worked at kensho')]"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "template = \"\"\"Answer the question based only on the following context:\n",
        "{context}\n",
        "\n",
        "Question: {question}\"\"\"\n",
        "prompt = ChatPromptTemplate.from_template(template)"
      ],
      "metadata": {
        "id": "yYtzGC6Z_AEk"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# How are we going to build the chain?\n",
        "# i. We want the first and only input to the chain to be the user question.\n",
        "# ii. Then we want to fetch the relevant document for the context which will be passed to the prompt\n",
        "# iii. Finally, we want to pass the formulated prompt to the model and the output-parser later on."
      ],
      "metadata": {
        "id": "XGQ9tz4b_VmP"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# i. we want to create `Something` which takes a single question and turns it into\n",
        "# a dictionary of 2 elements, `context` and `question`.\n",
        "from langchain.schema.runnable import RunnableMap"
      ],
      "metadata": {
        "id": "GT70l7ug_3mN"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain = RunnableMap({\n",
        "    \"context\": lambda x: retriever.get_relevant_documents(x[\"question\"]),\n",
        "    \"question\": lambda x: x[\"question\"]\n",
        "}) | prompt | model | output_parser"
      ],
      "metadata": {
        "id": "eNwH8Vu6_NkX"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain.invoke({\"question\": \"Where did harrison work?\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "6Y0EKHhiAVHS",
        "outputId": "75bfa164-e03f-43c3-b14e-0c5c84ed36d7"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Harrison worked at Kensho.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = RunnableMap({\n",
        "    \"context\": lambda x: retriever.get_relevant_documents(x[\"question\"]),\n",
        "    \"question\": lambda x: x[\"question\"]\n",
        "})"
      ],
      "metadata": {
        "id": "YQ6jOQx_AdOB"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs.invoke({\"question\": \"Where did harrison work?\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wq6soeLxAu2s",
        "outputId": "c0ede4a0-aa28-4976-af38-eaf6de21d42a"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'context': [Document(page_content='harrison worked at kensho'),\n",
              "  Document(page_content='bears like to eat honey')],\n",
              " 'question': 'Where did harrison work?'}"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### `Bind and OpenAI Functions`"
      ],
      "metadata": {
        "id": "Jwcn0ZHuA6u2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "functions = [\n",
        "    {\n",
        "        \"name\": \"weather_search\",\n",
        "        \"description\": \"Search for weather given at airport code\",\n",
        "        \"parameters\": {\n",
        "            \"type\": \"object\",\n",
        "            \"properties\": {\n",
        "                \"airport_code\": {\n",
        "                    \"type\": \"string\",\n",
        "                    \"description\": \"The airport code to get the weather for\"\n",
        "                },\n",
        "            },\n",
        "            \"required\": [\"airport_code\"]\n",
        "        }\n",
        "    }\n",
        "]"
      ],
      "metadata": {
        "id": "1QoWxzQdAxw0"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"human\", \"{input}\")\n",
        "    ]\n",
        ")\n",
        "model = ChatOpenAI(temperature=0).bind(functions=functions)"
      ],
      "metadata": {
        "id": "sQTvnQsdBl4x"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "runnable = prompt | model"
      ],
      "metadata": {
        "id": "EdnRM8DBCCJB"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "runnable.invoke({\"input\": \"what is the weather in sf?\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NXCNSCLcCDyG",
        "outputId": "cc7523ba-2a25-4280-ddc4-d19e79eb33cf"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{\\n  \"airport_code\": \"SFO\"\\n}', 'name': 'weather_search'}})"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "functions = [\n",
        "    {\n",
        "        \"name\": \"weather_search\",\n",
        "        \"description\": \"Search for weather given at airport code\",\n",
        "        \"parameters\": {\n",
        "            \"type\": \"object\",\n",
        "            \"properties\": {\n",
        "                \"airport_code\": {\n",
        "                    \"type\": \"string\",\n",
        "                    \"description\": \"The airport code to get the weather for\"\n",
        "                },\n",
        "            },\n",
        "            \"required\": [\"airport_code\"]\n",
        "        }\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"sports_search\",\n",
        "        \"description\": \"Search for news of recent sport events\",\n",
        "        \"parameters\": {\n",
        "            \"type\": \"object\",\n",
        "            \"properties\": {\n",
        "                \"team_name\": {\n",
        "                    \"type\": \"string\",\n",
        "                    \"description\": \"The sports team to search for\"\n",
        "                },\n",
        "            },\n",
        "            \"required\": [\"team_name\"]\n",
        "        }\n",
        "    }\n",
        "]"
      ],
      "metadata": {
        "id": "o20B6WgjCKSD"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = model.bind(functions=functions)"
      ],
      "metadata": {
        "id": "BOgn4_zuCyjB"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "runnable = prompt | model"
      ],
      "metadata": {
        "id": "HmQXyp6KC142"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "runnable.invoke({\"input\": \"how did the patriots do yesterday?\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "76T7wZtpC6CH",
        "outputId": "fb794f29-c8c1-447c-ff42-16df29c88f50"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{\\n  \"team_name\": \"patriots\"\\n}', 'name': 'sports_search'}})"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### `Fallbacks`\n",
        "`with LangChain, we can attach fallbacks not only to the model but any component of the chain or the chain as whole.`"
      ],
      "metadata": {
        "id": "ezgtaUErDD18"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# let's say we want to output a JSON formatted string from the LLM\n",
        "# NOTE: using an older version of OpenAI LLM API just to showcase the need for `fallbacks`.\n",
        "from langchain.llms import OpenAI\n",
        "import json"
      ],
      "metadata": {
        "id": "xBKFDf1_DAdW"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "simple_model = OpenAI(\n",
        "    temperature=0,\n",
        "    max_tokens=1000,\n",
        "    model=\"text-davinci-001\"\n",
        ")\n",
        "simple_chain = simple_model | json.loads"
      ],
      "metadata": {
        "id": "PUDoUk-rDjcQ"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "challenge = \"write three poems in a json blob, where each poem is a json blob of a title, author, and first line\""
      ],
      "metadata": {
        "id": "ZhqbepjNDvbq"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "simple_model.invoke(challenge)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "id": "P9ZDOvZ1D6Sh",
        "outputId": "f1e14c6e-94ba-4b2b-b22f-d4d6bdf02d0a"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n\\n[\"The Waste Land\",\"T.S. Eliot\",\"April is the cruelest month, breeding lilacs out of the dead land\"]\\n\\n[\"The Raven\",\"Edgar Allan Poe\",\"Once upon a midnight dreary, while I pondered, weak and weary\"]\\n\\n[\"Ode to a Nightingale\",\"John Keats\",\"Thou still unravish\\'d bride of quietness, Thou foster-child of silence and slow time\"]'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# calling the below line, we get the JSONDecodeError, since the output string\n",
        "# eventhough it has some structure, cannot be decoded by json.loads() function.\n",
        "# simple_chain.invoke(challenge)"
      ],
      "metadata": {
        "id": "BTWtvJ6bD9l8"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = ChatOpenAI(temperature=0) # using a newer version of OpenAI models\n",
        "chain = model | StrOutputParser() | json.loads"
      ],
      "metadata": {
        "id": "LECQ5UVzEMAE"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain.invoke(challenge)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JxrrGy6REppM",
        "outputId": "126c5bc6-91b9-4e4f-c3f7-7f0fc406867f"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'poem1': {'title': 'Whispers of the Wind',\n",
              "  'author': 'Emily Rivers',\n",
              "  'first_line': 'Softly it comes, the whisper of the wind'},\n",
              " 'poem2': {'title': 'Silent Serenade',\n",
              "  'author': 'Jacob Moore',\n",
              "  'first_line': 'In the stillness of night, a silent serenade'},\n",
              " 'poem3': {'title': 'Dancing Shadows',\n",
              "  'author': 'Sophia Anderson',\n",
              "  'first_line': 'Shadows dance upon the moonlit floor'}}"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# so we can start with a simple chain and attach fallbacks with `.with_fallbacks`\n",
        "# so first the core runnable `simple_chain` will try to run and if error is raised\n",
        "# then it goes through list of runnables provided as fallbacks, in this case its just\n",
        "# 1 extra runnable `chain`.\n",
        "final_chain = simple_chain.with_fallbacks([chain])"
      ],
      "metadata": {
        "id": "XPnH0FawEr7D"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_chain.invoke(challenge)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5vhLOKw9Fjur",
        "outputId": "128d22fa-85d7-48f8-dde7-d00c2aa63927"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'poem1': {'title': 'Whispers of the Wind',\n",
              "  'author': 'Emily Rivers',\n",
              "  'first_line': 'Softly it comes, the whisper of the wind'},\n",
              " 'poem2': {'title': 'Silent Serenade',\n",
              "  'author': 'Jacob Moore',\n",
              "  'first_line': 'In the stillness of night, a silent serenade'},\n",
              " 'poem3': {'title': 'Dancing Shadows',\n",
              "  'author': 'Sophia Anderson',\n",
              "  'first_line': 'Shadows dance upon the moonlit floor'}}"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import langchain\n",
        "langchain.debug = True\n",
        "final_chain.invoke(challenge)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tu4qx4rZFmba",
        "outputId": "6d59e5b0-6fd9-47cc-c6ae-47c68dfc30c7"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:RunnableWithFallbacks] Entering Chain run with input:\n",
            "\u001b[0m{\n",
            "  \"input\": \"write three poems in a json blob, where each poem is a json blob of a title, author, and first line\"\n",
            "}\n",
            "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:RunnableWithFallbacks > 2:chain:RunnableSequence] Entering Chain run with input:\n",
            "\u001b[0m{\n",
            "  \"input\": \"write three poems in a json blob, where each poem is a json blob of a title, author, and first line\"\n",
            "}\n",
            "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:chain:RunnableWithFallbacks > 2:chain:RunnableSequence > 3:llm:OpenAI] Entering LLM run with input:\n",
            "\u001b[0m{\n",
            "  \"prompts\": [\n",
            "    \"write three poems in a json blob, where each poem is a json blob of a title, author, and first line\"\n",
            "  ]\n",
            "}\n",
            "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[1:chain:RunnableWithFallbacks > 2:chain:RunnableSequence > 3:llm:OpenAI] [1.32s] Exiting LLM run with output:\n",
            "\u001b[0m{\n",
            "  \"generations\": [\n",
            "    [\n",
            "      {\n",
            "        \"text\": \"\\n\\n[\\\"The Waste Land\\\",\\\"T.S. Eliot\\\",\\\"April is the cruelest month, breeding lilacs out of the dead land\\\"]\\n\\n[\\\"The Raven\\\",\\\"Edgar Allan Poe\\\",\\\"Once upon a midnight dreary, while I pondered, weak and weary\\\"]\\n\\n[\\\"Ode to a Nightingale\\\",\\\"John Keats\\\",\\\"Thou still unravish'd bride of quietness, Thou foster-child of silence and slow time\\\"]\",\n",
            "        \"generation_info\": {\n",
            "          \"finish_reason\": \"stop\",\n",
            "          \"logprobs\": null\n",
            "        },\n",
            "        \"type\": \"Generation\"\n",
            "      }\n",
            "    ]\n",
            "  ],\n",
            "  \"llm_output\": {\n",
            "    \"token_usage\": {\n",
            "      \"completion_tokens\": 94,\n",
            "      \"total_tokens\": 118,\n",
            "      \"prompt_tokens\": 24\n",
            "    },\n",
            "    \"model_name\": \"text-davinci-001\"\n",
            "  },\n",
            "  \"run\": null\n",
            "}\n",
            "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:RunnableWithFallbacks > 2:chain:RunnableSequence > 4:chain:loads] Entering Chain run with input:\n",
            "\u001b[0m{\n",
            "  \"input\": \"\\n\\n[\\\"The Waste Land\\\",\\\"T.S. Eliot\\\",\\\"April is the cruelest month, breeding lilacs out of the dead land\\\"]\\n\\n[\\\"The Raven\\\",\\\"Edgar Allan Poe\\\",\\\"Once upon a midnight dreary, while I pondered, weak and weary\\\"]\\n\\n[\\\"Ode to a Nightingale\\\",\\\"John Keats\\\",\\\"Thou still unravish'd bride of quietness, Thou foster-child of silence and slow time\\\"]\"\n",
            "}\n",
            "\u001b[31;1m\u001b[1;3m[chain/error]\u001b[0m \u001b[1m[1:chain:RunnableWithFallbacks > 2:chain:RunnableSequence > 4:chain:loads] [1ms] Chain run errored with error:\n",
            "\u001b[0m\"JSONDecodeError('Extra data: line 5 column 1 (char 103)')\"\n",
            "\u001b[31;1m\u001b[1;3m[chain/error]\u001b[0m \u001b[1m[1:chain:RunnableWithFallbacks > 2:chain:RunnableSequence] [1.33s] Chain run errored with error:\n",
            "\u001b[0m\"JSONDecodeError('Extra data: line 5 column 1 (char 103)')\"\n",
            "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:RunnableWithFallbacks > 5:chain:RunnableSequence] Entering Chain run with input:\n",
            "\u001b[0m{\n",
            "  \"input\": \"write three poems in a json blob, where each poem is a json blob of a title, author, and first line\"\n",
            "}\n",
            "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:chain:RunnableWithFallbacks > 5:chain:RunnableSequence > 6:llm:ChatOpenAI] Entering LLM run with input:\n",
            "\u001b[0m{\n",
            "  \"prompts\": [\n",
            "    \"Human: write three poems in a json blob, where each poem is a json blob of a title, author, and first line\"\n",
            "  ]\n",
            "}\n",
            "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[1:chain:RunnableWithFallbacks > 5:chain:RunnableSequence > 6:llm:ChatOpenAI] [15.70s] Exiting LLM run with output:\n",
            "\u001b[0m{\n",
            "  \"generations\": [\n",
            "    [\n",
            "      {\n",
            "        \"text\": \"{\\n  \\\"poem1\\\": {\\n    \\\"title\\\": \\\"Whispers of the Wind\\\",\\n    \\\"author\\\": \\\"Emily Rivers\\\",\\n    \\\"first_line\\\": \\\"Softly it comes, the whisper of the wind\\\"\\n  },\\n  \\\"poem2\\\": {\\n    \\\"title\\\": \\\"Silent Serenade\\\",\\n    \\\"author\\\": \\\"Jacob Moore\\\",\\n    \\\"first_line\\\": \\\"In the stillness of night, a silent serenade\\\"\\n  },\\n  \\\"poem3\\\": {\\n    \\\"title\\\": \\\"Dancing Shadows\\\",\\n    \\\"author\\\": \\\"Sophia Anderson\\\",\\n    \\\"first_line\\\": \\\"Shadows dance upon the moonlit floor\\\"\\n  }\\n}\",\n",
            "        \"generation_info\": {\n",
            "          \"finish_reason\": \"stop\"\n",
            "        },\n",
            "        \"type\": \"ChatGeneration\",\n",
            "        \"message\": {\n",
            "          \"lc\": 1,\n",
            "          \"type\": \"constructor\",\n",
            "          \"id\": [\n",
            "            \"langchain\",\n",
            "            \"schema\",\n",
            "            \"messages\",\n",
            "            \"AIMessage\"\n",
            "          ],\n",
            "          \"kwargs\": {\n",
            "            \"content\": \"{\\n  \\\"poem1\\\": {\\n    \\\"title\\\": \\\"Whispers of the Wind\\\",\\n    \\\"author\\\": \\\"Emily Rivers\\\",\\n    \\\"first_line\\\": \\\"Softly it comes, the whisper of the wind\\\"\\n  },\\n  \\\"poem2\\\": {\\n    \\\"title\\\": \\\"Silent Serenade\\\",\\n    \\\"author\\\": \\\"Jacob Moore\\\",\\n    \\\"first_line\\\": \\\"In the stillness of night, a silent serenade\\\"\\n  },\\n  \\\"poem3\\\": {\\n    \\\"title\\\": \\\"Dancing Shadows\\\",\\n    \\\"author\\\": \\\"Sophia Anderson\\\",\\n    \\\"first_line\\\": \\\"Shadows dance upon the moonlit floor\\\"\\n  }\\n}\",\n",
            "            \"additional_kwargs\": {}\n",
            "          }\n",
            "        }\n",
            "      }\n",
            "    ]\n",
            "  ],\n",
            "  \"llm_output\": {\n",
            "    \"token_usage\": {\n",
            "      \"completion_tokens\": 134,\n",
            "      \"prompt_tokens\": 31,\n",
            "      \"total_tokens\": 165\n",
            "    },\n",
            "    \"model_name\": \"gpt-3.5-turbo\",\n",
            "    \"system_fingerprint\": null\n",
            "  },\n",
            "  \"run\": null\n",
            "}\n",
            "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:RunnableWithFallbacks > 5:chain:RunnableSequence > 7:parser:StrOutputParser] Entering Parser run with input:\n",
            "\u001b[0m[inputs]\n",
            "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:RunnableWithFallbacks > 5:chain:RunnableSequence > 7:parser:StrOutputParser] [0ms] Exiting Parser run with output:\n",
            "\u001b[0m{\n",
            "  \"output\": \"{\\n  \\\"poem1\\\": {\\n    \\\"title\\\": \\\"Whispers of the Wind\\\",\\n    \\\"author\\\": \\\"Emily Rivers\\\",\\n    \\\"first_line\\\": \\\"Softly it comes, the whisper of the wind\\\"\\n  },\\n  \\\"poem2\\\": {\\n    \\\"title\\\": \\\"Silent Serenade\\\",\\n    \\\"author\\\": \\\"Jacob Moore\\\",\\n    \\\"first_line\\\": \\\"In the stillness of night, a silent serenade\\\"\\n  },\\n  \\\"poem3\\\": {\\n    \\\"title\\\": \\\"Dancing Shadows\\\",\\n    \\\"author\\\": \\\"Sophia Anderson\\\",\\n    \\\"first_line\\\": \\\"Shadows dance upon the moonlit floor\\\"\\n  }\\n}\"\n",
            "}\n",
            "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:RunnableWithFallbacks > 5:chain:RunnableSequence > 8:chain:loads] Entering Chain run with input:\n",
            "\u001b[0m{\n",
            "  \"input\": \"{\\n  \\\"poem1\\\": {\\n    \\\"title\\\": \\\"Whispers of the Wind\\\",\\n    \\\"author\\\": \\\"Emily Rivers\\\",\\n    \\\"first_line\\\": \\\"Softly it comes, the whisper of the wind\\\"\\n  },\\n  \\\"poem2\\\": {\\n    \\\"title\\\": \\\"Silent Serenade\\\",\\n    \\\"author\\\": \\\"Jacob Moore\\\",\\n    \\\"first_line\\\": \\\"In the stillness of night, a silent serenade\\\"\\n  },\\n  \\\"poem3\\\": {\\n    \\\"title\\\": \\\"Dancing Shadows\\\",\\n    \\\"author\\\": \\\"Sophia Anderson\\\",\\n    \\\"first_line\\\": \\\"Shadows dance upon the moonlit floor\\\"\\n  }\\n}\"\n",
            "}\n",
            "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:RunnableWithFallbacks > 5:chain:RunnableSequence > 8:chain:loads] [0ms] Exiting Chain run with output:\n",
            "\u001b[0m{\n",
            "  \"poem1\": {\n",
            "    \"title\": \"Whispers of the Wind\",\n",
            "    \"author\": \"Emily Rivers\",\n",
            "    \"first_line\": \"Softly it comes, the whisper of the wind\"\n",
            "  },\n",
            "  \"poem2\": {\n",
            "    \"title\": \"Silent Serenade\",\n",
            "    \"author\": \"Jacob Moore\",\n",
            "    \"first_line\": \"In the stillness of night, a silent serenade\"\n",
            "  },\n",
            "  \"poem3\": {\n",
            "    \"title\": \"Dancing Shadows\",\n",
            "    \"author\": \"Sophia Anderson\",\n",
            "    \"first_line\": \"Shadows dance upon the moonlit floor\"\n",
            "  }\n",
            "}\n",
            "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:RunnableWithFallbacks > 5:chain:RunnableSequence] [15.71s] Exiting Chain run with output:\n",
            "\u001b[0m{\n",
            "  \"poem1\": {\n",
            "    \"title\": \"Whispers of the Wind\",\n",
            "    \"author\": \"Emily Rivers\",\n",
            "    \"first_line\": \"Softly it comes, the whisper of the wind\"\n",
            "  },\n",
            "  \"poem2\": {\n",
            "    \"title\": \"Silent Serenade\",\n",
            "    \"author\": \"Jacob Moore\",\n",
            "    \"first_line\": \"In the stillness of night, a silent serenade\"\n",
            "  },\n",
            "  \"poem3\": {\n",
            "    \"title\": \"Dancing Shadows\",\n",
            "    \"author\": \"Sophia Anderson\",\n",
            "    \"first_line\": \"Shadows dance upon the moonlit floor\"\n",
            "  }\n",
            "}\n",
            "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:RunnableWithFallbacks] [17.05s] Exiting Chain run with output:\n",
            "\u001b[0m{\n",
            "  \"poem1\": {\n",
            "    \"title\": \"Whispers of the Wind\",\n",
            "    \"author\": \"Emily Rivers\",\n",
            "    \"first_line\": \"Softly it comes, the whisper of the wind\"\n",
            "  },\n",
            "  \"poem2\": {\n",
            "    \"title\": \"Silent Serenade\",\n",
            "    \"author\": \"Jacob Moore\",\n",
            "    \"first_line\": \"In the stillness of night, a silent serenade\"\n",
            "  },\n",
            "  \"poem3\": {\n",
            "    \"title\": \"Dancing Shadows\",\n",
            "    \"author\": \"Sophia Anderson\",\n",
            "    \"first_line\": \"Shadows dance upon the moonlit floor\"\n",
            "  }\n",
            "}\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'poem1': {'title': 'Whispers of the Wind',\n",
              "  'author': 'Emily Rivers',\n",
              "  'first_line': 'Softly it comes, the whisper of the wind'},\n",
              " 'poem2': {'title': 'Silent Serenade',\n",
              "  'author': 'Jacob Moore',\n",
              "  'first_line': 'In the stillness of night, a silent serenade'},\n",
              " 'poem3': {'title': 'Dancing Shadows',\n",
              "  'author': 'Sophia Anderson',\n",
              "  'first_line': 'Shadows dance upon the moonlit floor'}}"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "langchain.debug = False"
      ],
      "metadata": {
        "id": "szXbn70xGKCB"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### `Interface`"
      ],
      "metadata": {
        "id": "Ecy-SD9QF0vP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = ChatPromptTemplate.from_template(\n",
        "    \"Tell me a short joke about {topic}\"\n",
        ")\n",
        "model = ChatOpenAI()\n",
        "output_parser = StrOutputParser()\n",
        "\n",
        "chain = prompt | model | output_parser"
      ],
      "metadata": {
        "id": "BdWgZoAXFui8"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We will explore few different elements of the interface"
      ],
      "metadata": {
        "id": "aJrx69_7GOCx"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain.invoke({\"topic\": \"bears\"}) # synchronous method that calls it on 1 single input."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "yIQvpwo-GFMW",
        "outputId": "799766ec-d05b-4257-91bd-6638c4c5cd25"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Why don't bears wear shoes? \\n\\nBecause they already have bear feet!\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chain.batch([{\"topic\": \"bears\"}, {\"topic\": \"frogs\"}]) # synchronous method that calls it on list of inputs."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lwq0emARGHtU",
        "outputId": "3234afab-1e55-47a6-d338-1feeae4eebb1"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"Why don't bears wear shoes?\\n\\nBecause they have bear feet!\",\n",
              " 'Why are frogs so happy?\\n\\nBecause they eat whatever bugs them!']"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# stream back the output based on tokens generated (ideal for showcasing a streaming property when recording bot-replies in chat-interface)\n",
        "for t in chain.stream({\"topic\": \"bears\"}):\n",
        "    print(t)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jc_5Ym47GlFt",
        "outputId": "e8ec8925-8e2f-4aad-c04d-16dc0a12fe9a"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Why\n",
            " don\n",
            "'t\n",
            " bears\n",
            " wear\n",
            " socks\n",
            "?\n",
            " \n",
            "\n",
            "\n",
            "Because\n",
            " they\n",
            " have\n",
            " bear\n",
            " feet\n",
            "!\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# calls it in a asynchronous manner\n",
        "response = await chain.ainvoke({\"topic\": \"bears\"})\n",
        "response"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "QN6hbpj-GsWE",
        "outputId": "d2307988-e966-49ad-fc8c-71da88a94fb9"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Why don't bears wear shoes? \\n\\nBecause they have bear feet!\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### `OpenAI Function Calling in LangChain`"
      ],
      "metadata": {
        "id": "WnbiHXcdHxxz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Objective: How to use OpenAI functions and combine them with LCEL (LangChain Expression Language)\n",
        "# Additionally, explore pydantic which makes it easier to work with OpenAI functions."
      ],
      "metadata": {
        "id": "Pr4KsMpjHMLu"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List\n",
        "from pydantic import BaseModel, Field"
      ],
      "metadata": {
        "id": "d1xEb5LeK-Mf"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### `Pydantic Syntax`\n",
        "`Pydantic data classes are a blend of Python's data classes with the validation power of pydantic (since in normal python we specify data-types for each attribute but they're not validated and are just used for user reference)`\n",
        "\n",
        "`They offer a concise way to define data structures while ensuring that the data adheres to specified types and constraints.`"
      ],
      "metadata": {
        "id": "O0XYDF2YNWPJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# in standard python you would create a class like this:\n",
        "class User:\n",
        "    def __init__(self, name: str, age: int, email: str):\n",
        "        self.name = name\n",
        "        self.age = age\n",
        "        self.email = email"
      ],
      "metadata": {
        "id": "T1Nfu6SGNVQu"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "foo = User(name=\"Joe\", age=32, email=\"joe@gmail.com\")\n",
        "foo.name"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "ekpZcFR-N_JR",
        "outputId": "258adf61-1e2e-4bd7-c693-d1e7208264ac"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Joe'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "foo = User(name=\"Joe\", age=\"bar\", email=\"joe@gmail.com\")\n",
        "foo.age # expected data-type for attr. `age` was \"int\" but the class cannot validate and accepted \"str\" too."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "YL8-46CvOK9s",
        "outputId": "451825ac-97c1-4007-a10d-7615c0ad1f43"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'bar'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class pUser(BaseModel):\n",
        "    name: str\n",
        "    age: int\n",
        "    email: str"
      ],
      "metadata": {
        "id": "EO3_uE_rORXF"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "foo_p = pUser(name=\"Joe\", age=32, email=\"joe@gmail.com\")\n",
        "foo_p.name"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "k4x5hoOtPBZh",
        "outputId": "83afa75e-1fef-41d9-e7b3-269c7bba53f2"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Joe'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Note: below code will give ValidationError, since attr. `age` is supposed to be \"int\"\n",
        "# foo_p = pUser(name=\"Joe\", age=\"bar\", email=\"joe@gmail.com\")"
      ],
      "metadata": {
        "id": "p-tM-lqCPGBu"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# with Pydantic we can nest these data-structures\n",
        "class Class(BaseModel):\n",
        "    students: List[pUser]"
      ],
      "metadata": {
        "id": "CuDnjIokPWFe"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "obj = Class(\n",
        "    students=[pUser(name=\"Joe\", age=32, email=\"joe@gmail.com\")]\n",
        ")"
      ],
      "metadata": {
        "id": "sVEifobYPurH"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "obj"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FKZHhGJfP6u7",
        "outputId": "bc72d86b-8445-4337-b090-4aa72cb60730"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Class(students=[pUser(name='Joe', age=32, email='joe@gmail.com')])"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### `Pydantic to OpenAI function definition`"
      ],
      "metadata": {
        "id": "iVqr03ZFQAMi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class WeatherSearch(BaseModel):\n",
        "    \"\"\"Call this with an airport code to get the weather at that airport\"\"\"\n",
        "    airport_code: str = Field(description=\"airport code to get weather for\")"
      ],
      "metadata": {
        "id": "GAxrI9TWP7jA"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.utils.openai_functions import convert_pydantic_to_openai_function"
      ],
      "metadata": {
        "id": "ifbwAEuMQRPf"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "weather_function = convert_pydantic_to_openai_function(WeatherSearch)"
      ],
      "metadata": {
        "id": "mSyboysyQUo5"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "weather_function # the main description is pulled from the DocString of the class definition"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r0JgzY7CQaVo",
        "outputId": "bbfdf776-745a-4162-8a85-de7d9aa2a5e2"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'name': 'WeatherSearch',\n",
              " 'description': 'Call this with an airport code to get the weather at that airport',\n",
              " 'parameters': {'title': 'WeatherSearch',\n",
              "  'description': 'Call this with an airport code to get the weather at that airport',\n",
              "  'type': 'object',\n",
              "  'properties': {'airport_code': {'title': 'Airport Code',\n",
              "    'description': 'airport code to get weather for',\n",
              "    'type': 'string'}},\n",
              "  'required': ['airport_code']}}"
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class WeatherSearch(BaseModel):\n",
        "    airport_code: str = Field(description=\"airport code to get weather for\")"
      ],
      "metadata": {
        "id": "dQuVKRVhRJ6q"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# DocString is a required **compulsory when defining a class definition inherited from Pydantic BaseModel\n",
        "# Note: the below code will produce error a `KeyError` on calling `convert_pydantic_to_openai_function`\n",
        "# since the DocString is not provided\n",
        "# convert_pydantic_to_openai_function(WeatherSearch)"
      ],
      "metadata": {
        "id": "0N1Wg4-dQa0f"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# However, when defining the attributes it is not compulsory to provide the description\n",
        "class WeatherSearch2(BaseModel):\n",
        "    \"\"\"Call this with an airport code to get the weather at that airport\"\"\"\n",
        "    airport_code: str"
      ],
      "metadata": {
        "id": "78vNJxprRQ1r"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "convert_pydantic_to_openai_function(WeatherSearch2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vdqgJNMTRrFQ",
        "outputId": "309e53bd-b948-4d08-9cd5-4869dd8070ed"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'name': 'WeatherSearch2',\n",
              " 'description': 'Call this with an airport code to get the weather at that airport',\n",
              " 'parameters': {'title': 'WeatherSearch2',\n",
              "  'description': 'Call this with an airport code to get the weather at that airport',\n",
              "  'type': 'object',\n",
              "  'properties': {'airport_code': {'title': 'Airport Code', 'type': 'string'}},\n",
              "  'required': ['airport_code']}}"
            ]
          },
          "metadata": {},
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# combining OpenAI function with LCEL\n",
        "from langchain.chat_models import ChatOpenAI"
      ],
      "metadata": {
        "id": "AUhlVa07RuFz"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = ChatOpenAI()"
      ],
      "metadata": {
        "id": "kbwN9aDtR1Sz"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.invoke(\"what is the weather in SF today?\", functions=[weather_function]) # w/ keyword args `functions`"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PgRZFLaFR4-s",
        "outputId": "3d6d1edc-9982-4adf-c564-954e3868c633"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{\\n  \"airport_code\": \"SFO\"\\n}', 'name': 'WeatherSearch'}})"
            ]
          },
          "metadata": {},
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# we can bind the function invocation to the model,\n",
        "# reason: we can pass that {model, function} pair together and\n",
        "# do not have to worry about the explicit mentioning of kwargs\n",
        "model_with_function = model.bind(functions=[weather_function])"
      ],
      "metadata": {
        "id": "jH2SW81CSJeb"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_with_function.invoke(\"what is the weather in SF?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b6wuibyMSi-k",
        "outputId": "4b148b6f-97c3-4cbe-9694-e8a1bc5614d7"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{\\n  \"airport_code\": \"SFO\"\\n}', 'name': 'WeatherSearch'}})"
            ]
          },
          "metadata": {},
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### `Forcing it to use a function`"
      ],
      "metadata": {
        "id": "XNhoKqmmStpY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_with_forced_function = model.bind(functions=[weather_function], function_call={\"name\": \"WeatherSearch\"})"
      ],
      "metadata": {
        "id": "17Rb5IUlSmcD"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_with_forced_function.invoke(\"what is the weather in SF?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jYvIN6XbS2dV",
        "outputId": "31b33081-60f6-464a-ed4c-3582a8be1169"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{\\n  \"airport_code\": \"SFO\"\\n}', 'name': 'WeatherSearch'}})"
            ]
          },
          "metadata": {},
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_with_forced_function.invoke(\"hi!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B1J7nMPYS5mL",
        "outputId": "ac9d7111-15d1-4e30-bed6-8c14b0446d11"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{\\n  \"airport_code\": \"LAX\"\\n}', 'name': 'WeatherSearch'}})"
            ]
          },
          "metadata": {},
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### `Using in a Chain`"
      ],
      "metadata": {
        "id": "TchZNvPdTADm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import ChatPromptTemplate"
      ],
      "metadata": {
        "id": "ZkA8fK0xS9tD"
      },
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"You are a helpful assistant\"),\n",
        "    (\"user\", \"{input}\")\n",
        "])"
      ],
      "metadata": {
        "id": "VAWebDO2TDy6"
      },
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain = prompt | model_with_function"
      ],
      "metadata": {
        "id": "YqFrYWu4TK1B"
      },
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain.invoke({\"input\": \"what is the weather in SF?\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ya_-t8-STNYq",
        "outputId": "2098db34-f9de-46b7-9c76-ccd0770b0c0e"
      },
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{\\n  \"airport_code\": \"SFO\"\\n}', 'name': 'WeatherSearch'}})"
            ]
          },
          "metadata": {},
          "execution_count": 95
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### `Using multiple functions`"
      ],
      "metadata": {
        "id": "HLnbSUROTUB7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# we can pass a set of functions and let the LLM decide which to use based on the question context."
      ],
      "metadata": {
        "id": "RRuEVUfvTQDH"
      },
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class WeatherSearch(BaseModel):\n",
        "    \"\"\"Call this with an airport code to get the weather at that airport\"\"\"\n",
        "    airport_code: str = Field(description=\"airport code to get weather for\")\n",
        "\n",
        "class ArtistSearch(BaseModel):\n",
        "    \"\"\"Call this to get the names of songs by a particular artist\"\"\"\n",
        "    artist_name: str = Field(description=\"name of the artisit to look up\")\n",
        "    n: int = Field(description=\"number of results\")"
      ],
      "metadata": {
        "id": "FSdpAIijTim0"
      },
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "functions = [\n",
        "    convert_pydantic_to_openai_function(WeatherSearch),\n",
        "    convert_pydantic_to_openai_function(ArtistSearch)\n",
        "]"
      ],
      "metadata": {
        "id": "lj4xRFcTTz6r"
      },
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_with_functions = model.bind(functions=functions)"
      ],
      "metadata": {
        "id": "9aKv8uo2T3--"
      },
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_with_functions.invoke(\"what is the weather in SF?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1fhsxgkqUDMx",
        "outputId": "939dea3d-8d09-4b48-af13-fcbb194e25f4"
      },
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{\\n\"airport_code\": \"SFO\"\\n}', 'name': 'WeatherSearch'}})"
            ]
          },
          "metadata": {},
          "execution_count": 102
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_with_functions.invoke(\"what are three songs by taylor swift?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1aFVMo9fUHmd",
        "outputId": "cfd429c0-0318-4afa-9223-69d3d279386b"
      },
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{\\n\"artist_name\": \"taylor swift\",\\n\"n\": 3\\n}', 'name': 'ArtistSearch'}})"
            ]
          },
          "metadata": {},
          "execution_count": 103
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_with_functions.invoke(\"hi!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H0rkKFnzUMFk",
        "outputId": "f1c18e55-10f4-444e-f07b-856cb0b27360"
      },
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='Hello! How can I assist you today?')"
            ]
          },
          "metadata": {},
          "execution_count": 104
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### `Tagging and Extraction`\n",
        "`Allows to extract structured data from unstructured text`"
      ],
      "metadata": {
        "id": "PEDeYcqqUYU7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List\n",
        "from pydantic import BaseModel, Field\n",
        "from langchain.utils.openai_functions import convert_pydantic_to_openai_function"
      ],
      "metadata": {
        "id": "DkuK5qCKUOwp"
      },
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### `Tagging`"
      ],
      "metadata": {
        "id": "sdKQ_JXUc3a9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Tagging(BaseModel):\n",
        "    \"\"\"Tag the piece of text with particular info.\"\"\"\n",
        "    sentiment: str = Field(description=\"sentiment of text, should be `pos`, `neg` or `neutral`\") # through the tags (i.e. `pos`, `neg`, `neutral`) we're telling the LLM how the data should be structured,\n",
        "    language: str = Field(description=\"language of text (should be ISO 639-1 code)\")"
      ],
      "metadata": {
        "id": "4uRoUvYObXj0"
      },
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "convert_pydantic_to_openai_function(Tagging)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y6Zeu66JbqHW",
        "outputId": "4e774e2d-9f7e-4910-bcba-076b8046b385"
      },
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'name': 'Tagging',\n",
              " 'description': 'Tag the piece of text with particular info.',\n",
              " 'parameters': {'title': 'Tagging',\n",
              "  'description': 'Tag the piece of text with particular info.',\n",
              "  'type': 'object',\n",
              "  'properties': {'sentiment': {'title': 'Sentiment',\n",
              "    'description': 'sentiment of text, should be `pos`, `neg` or `neutral`',\n",
              "    'type': 'string'},\n",
              "   'language': {'title': 'Language',\n",
              "    'description': 'language of text (should be ISO 639-1 code)',\n",
              "    'type': 'string'}},\n",
              "  'required': ['sentiment', 'language']}}"
            ]
          },
          "metadata": {},
          "execution_count": 109
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.chat_models import ChatOpenAI"
      ],
      "metadata": {
        "id": "as63BtVebsII"
      },
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = ChatOpenAI(temperature=0.)"
      ],
      "metadata": {
        "id": "nOhhx72vb9jQ"
      },
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tagging_functions = [convert_pydantic_to_openai_function(Tagging)]"
      ],
      "metadata": {
        "id": "dkLEPtfHb_o3"
      },
      "execution_count": 112,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"Think carefully, and then tag the text as instructed\"),\n",
        "    (\"user\", \"{input}\")\n",
        "])"
      ],
      "metadata": {
        "id": "G34VtvzucB9S"
      },
      "execution_count": 113,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_with_functions = model.bind(\n",
        "    functions=tagging_functions,\n",
        "    function_call={\"name\": \"Tagging\"}\n",
        ")"
      ],
      "metadata": {
        "id": "gLwK254-cJ_z"
      },
      "execution_count": 114,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tagging_chain = prompt | model_with_functions"
      ],
      "metadata": {
        "id": "cCCq5jWQcRxv"
      },
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tagging_chain.invoke({\"input\": \"I love langchain\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MBBeNvKMcU0c",
        "outputId": "c218941a-2c0a-42a9-ade6-6b21ea42f091"
      },
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{\\n  \"sentiment\": \"pos\",\\n  \"language\": \"en\"\\n}', 'name': 'Tagging'}})"
            ]
          },
          "metadata": {},
          "execution_count": 116
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tagging_chain.invoke({\"input\": \"no mi piace questo cibo\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wLn9s-F_cYlk",
        "outputId": "3848cf13-a00a-441c-c2b1-50cc6e00ebee"
      },
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{\\n  \"sentiment\": \"neg\",\\n  \"language\": \"it\"\\n}', 'name': 'Tagging'}})"
            ]
          },
          "metadata": {},
          "execution_count": 117
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.output_parsers.openai_functions import JsonOutputFunctionsParser"
      ],
      "metadata": {
        "id": "mbUFW8Qwcqj2"
      },
      "execution_count": 118,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tagging_chain = prompt | model_with_functions | JsonOutputFunctionsParser()"
      ],
      "metadata": {
        "id": "qBffyqITce2j"
      },
      "execution_count": 119,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tagging_chain.invoke({\"input\": \"non mi piace questo cibo\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jEibUJHZcvsd",
        "outputId": "6d384e2d-93f5-49aa-c047-4ee7a402f391"
      },
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'sentiment': 'neg', 'language': 'it'}"
            ]
          },
          "metadata": {},
          "execution_count": 120
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### `Extraction`\n",
        "`Extraction is similar to tagging, but used for extracting multiple piece of information`"
      ],
      "metadata": {
        "id": "qJEmbYayc5Uj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Optional\n",
        "class Person(BaseModel):\n",
        "    \"\"\"Information about a Person.\"\"\"\n",
        "    name: str = Field(description=\"person's name\")\n",
        "    age: Optional[int] = Field(description=\"person's age\")"
      ],
      "metadata": {
        "id": "DU8nYwYXc0FY"
      },
      "execution_count": 123,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Information(BaseModel):\n",
        "    \"\"\"Information to extract\"\"\"\n",
        "    people: List[Person] = Field(description=\"List of info about people\")"
      ],
      "metadata": {
        "id": "Td8q36AFdavJ"
      },
      "execution_count": 124,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "convert_pydantic_to_openai_function(Information)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WWuxN5_jduR9",
        "outputId": "bd8cf71f-75ec-4015-9a84-bc68c5849f12"
      },
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'name': 'Information',\n",
              " 'description': 'Information to extract',\n",
              " 'parameters': {'title': 'Information',\n",
              "  'description': 'Information to extract',\n",
              "  'type': 'object',\n",
              "  'properties': {'people': {'title': 'People',\n",
              "    'description': 'List of info about people',\n",
              "    'type': 'array',\n",
              "    'items': {'title': 'Person',\n",
              "     'description': 'Information about a Person.',\n",
              "     'type': 'object',\n",
              "     'properties': {'name': {'title': 'Name',\n",
              "       'description': \"person's name\",\n",
              "       'type': 'string'},\n",
              "      'age': {'title': 'Age',\n",
              "       'description': \"person's age\",\n",
              "       'type': 'integer'}},\n",
              "     'required': ['name']}}},\n",
              "  'required': ['people']}}"
            ]
          },
          "metadata": {},
          "execution_count": 125
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "extraction_functions = [convert_pydantic_to_openai_function(Information)]\n",
        "extraction_model = model.bind(functions=extraction_functions, function_call={\"name\": \"Information\"})"
      ],
      "metadata": {
        "id": "-4MrEN_PdwiP"
      },
      "execution_count": 126,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "extraction_model.invoke(\"Joe is 30, his mom is Martha\") # model seems to think when it doenot know a person's age, then \"age=0\", but we can do better!"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YckXvY1Pd8tK",
        "outputId": "d89ff485-5556-48ba-8f7f-405d853d2b88"
      },
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{\\n  \"people\": [\\n    {\\n      \"name\": \"Joe\",\\n      \"age\": 30\\n    },\\n    {\\n      \"name\": \"Martha\",\\n      \"age\": 0\\n    }\\n  ]\\n}', 'name': 'Information'}})"
            ]
          },
          "metadata": {},
          "execution_count": 128
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Note: below prompt makes sure via `system-prior` that when a particular\n",
        "# attribute value is unknown, then ignore that attribute for that instance.\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"Extract the relevant information, if not explicitly provided do not guess. Extract partial info\"),\n",
        "    (\"user\", \"{input}\")\n",
        "])"
      ],
      "metadata": {
        "id": "JocGGU5feCw_"
      },
      "execution_count": 129,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "extraction_chain = prompt | extraction_model"
      ],
      "metadata": {
        "id": "BcKJwLzievJ3"
      },
      "execution_count": 130,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "extraction_chain.invoke({\"input\": \"Joe is 30, his mom is Martha\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XQEWbzEleyYZ",
        "outputId": "a702dada-17d7-45c0-9859-3f112d64c52c"
      },
      "execution_count": 131,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{\\n  \"people\": [\\n    {\\n      \"name\": \"Joe\",\\n      \"age\": 30\\n    },\\n    {\\n      \"name\": \"Martha\"\\n    }\\n  ]\\n}', 'name': 'Information'}})"
            ]
          },
          "metadata": {},
          "execution_count": 131
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "extraction_chain = prompt | extraction_model | JsonOutputFunctionsParser()"
      ],
      "metadata": {
        "id": "K_nxWcrVe3Zz"
      },
      "execution_count": 132,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Note: considering from the standpoint of `Extraction procedure`, we do not require\n",
        "# the `people` item to be specified, what we want is just a list of dictionaries\n",
        "# constituting the function calls\n",
        "extraction_chain.invoke({\"input\": \"Joe is 30, his mom is Martha\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PsagjXBHfBvx",
        "outputId": "84bf3909-958d-41b7-f794-84ff6537ebc1"
      },
      "execution_count": 136,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'people': [{'name': 'Joe', 'age': 30}, {'name': 'Martha'}]}"
            ]
          },
          "metadata": {},
          "execution_count": 136
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# looks for particular key in the output and extract only that\n",
        "from langchain.output_parsers.openai_functions import JsonKeyOutputFunctionsParser"
      ],
      "metadata": {
        "id": "g5QX3lPSfIb-"
      },
      "execution_count": 142,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "extraction_chain = prompt | extraction_model | JsonKeyOutputFunctionsParser(key_name=\"people\")"
      ],
      "metadata": {
        "id": "Vapn-_gmgANM"
      },
      "execution_count": 141,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "extraction_chain.invoke({\"input\": \"Joe is 30, his mom is Martha\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l0uvjx0KgDSP",
        "outputId": "8be201bc-adc5-42d5-81ef-d216a59efd75"
      },
      "execution_count": 140,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'name': 'Joe', 'age': 30}, {'name': 'Martha'}]"
            ]
          },
          "metadata": {},
          "execution_count": 140
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### `Doing it for real`\n",
        "`We can apply tagging to a larger body of text.`\n",
        "`For example, let's load this blog post and extract tag information from a sub-set of the text`"
      ],
      "metadata": {
        "id": "CLzwTR5JgUAb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import WebBaseLoader\n",
        "loader = WebBaseLoader(\"https://lilianweng.github.io/posts/2023-06-23-agent/\")\n",
        "documents = loader.load()"
      ],
      "metadata": {
        "id": "9kIYHNdmgKxN"
      },
      "execution_count": 144,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc = documents[0]"
      ],
      "metadata": {
        "id": "8h7Dnye9jx2J"
      },
      "execution_count": 145,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "page_content = doc.page_content[:10000]\n",
        "print(page_content[:1000])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jtHIiOhpj4CZ",
        "outputId": "01290b6f-930a-4d3c-a9f9-0f631dd87299"
      },
      "execution_count": 147,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "LLM Powered Autonomous Agents | Lil'Log\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Lil'Log\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Posts\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Archive\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Search\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Tags\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "FAQ\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "emojisearch.app\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "      LLM Powered Autonomous Agents\n",
            "    \n",
            "Date: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian Weng\n",
            "\n",
            "\n",
            " \n",
            "\n",
            "\n",
            "Table of Contents\n",
            "\n",
            "\n",
            "\n",
            "Agent System Overview\n",
            "\n",
            "Component One: Planning\n",
            "\n",
            "Task Decomposition\n",
            "\n",
            "Self-Reflection\n",
            "\n",
            "\n",
            "Component Two: Memory\n",
            "\n",
            "Types of Memory\n",
            "\n",
            "Maximum Inner Product Search (MIPS)\n",
            "\n",
            "\n",
            "Component Three: Tool Use\n",
            "\n",
            "Case Studies\n",
            "\n",
            "Scientific Discovery Agent\n",
            "\n",
            "Generative Agents Simulation\n",
            "\n",
            "Proof-of-Concept Examples\n",
            "\n",
            "\n",
            "Challenges\n",
            "\n",
            "Citation\n",
            "\n",
            "References\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Overview(BaseModel):\n",
        "    \"\"\"Overview of a section of text.\"\"\"\n",
        "    summary: str = Field(description=\"Provide a concise summary of the content.\")\n",
        "    language: str = Field(description=\"Provide the language that the content is written in.\")\n",
        "    keywords: str = Field(description=\"Provide keywords related to the content.\")"
      ],
      "metadata": {
        "id": "jYpBJAvBj7fJ"
      },
      "execution_count": 148,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "overview_tagging_functions = [\n",
        "    convert_pydantic_to_openai_function(Overview)\n",
        "]\n",
        "tagging_model = model.bind(\n",
        "    functions=overview_tagging_functions,\n",
        "    function_call={\"name\": \"Overview\"}\n",
        ")\n",
        "tagging_chain = prompt | tagging_model | JsonOutputFunctionsParser()"
      ],
      "metadata": {
        "id": "8r-pjVqFka7K"
      },
      "execution_count": 151,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tagging_chain.invoke({\"input\": page_content})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sRxsDvkmkqpt",
        "outputId": "a0af2693-5ef5-4a14-cdc6-3ba3aa487828"
      },
      "execution_count": 152,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'summary': 'This article discusses the concept of building autonomous agents powered by LLM (large language model) as their core controller. It explores the key components of such agent systems, including planning, memory, and tool use. It also covers various techniques for task decomposition and self-reflection in autonomous agents. The article provides examples of case studies and challenges in implementing LLM-powered agents.',\n",
              " 'language': 'English',\n",
              " 'keywords': 'LLM, autonomous agents, planning, memory, tool use, task decomposition, self-reflection, case studies, challenges'}"
            ]
          },
          "metadata": {},
          "execution_count": 152
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Paper(BaseModel):\n",
        "    \"\"\"Information about papers mentioned.\"\"\"\n",
        "    title: str\n",
        "    author: Optional[str]\n",
        "\n",
        "class Info(BaseModel):\n",
        "    \"\"\"Information to extract.\"\"\"\n",
        "    papers: List[Paper]"
      ],
      "metadata": {
        "id": "sLugzp2UktZB"
      },
      "execution_count": 153,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "paper_extraction_function = [\n",
        "    convert_pydantic_to_openai_function(Info)\n",
        "]\n",
        "extraction_model = model.bind(\n",
        "    functions=paper_extraction_function,\n",
        "    function_call={\"name\": \"Info\"}\n",
        ")\n",
        "extraction_chain = prompt | extraction_model | JsonKeyOutputFunctionsParser(key_name=\"papers\")"
      ],
      "metadata": {
        "id": "SwiqKca2lIIw"
      },
      "execution_count": 155,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "extraction_chain.invoke({\"input\": page_content})\n",
        "# this is actually title we're passing in, we don't want that, it should rather extract the papers mentioned within."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lCYP-5w6lm8C",
        "outputId": "8d088de7-06b3-4904-eb15-cddf93ee7ce0"
      },
      "execution_count": 158,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'title': 'LLM Powered Autonomous Agents', 'author': 'Lilian Weng'}]"
            ]
          },
          "metadata": {},
          "execution_count": 158
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "template = \"\"\"A article will be passed to you. Extract from it all papers that are mentioned by this article.\n",
        "\n",
        "Do not extract the name of the article itself. If no papers are mentioned that's fine - you don't need to extract any! Just return an empty list.\n",
        "\n",
        "Do not make up or guess ANY extra information. Only extract what exactly is in the text.\"\"\"\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", template),\n",
        "    (\"human\", \"{input}\")\n",
        "])"
      ],
      "metadata": {
        "id": "wEs4ykUllVFt"
      },
      "execution_count": 156,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "extraction_chain = prompt | extraction_model | JsonKeyOutputFunctionsParser(key_name=\"papers\")"
      ],
      "metadata": {
        "id": "ZQoygKbYlkJA"
      },
      "execution_count": 159,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "extraction_chain.invoke({\"input\": page_content})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lAnnviLyl7O6",
        "outputId": "30672be5-1154-48c0-c8f3-59d3009288b1"
      },
      "execution_count": 161,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'title': 'Chain of thought (CoT; Wei et al. 2022)', 'author': 'Wei et al.'},\n",
              " {'title': 'Tree of Thoughts (Yao et al. 2023)', 'author': 'Yao et al.'},\n",
              " {'title': 'LLM+P (Liu et al. 2023)', 'author': 'Liu et al.'},\n",
              " {'title': 'ReAct (Yao et al. 2023)', 'author': 'Yao et al.'},\n",
              " {'title': 'Reflexion (Shinn & Labash 2023)', 'author': 'Shinn & Labash'},\n",
              " {'title': 'Chain of Hindsight (CoH; Liu et al. 2023)',\n",
              "  'author': 'Liu et al.'},\n",
              " {'title': 'Algorithm Distillation (AD; Laskin et al. 2023)',\n",
              "  'author': 'Laskin et al.'}]"
            ]
          },
          "metadata": {},
          "execution_count": 161
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "extraction_chain.invoke({\"input\": \"hi\"}) # based on the instruction from prompt `return empty list`"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "spl3i8Vfl8pq",
        "outputId": "aaaae524-2c2f-4235-ec72-d45297f8e89e"
      },
      "execution_count": 163,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {},
          "execution_count": 163
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# what is we want to extract all the papers mentioned in the whole article.\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_overlap=0)"
      ],
      "metadata": {
        "id": "GwbOlg5umStN"
      },
      "execution_count": 164,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "splits = text_splitter.split_text(doc.page_content)"
      ],
      "metadata": {
        "id": "qpZTLIoWmpEr"
      },
      "execution_count": 165,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(splits)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hsRP6nmjmsaz",
        "outputId": "b0d48a4f-cf94-4b0e-e4d9-435e84b6521f"
      },
      "execution_count": 166,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "14"
            ]
          },
          "metadata": {},
          "execution_count": 166
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Given the above proc., we are gonna try to automatize the above proc. + extraction + output-parsing\n",
        "# i. We're gonna take the doc.page_content and split it up into chunks\n",
        "# ii. pass individual split to the extraction chain defined above\n",
        "# iii. finally, join all the results"
      ],
      "metadata": {
        "id": "SIH9GSY8mtqt"
      },
      "execution_count": 167,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def flatten(matrix):\n",
        "    flat_list = []\n",
        "    for row in matrix:\n",
        "        flat_list += row\n",
        "    return flat_list"
      ],
      "metadata": {
        "id": "LJbx94DInVJD"
      },
      "execution_count": 168,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "flatten([[1, 2], [3, 4]])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4mv2x42VnY51",
        "outputId": "5ddf7abd-89dc-45a8-f70a-7acc8bcfd53c"
      },
      "execution_count": 169,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1, 2, 3, 4]"
            ]
          },
          "metadata": {},
          "execution_count": 169
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# i. (a) we need to create some-method of split the page-content and passing it into the chain.\n",
        "# for that we require each split (i.e. a string chunk) to be fed as dictionary with the \"input\" key-value\n",
        "from langchain.schema.runnable import RunnableLambda"
      ],
      "metadata": {
        "id": "5oZEyQIFnbMY"
      },
      "execution_count": 175,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# RunnableLambda is simple wrapper in LangChain that takes in a lambda function and converts to Runnable object\n",
        "prep = RunnableLambda(\n",
        "    lambda x: [{\"input\": doc_chunk} for doc_chunk in text_splitter.split_text(x)]\n",
        ")"
      ],
      "metadata": {
        "id": "VLNebKfencYg"
      },
      "execution_count": 176,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prep.invoke(\"hi\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Q8zZOqMoAlE",
        "outputId": "4203ce1f-d12c-4798-a925-e86ec5b771b3"
      },
      "execution_count": 177,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'input': 'hi'}]"
            ]
          },
          "metadata": {},
          "execution_count": 177
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# NOTE: extraction_chain operates on single, and here we have list of elements,\n",
        "# .map() -> take previous input (i.e. list) and map the chain (i.e. extraction_chain) over them.\n",
        "# Additionally, since `flatten` is not the first component so not compulsory to wrap it into a\n",
        "# RunnableLambda (but we can its optional, works either way)\n",
        "chain = prep | extraction_chain.map() | flatten"
      ],
      "metadata": {
        "id": "0SFZaIfCoBbZ"
      },
      "execution_count": 178,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain.invoke(doc.page_content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O9shV-jYo3JA",
        "outputId": "be2e115a-903d-443a-b624-1238451ebabf"
      },
      "execution_count": 179,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'title': 'AutoGPT', 'author': ''},\n",
              " {'title': 'GPT-Engineer', 'author': ''},\n",
              " {'title': 'BabyAGI', 'author': ''},\n",
              " {'title': 'Chain of thought (CoT; Wei et al. 2022)', 'author': 'Wei et al.'},\n",
              " {'title': 'Tree of Thoughts (Yao et al. 2023)', 'author': 'Yao et al.'},\n",
              " {'title': 'LLM+P (Liu et al. 2023)', 'author': 'Liu et al.'},\n",
              " {'title': 'ReAct (Yao et al. 2023)', 'author': 'Yao et al.'},\n",
              " {'title': 'Reflexion (Shinn & Labash 2023)', 'author': 'Shinn & Labash'},\n",
              " {'title': 'Reflexion framework', 'author': 'Shinn & Labash'},\n",
              " {'title': 'Chain of Hindsight', 'author': 'Liu et al.'},\n",
              " {'title': 'Algorithm Distillation', 'author': 'Laskin et al.'},\n",
              " {'title': 'Algorithm Distillation', 'author': 'Laskin et al. 2023'},\n",
              " {'title': 'ED (expert distillation)', 'author': ''},\n",
              " {'title': 'RL^2', 'author': 'Duan et al. 2017'},\n",
              " {'title': 'LSH: Locality-Sensitive Hashing', 'author': ''},\n",
              " {'title': 'ANNOY: Approximate Nearest Neighbors Oh Yeah', 'author': ''},\n",
              " {'title': 'HNSW: Hierarchical Navigable Small World', 'author': ''},\n",
              " {'title': 'FAISS: Facebook AI Similarity Search', 'author': ''},\n",
              " {'title': 'ScaNN: Scalable Nearest Neighbors', 'author': ''},\n",
              " {'title': 'MRKL: Modular Reasoning, Knowledge and Language',\n",
              "  'author': 'Karpas et al. 2022'},\n",
              " {'title': 'TALM: Tool Augmented Language Models',\n",
              "  'author': 'Parisi et al. 2022'},\n",
              " {'title': 'Toolformer', 'author': 'Schick et al. 2023'},\n",
              " {'title': 'HuggingGPT', 'author': 'Shen et al. 2023'},\n",
              " {'title': 'API-Bank: A Benchmark for Evaluating Tool-Augmented Language Models',\n",
              "  'author': 'Li et al. 2023'},\n",
              " {'title': 'ChemCrow: Augmenting Language Models with Expert-Designed Tools for Scientific Discovery',\n",
              "  'author': 'Bran et al. 2023'},\n",
              " {'title': 'Boiko et al. (2023)', 'author': 'Boiko et al.'},\n",
              " {'title': 'Generative Agents Simulation', 'author': 'Park, et al. 2023'},\n",
              " {'title': 'Park et al. 2023', 'author': ''},\n",
              " {'title': 'Super Mario: A Platform Game', 'author': 'John Smith'},\n",
              " {'title': 'MVC Architecture in Python', 'author': 'Jane Doe'},\n",
              " {'title': 'Keyboard Control in Python Games', 'author': 'Alice Johnson'},\n",
              " {'title': 'Paper A', 'author': 'Author A'},\n",
              " {'title': 'Paper B', 'author': 'Author B'},\n",
              " {'title': 'Paper C', 'author': 'Author C'},\n",
              " {'title': 'Chain of thought prompting elicits reasoning in large language models.',\n",
              "  'author': 'Wei et al.'},\n",
              " {'title': 'Tree of Thoughts: Deliberate Problem Solving with Large Language Models.',\n",
              "  'author': 'Yao et al.'},\n",
              " {'title': 'Chain of Hindsight Aligns Language Models with Feedback',\n",
              "  'author': 'Liu et al.'},\n",
              " {'title': 'LLM+P: Empowering Large Language Models with Optimal Planning Proficiency',\n",
              "  'author': 'Liu et al.'},\n",
              " {'title': 'ReAct: Synergizing reasoning and acting in language models.',\n",
              "  'author': 'Yao et al.'},\n",
              " {'title': 'Reflexion: an autonomous agent with dynamic memory and self-reflection',\n",
              "  'author': 'Shinn & Labash'},\n",
              " {'title': 'In-context Reinforcement Learning with Algorithm Distillation',\n",
              "  'author': 'Laskin et al.'},\n",
              " {'title': 'MRKL Systems A modular, neuro-symbolic architecture that combines large language models, external knowledge sources and discrete reasoning.',\n",
              "  'author': 'Karpas et al.'},\n",
              " {'title': 'API-Bank: A Benchmark for Tool-Augmented LLMs',\n",
              "  'author': 'Li et al.'},\n",
              " {'title': 'HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in HuggingFace',\n",
              "  'author': 'Shen et al.'},\n",
              " {'title': 'ChemCrow: Augmenting large-language models with chemistry tools.',\n",
              "  'author': 'Bran et al.'},\n",
              " {'title': 'Emergent autonomous scientific research capabilities of large language models.',\n",
              "  'author': 'Boiko et al.'},\n",
              " {'title': 'Generative Agents: Interactive Simulacra of Human Behavior.',\n",
              "  'author': 'Joon Sung Park, et al.'}]"
            ]
          },
          "metadata": {},
          "execution_count": 179
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### `Tools and Routing`"
      ],
      "metadata": {
        "id": "EXegzhKYpQ4l"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PnpYhcP9o4bT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}