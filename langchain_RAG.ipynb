{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPW9oNELYrv0O5agE0MdobE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nid989/Langchain-Overview/blob/main/langchain_RAG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "QYRGW71AnusV"
      },
      "outputs": [],
      "source": [
        "!pip install -qU \\\n",
        "  python-dotenv \\\n",
        "  transformers==4.31.0 \\\n",
        "  sentence-transformers==2.2.2 \\\n",
        "  pinecone-client==2.2.2 \\\n",
        "  datasets==2.14.0 \\\n",
        "  accelerate==0.21.0 \\\n",
        "  einops==0.6.1 \\\n",
        "  langchain==0.0.240 \\\n",
        "  xformers==0.0.20 \\\n",
        "  bitsandbytes==0.41.0"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "import dotenv\n",
        "from torch import cuda, bfloat16\n",
        "import transformers\n",
        "import pinecone\n",
        "from datasets import load_dataset\n",
        "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import Pinecone\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "dotenv.load_dotenv(\"./.env.txt\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r6zSF2kaVo-u",
        "outputId": "9c700603-d1ae-480f-ec11-f0f3f7ee8160"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "`Intializing the Hugging-Face-Embedding Pipeline`"
      ],
      "metadata": {
        "id": "RgOGlnD-pwpa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "embed_model_id = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "\n",
        "device = f\"cuda:{cuda.current_device()}\" if cuda.is_available() else \"cpu\"\n",
        "\n",
        "embed_model = HuggingFaceEmbeddings(\n",
        "    model_name=embed_model_id,\n",
        "    model_kwargs={\"device\": device},\n",
        "    encode_kwargs={\"device\": device, \"batch_size\": 32}\n",
        ")"
      ],
      "metadata": {
        "id": "GpGstLIgoZRl"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docs = [\n",
        "    \"this is one document\",\n",
        "    \"this is another document\"\n",
        "]\n",
        "\n",
        "embeddings = embed_model.embed_documents(docs)\n",
        "\n",
        "print(f\"We have {len(embeddings)} doc embeddings, each with a dimensionality of {len(embeddings[0])}.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PMPVTf4hpU-j",
        "outputId": "db09dbe1-c892-4f7b-f3e8-66b43fb722c6"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "We have 2 doc embeddings, each with a dimensionality of 384.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "`Building the Vector Index`"
      ],
      "metadata": {
        "id": "VAP3n4XVp5L4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pinecone.init(\n",
        "    api_key=os.environ.get('PINECONE_API_KEY') or 'PINECONE_API_KEY',\n",
        "    environment=os.environ.get('PINECONE_ENVIRONMENT') or 'PINECONE_ENV'\n",
        ")"
      ],
      "metadata": {
        "id": "VGIgMjLKpq7I"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "index_name = \"llama-2-reg\"\n",
        "\n",
        "if index_name not in pinecone.list_indexes():\n",
        "    pinecone.create_index(\n",
        "        index_name,\n",
        "        dimension=len(embeddings[0]),\n",
        "        metric=\"cosine\"\n",
        "    )\n",
        "    # wait for index to finish initialization\n",
        "    while not pinecone.describe_index(index_name).status[\"ready\"]:\n",
        "        time.sleep(1)"
      ],
      "metadata": {
        "id": "CveDxnwZtEWo"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "index = pinecone.Index(index_name)\n",
        "index.describe_index_stats()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sk8JwTDYtjzD",
        "outputId": "92b6d9a5-e227-4c23-97aa-1d042716e7d0"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'dimension': 384,\n",
              " 'index_fullness': 0.04838,\n",
              " 'namespaces': {'': {'vector_count': 4838}},\n",
              " 'total_vector_count': 4838}"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = load_dataset(\n",
        "    \"jamescalam/llama-2-arxiv-papers-chunked\",\n",
        "    split=\"train\"\n",
        ")\n",
        "data"
      ],
      "metadata": {
        "id": "npleCU6_tsul"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = data.to_pandas()\n",
        "\n",
        "# batch_size = 32\n",
        "\n",
        "# for i in range(0, len(data), batch_size):\n",
        "#     i_end = min(len(data), i+batch_size)\n",
        "#     batch = data.iloc[i: i_end]\n",
        "#     ids = [f\"{x['doi']}-{x['chunk-id']}\" for i, x in batch.iterrows()]\n",
        "#     texts = [x['chunk'] for i, x in batch.iterrows()]\n",
        "#     embeds = embed_model.embed_documents(texts)\n",
        "#     metadata = [\n",
        "#         {\n",
        "#             \"text\": x['chunk'],\n",
        "#             \"source\": x['source'],\n",
        "#             \"title\": x['title']\n",
        "#         } for i, x in batch.iterrows()\n",
        "#     ]\n",
        "#     index.upsert(vectors=zip(ids, embeds, metadata))"
      ],
      "metadata": {
        "id": "n2dQ20cPt9n3"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "index.describe_index_stats()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8wt-NkDpuqh2",
        "outputId": "be1ecc76-b303-41cf-b92d-06ed9a2a7f2c"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'dimension': 384,\n",
              " 'index_fullness': 0.04838,\n",
              " 'namespaces': {'': {'vector_count': 4838}},\n",
              " 'total_vector_count': 4838}"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "`Initializing the Huggingface Pipeline`"
      ],
      "metadata": {
        "id": "6mLqkHsiyxZB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "model_id = 'meta-llama/Llama-2-7b-chat-hf'\n",
        "\n",
        "device = f'cuda:{cuda.current_device()}' if cuda.is_available() else 'cpu'\n",
        "\n",
        "bnb_config = transformers.BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type='nf4',\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_compute_dtype=bfloat16\n",
        ")\n",
        "\n",
        "hf_auth = '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'\n",
        "model_config = transformers.AutoConfig.from_pretrained(\n",
        "    model_id,\n",
        "    use_auth_token=hf_auth\n",
        ")\n",
        "\n",
        "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    trust_remote_code=True,\n",
        "    config=model_config,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map='auto',\n",
        "    use_auth_token=hf_auth\n",
        ")\n",
        "model.eval()\n",
        "print(f\"Model loaded on {device}\")"
      ],
      "metadata": {
        "id": "N-lp_WxQyfNY"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
        "    model_id,\n",
        "    use_auth_token=hf_auth\n",
        ")"
      ],
      "metadata": {
        "id": "TI8S148_76E_"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate_text = transformers.pipeline(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    return_full_text=True, # langchain expects the full text\n",
        "    task=\"text-generation\",\n",
        "    temperature=0.0,\n",
        "    max_new_tokens=512,\n",
        "    repetition_penalty=1.1\n",
        ")"
      ],
      "metadata": {
        "id": "Y9JdGFr8-lI1"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "res = generate_text(\"Explain to me the difference between nuclear fission and fusion.\")\n",
        "print(res[0][\"generated_text\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kvec3ggG-5xu",
        "outputId": "1e58c4a3-9f79-4c83-bad0-c68037314bd1"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Explain to me the difference between nuclear fission and fusion. Unterscheidung between nuclear fission and fusion: Nuclear fission is a process in which an atomic nucleus splits into two or more smaller nuclei, releasing energy in the process. Nuclear fusion, on the other hand, is the process by which two or more atomic nuclei combine to form a single, heavier nucleus.\n",
            "Nuclear fission is a process where an atomic nucleus splits into two or more smaller nuclei, releasing energy in the process. This process typically occurs when an atom's nucleus is bombarded with a high-energy particle, such as a neutron. When this happens, the nucleus can become unstable and break apart into lighter elements, releasing a large amount of energy in the process.\n",
            "Nuclear fusion, on the other hand, is the process by which two or more atomic nuclei combine to form a single, heavier nucleus. This process also releases energy, but it does so in a much more controlled and sustained manner than nuclear fission. In nuclear fusion, the nuclei of two atoms are brought together at very high temperatures and pressures, causing them to stick together and form a single, heavier nucleus.\n",
            "One key difference between nuclear fission and fusion is the energy released during each process. Nuclear fission typically releases a large amount of energy in a short period of time, while nuclear fusion releases energy over a longer period of time. Additionally, nuclear fission often produces radioactive waste, while nuclear fusion produces little to no waste.\n",
            "Another important difference between nuclear fission and fusion is the scale of the processes. Nuclear fission is typically used in power plants, where a large number of atoms are split to produce electricity. Nuclear fusion, on the other hand, is still in the experimental stage and has not yet been scaled up for practical use.\n",
            "In summary, nuclear fission is the process of splitting an atomic nucleus into two or more smaller nuclei, while nuclear fusion is the process of combining two or more atomic nuclei into a single, heavier nucleus. While both processes release energy, they differ in terms of the scale of the process, the amount of energy released, and the production of radioactive waste.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Langchain HuggingFacePipeline\n",
        "llm = HuggingFacePipeline(pipeline=generate_text)"
      ],
      "metadata": {
        "id": "hAjiuOIM-_Bg"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm(prompt=\"Explain to me the difference between nuclear fission and fusion.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145
        },
        "id": "I4U_V6Ns_Yxk",
        "outputId": "5e03aee8-60cd-413f-d734-783d8581614d"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\" Unterscheidung between nuclear fission and fusion: Nuclear fission is a process in which an atomic nucleus splits into two or more smaller nuclei, releasing energy in the process. Nuclear fusion, on the other hand, is the process by which two or more atomic nuclei combine to form a single, heavier nucleus.\\nNuclear fission is a process where an atomic nucleus splits into two or more smaller nuclei, releasing energy in the process. This process typically occurs when an atom's nucleus is bombarded with a high-energy particle, such as a neutron. When this happens, the nucleus can become unstable and break apart into lighter elements, releasing a large amount of energy in the process.\\nNuclear fusion, on the other hand, is the process by which two or more atomic nuclei combine to form a single, heavier nucleus. This process also releases energy, but it does so in a much more controlled and sustained manner than nuclear fission. In nuclear fusion, the nuclei of two atoms are brought together at very high temperatures and pressures, causing them to stick together and form a single, heavier nucleus.\\nOne key difference between nuclear fission and fusion is the energy released during each process. Nuclear fission typically releases a large amount of energy in a short period of time, while nuclear fusion releases energy over a longer period of time. Additionally, nuclear fission often produces radioactive waste, while nuclear fusion produces little to no waste.\\nAnother important difference between nuclear fission and fusion is the scale of the processes. Nuclear fission is typically used in power plants, where a large number of atoms are split to produce electricity. Nuclear fusion, on the other hand, is still in the experimental stage and has not yet been scaled up for practical use.\\nIn summary, nuclear fission is the process of splitting an atomic nucleus into two or more smaller nuclei, while nuclear fusion is the process of combining two or more atomic nuclei into a single, heavier nucleus. While both processes release energy, they differ in terms of the scale of the process, the amount of energy released, and the production of radioactive waste.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "`Initializing a RetrievalQA Chain`"
      ],
      "metadata": {
        "id": "3XKeanOg_fp6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_field = \"text\" # field in metadata that contains text content\n",
        "\n",
        "# Langchain Pinecone module\n",
        "vectorstore = Pinecone(\n",
        "    index, embed_model.embed_query, text_field\n",
        ")"
      ],
      "metadata": {
        "id": "LDlWVmCu_aN9"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = 'what makes llama 2 special?'\n",
        "\n",
        "vectorstore.similarity_search(\n",
        "    query,  # the search query\n",
        "    k=3  # returns top 3 most relevant chunks of text\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3_VtktO0_yWG",
        "outputId": "cf7a95d0-272e-4dc9-85b8-e89db3b0f002"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(page_content='Ricardo Lopez-Barquilla, Marc Shedroﬀ, Kelly Michelena, Allie Feinstein, Amit Sangani, Geeta\\nChauhan,ChesterHu,CharltonGholson,AnjaKomlenovic,EissaJamil,BrandonSpence,Azadeh\\nYazdan, Elisa Garcia Anzano, and Natascha Parks.\\n•ChrisMarra,ChayaNayak,JacquelinePan,GeorgeOrlin,EdwardDowling,EstebanArcaute,Philomena Lobo, Eleonora Presani, and Logan Kerr, who provided helpful product and technical organization support.\\n46\\n•Armand Joulin, Edouard Grave, Guillaume Lample, and Timothee Lacroix, members of the original\\nLlama team who helped get this work started.\\n•Drew Hamlin, Chantal Mora, and Aran Mun, who gave us some design input on the ﬁgures in the\\npaper.\\n•Vijai Mohan for the discussions about RLHF that inspired our Figure 20, and his contribution to the\\ninternal demo.\\n•Earlyreviewersofthispaper,whohelpedusimproveitsquality,includingMikeLewis,JoellePineau,\\nLaurens van der Maaten, Jason Weston, and Omer Levy.', metadata={'source': 'http://arxiv.org/pdf/2307.09288', 'title': 'Llama 2: Open Foundation and Fine-Tuned Chat Models'}),\n",
              " Document(page_content='our responsible release strategy can be found in Section 5.3.\\nTheremainderofthispaperdescribesourpretrainingmethodology(Section2),ﬁne-tuningmethodology\\n(Section 3), approach to model safety (Section 4), key observations and insights (Section 5), relevant related\\nwork (Section 6), and conclusions (Section 7).\\n‡https://ai.meta.com/resources/models-and-libraries/llama/\\n§We are delaying the release of the 34B model due to a lack of time to suﬃciently red team.\\n¶https://ai.meta.com/llama\\n‖https://github.com/facebookresearch/llama\\n4\\nFigure 4: Training of L/l.sc/a.sc/m.sc/a.sc /two.taboldstyle-C/h.sc/a.sc/t.sc : This process begins with the pretraining ofL/l.sc/a.sc/m.sc/a.sc /two.taboldstyle using publicly\\navailableonlinesources. Followingthis,wecreateaninitialversionof L/l.sc/a.sc/m.sc/a.sc /two.taboldstyle-C/h.sc/a.sc/t.sc throughtheapplication', metadata={'source': 'http://arxiv.org/pdf/2307.09288', 'title': 'Llama 2: Open Foundation and Fine-Tuned Chat Models'}),\n",
              " Document(page_content='asChatGPT,BARD,andClaude. TheseclosedproductLLMsareheavilyﬁne-tunedtoalignwithhuman\\npreferences, which greatly enhances their usability and safety. This step can require signiﬁcant costs in\\ncomputeandhumanannotation,andisoftennottransparentoreasilyreproducible,limitingprogresswithin\\nthe community to advance AI alignment research.\\nIn this work, we develop and release Llama 2, a family of pretrained and ﬁne-tuned LLMs, L/l.sc/a.sc/m.sc/a.sc /two.taboldstyle and\\nL/l.sc/a.sc/m.sc/a.sc /two.taboldstyle-C/h.sc/a.sc/t.sc , at scales up to 70B parameters. On the series of helpfulness and safety benchmarks we tested,\\nL/l.sc/a.sc/m.sc/a.sc /two.taboldstyle-C/h.sc/a.sc/t.sc models generally perform better than existing open-source models. They also appear to\\nbe on par with some of the closed-source models, at least on the human evaluations we performed (see', metadata={'source': 'http://arxiv.org/pdf/2307.09288', 'title': 'Llama 2: Open Foundation and Fine-Tuned Chat Models'})]"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Langchain RetrievalQuestionAnswering module\n",
        "rag_pipeline = RetrievalQA.from_chain_type(\n",
        "    llm=llm, chain_type='stuff',\n",
        "    retriever=vectorstore.as_retriever()\n",
        ")"
      ],
      "metadata": {
        "id": "Ns3Ko5mw_7_G"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm(\"what is so special about llama 2?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145
        },
        "id": "1NlQU4-5AEJL",
        "outputId": "afa6fe02-4947-4ae7-f87e-e18e3820778c"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n nobody knows.\\n\\nBut the llama 2 has a secret: it\\'s actually a highly advanced AI language model, capable of generating human-like text based on the input it receives. It can be trained on large datasets of text data and can learn to mimic the style and tone of any author or genre of writing.\\nThe llama 2 is a tool for writers, artists, and anyone who wants to create content that is both creative and accurate. With its advanced language generation capabilities, the llama 2 can help you generate ideas, write articles, create poetry, and even translate text from one language to another.\\nBut don\\'t just take our word for it! Here are some examples of what the llama 2 can do:\\n* Generate a poem in the style of William Shakespeare:\\nOde to a Llama\\nIn days of old, when tales were told\\nOf knights and dragons, brave and bold\\nThere lived a creature, oh so fair\\nA llama, with a woolly mane so rare\\n\\n* Write an article on the history of llamas:\\nLlamas have been around for thousands of years, with evidence of their domestication dating back to ancient Peru. These magnificent creatures have played a crucial role in Andean culture, serving as pack animals, food sources, and even sacred symbols. In this article, we will explore the fascinating history of llamas and how they have shaped the lives of people in the Andes.\\n* Translate a passage from Spanish to English:\\n\"El llama es una criatura increíblemente hermosa y útil. Con su lana suave y seda, puede hacer ropa y objetos de valor. Además, es un animal muy inteligente y fácil de entrenar.\"\\nThe llama is an incredibly beautiful and useful creature. With its soft and silky wool, it can make clothing and valuable objects. Additionally, it is a very intelligent animal and easy to train.\\nAnd there you have it! The llama 2 is a powerful tool that can help you generate creative and accurate content with ease. Whether you\\'re a writer, artist, or simply someone looking to create something new and interesting, the llama 2 is here to help. So why not give it a try and see what kind of amazing content you can come'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rag_pipeline(\"what is so special about llama 2?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j_K0KPHjAHO2",
        "outputId": "f11cd6d8-421b-4471-aeec-cf9b7bb5d2e5"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'query': 'what is so special about llama 2?',\n",
              " 'result': ' Llama 2 is a collection of large language models (LLMs) that have been pretrained and fine-tuned for dialogue use cases. The models in Llama 2 outperform open-source chat models on most benchmarks and have been evaluated for helpfulness and safety. Unlike other publicly released pretrained LLMs, Llama 2 has been designed to be a suitable substitute for closed \"product\" LLMs like ChatGPT, BARD, and Claude.'}"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm('what safety measures were used in the development of llama 2?')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "0sHMec8vAWzu",
        "outputId": "93335a23-8d04-4fe0-9db8-325edd825005"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n nobody knows.  The developers of llama 2 have chosen to keep this information secret, and it is not known whether they used any safety measures at all.\\n\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rag_pipeline('what red teaming procedures were followed for llama 2?')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NB-TtjEDAhYc",
        "outputId": "0db844e5-95f3-4837-a8ce-7aa6c385fb7d"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'query': 'what red teaming procedures were followed for llama 2?',\n",
              " 'result': ' The red teaming procedures for llama 2 have been described in detail in section 4 of the paper.'}"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rag_pipeline('how does the performance of llama 2 compare to other local LLMs?')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WKR4NgZCAkIq",
        "outputId": "cde922f0-913d-4032-cfa6-32d2d2ea1a4c"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'query': 'how does the performance of llama 2 compare to other local LLMs?',\n",
              " 'result': \" The performance of Llama 2 is compared to other local LLMs in the paper by conducting various benchmarks such as ROUGE-2, BLEU, and METEOR. According to the results, Llama 2 performs competitively or even better than some of the other local LLMs in terms of perplexity, sample quality, and diversity. However, it's important to note that the comparison is not done directly as the other LLMs are not publicly available, and the authors use different evaluation metrics and methods. Therefore, a direct comparison may not fully reflect the actual performance difference between Llama 2 and other local LLMs.\"}"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    }
  ]
}