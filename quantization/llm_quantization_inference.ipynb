{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNehdrlZYVJH2s/iQGv6VYV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nid989/LLM-Overview/blob/main/LLM_Quantization_Inference.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "E9lfIiY9NXAK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8864352f-5e41-41ea-8bd6-9390cc751a2f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.6/92.6 MB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.7/7.7 MB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.0/302.0 kB\u001b[0m \u001b[31m35.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m56.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m57.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.0/295.0 kB\u001b[0m \u001b[31m37.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.6/85.6 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m258.1/258.1 kB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -qU python-dotenv\n",
        "!pip install -qU bitsandbytes\n",
        "!pip install -qU transformers\n",
        "!pip install -qU peft\n",
        "!pip install -qU accelerate"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import dotenv\n",
        "import torch\n",
        "from torch import cuda, bfloat16\n",
        "from peft import (\n",
        "    LoraConfig,\n",
        "    get_peft_model,\n",
        "    TaskType\n",
        ")\n",
        "from transformers import (\n",
        "    AutoConfig,\n",
        "    AutoModelForCausalLM,\n",
        "    BitsAndBytesConfig,\n",
        "    AutoTokenizer,\n",
        "    pipeline\n",
        ")\n",
        "\n",
        "# load environment variable files w/ saved authorization tokens\n",
        "_ = dotenv.load_dotenv(\"./.env.txt\")\n",
        "# set Huggingface authorization token\n",
        "hf_auth = os.environ.get(\"HF_AUTH\") or \"HF_AUTH\"\n",
        "# set cuda device\n",
        "device = f\"cuda:{cuda.current_device()}\" if torch.cuda.is_available() else \"cpu\""
      ],
      "metadata": {
        "id": "WNtR2yf7N2IL"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### `LLaMA 2`"
      ],
      "metadata": {
        "id": "R2myC931OmP1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "`LoRA`"
      ],
      "metadata": {
        "id": "Uw3jXROyZYSb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "# NOTE: requires colab-pro to be executed\n",
        "model_id = \"meta-llama/Llama-2-7b-chat-hf\"\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    use_auth_token=hf_auth\n",
        ")\n",
        "\n",
        "config = LoraConfig(\n",
        "    r=8, # rank of the update matrices; Lower rank results in smaller update matrices with fewer trainable parameters\n",
        "    lora_alpha=32, # LoRA scaling factor\n",
        "    target_modules=[\"self_attn.q_proj\", \"self_attn.k_proj\",\n",
        "                    \"self_attn.v_proj\", \"self_attn.o_proj\"], # modules to apply the LoRA update matrices; specific to each model\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\", # specifies if the bias parameters should be trained\n",
        "    task_type=TaskType.CAUSAL_LM\n",
        ")\n",
        "\n",
        "# transition original model to have LoRA layers\n",
        "model = get_peft_model(model, config)"
      ],
      "metadata": {
        "id": "hZH__4uTZYpb"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "`Q-LoRA`"
      ],
      "metadata": {
        "id": "IFi7cp16LHqF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "model_id = \"meta-llama/Llama-2-13b-chat-hf\"\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\", # 4bit quantization; Options[NF4 (normalized float 4 (default)), pure FP4]\n",
        "    bnb_4bit_use_double_quant=True, # second quantization, applied after the first quantization, to save an additional 0.4 bits per parameters\n",
        "    bnb_4bit_compute_dtype=bfloat16 # compute type; while 4-bit bitsandbytes stores weights in 4-bits, the computation still happens in 16 or 32-bit; Option[float16, bfloat15, float32, ...]\n",
        ")\n",
        "\n",
        "model_config = AutoConfig.from_pretrained(\n",
        "    model_id,\n",
        "    use_auth_token=hf_auth\n",
        ")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    trust_remote_code=True,\n",
        "    config=model_config,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        "    use_auth_token=hf_auth\n",
        ")\n",
        "model.eval() # inference/evaluation mode, no parameter optimization"
      ],
      "metadata": {
        "id": "LbwQZKhKO8QU"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    model_id,\n",
        "    use_auth_token=hf_auth\n",
        ")"
      ],
      "metadata": {
        "id": "VBsgUzpHQ4XJ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# transformer text-generation pipeline\n",
        "llama2_qlora_pipeline = pipeline(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    return_full_text=True,\n",
        "    task=\"text-generation\",\n",
        "    temperature=0.0001,\n",
        "    max_new_tokens=512,\n",
        "    repetition_penalty=1.1\n",
        ")"
      ],
      "metadata": {
        "id": "w-0JdualSPPc"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "res = llama2_qlora_pipeline(\"Write a short story about time travel.\")\n",
        "print(res[0][\"generated_text\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DMixGoVJTjp7",
        "outputId": "fc9841bc-a649-4afd-eb9d-4f3c0c3ddeb3"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Write a short story about time travel.\n",
            "\n",
            "Time Traveler's Dilemma\n",
            "\n",
            "As soon as the time machine was completed, Emily couldn't wait to try it out. She had spent years building it, pouring over theories and diagrams, testing and retesting every component. Finally, she was ready to see if it would work.\n",
            "\n",
            "She climbed inside and set the dials for a date 20 years into the future. The machine whirred to life, and before she knew it, she was standing in the middle of a bustling city street.\n",
            "\n",
            "At first, everything seemed familiar. The buildings were taller and more modern than they were now, but the people and the energy of the city were the same. But then, something caught her eye. A group of people walking down the street were wearing clothes that she had never seen before. They were sleek and shimmering, like nothing she had ever imagined.\n",
            "\n",
            "Emily felt a pang of excitement. She had always been fascinated by fashion, and the idea of seeing what the future held in store for style was too tempting to resist. She followed the group of people, her eyes drinking in every detail of their outfits.\n",
            "\n",
            "But as she walked, she began to notice strange things. People were looking at her strangely, and she realized that she must have stood out in her own time-traveling outfit. She quickly ducked into a nearby shop to avoid drawing attention to herself.\n",
            "\n",
            "Inside, she found a group of people huddled around a screen, watching a video of her arrival. They were all dressed in the same futuristic clothing that she had seen on the street, and they looked at her with a mixture of curiosity and suspicion.\n",
            "\n",
            "One of them stepped forward, a tall, imposing figure with piercing blue eyes. \"Who are you?\" he demanded.\n",
            "\n",
            "Emily hesitated, unsure of how much to reveal. \"I'm a time traveler,\" she said finally. \"I came from the past to see what the future holds.\"\n",
            "\n",
            "The man raised an eyebrow. \"You're a time traveler? That's impossible. Time travel is just a theory.\"\n",
            "\n",
            "Emily sighed. She had expected this reaction. \"I know it sounds crazy,\" she said. \"But I built a time machine,\n"
          ]
        }
      ]
    }
  ]
}